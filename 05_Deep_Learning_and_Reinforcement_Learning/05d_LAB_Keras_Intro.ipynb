{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 5, Part d: Keras Intro LAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras to Build and Train Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will use a neural network to predict diabetes using the Pima Diabetes Dataset.  We will start by training a Random Forest to get a performance baseline.  Then we will use the Keras package to quickly build and train a neural network and compare the performance.  We will see how different network structures affect the performance, training time, and level of overfitting (or underfitting).\n",
    "\n",
    "## UCI Pima Diabetes Dataset\n",
    "\n",
    "* UCI ML Repositiory (http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes)\n",
    "\n",
    "\n",
    "### Attributes: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI Pima Diabetes Dataset which has 8 numerical predictors and a binary outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Keras objects for Deep Learning\n",
    "from tensorflow.keras.models  import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the data set \n",
    "names = [\"times_pregnant\", \"glucose_tolerance_test\", \"blood_pressure\", \"skin_thickness\", \"insulin\", \n",
    "         \"bmi\", \"pedigree_function\", \"age\", \"has_diabetes\"]\n",
    "diabetes_df = pd.read_csv('data/diabetes.csv', names=names, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>times_pregnant</th>\n",
       "      <th>glucose_tolerance_test</th>\n",
       "      <th>blood_pressure</th>\n",
       "      <th>skin_thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree_function</th>\n",
       "      <th>age</th>\n",
       "      <th>has_diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0.144</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.3</td>\n",
       "      <td>0.258</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.3</td>\n",
       "      <td>0.933</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "      <td>25</td>\n",
       "      <td>37</td>\n",
       "      <td>25.4</td>\n",
       "      <td>0.583</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.484</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     times_pregnant  glucose_tolerance_test  blood_pressure  skin_thickness  \\\n",
       "628               5                     128              80               0   \n",
       "757               0                     123              72               0   \n",
       "266               0                     138               0               0   \n",
       "232               1                      79              80              25   \n",
       "15                7                     100               0               0   \n",
       "\n",
       "     insulin   bmi  pedigree_function  age  has_diabetes  \n",
       "628        0  34.6              0.144   45             0  \n",
       "757        0  36.3              0.258   52             1  \n",
       "266        0  36.3              0.933   25             1  \n",
       "232       37  25.4              0.583   22             0  \n",
       "15         0  30.0              0.484   32             1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peek at the data -- if there are lots of \"NaN\" you may have internet connectivity issues\n",
    "print(diabetes_df.shape)\n",
    "diabetes_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes_df.iloc[:, :-1].values\n",
    "y = diabetes_df[\"has_diabetes\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to Train, and Test (75%, 25%)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=11111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3489583333333333, 0.6510416666666666)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y), np.mean(1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that about 35% of the patients in this dataset have diabetes, while 65% do not.  This means we can get an accuracy of 65% without any model - just declare that no one has diabetes. We will calculate the ROC-AUC score to evaluate performance of our model, and also look at the accuracy as well to see if we improved upon the 65% accuracy.\n",
    "## Exercise 1: Get a baseline performance using Random Forest\n",
    "To begin, and get a baseline for classifier performance:\n",
    "1. Train a Random Forest model with 200 trees on the training data.\n",
    "2. Calculate the accuracy and roc_auc_score of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=200)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=200)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "## Train the RF Model\n",
    "rf_model = RandomForestClassifier(n_estimators=200)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.766\n",
      "roc-auc is 0.823\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set - both \"hard\" predictions, and the scores (percent of trees voting yes)\n",
    "y_pred_class_rf = rf_model.predict(X_test)\n",
    "y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_roc(y_test, y_pred, model_name):\n",
    "    fpr, tpr, thr = roc_curve(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(fpr, tpr, 'k-')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=.5)  # roc curve for random model\n",
    "    ax.grid(True)\n",
    "    ax.set(title='ROC Curve for {} on PIMA diabetes problem'.format(model_name),\n",
    "           xlim=[-0.01, 1.01], ylim=[-0.01, 1.01])\n",
    "    \n",
    "plot_roc(y_test, y_pred_prob_rf[:, 1], 'RF')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Single Hidden Layer Neural Network\n",
    "\n",
    "We will use the Sequential model to quickly build a neural network.  Our first network will be a single layer network.  We have 8 variables, so we set the input shape to 8.  Let's start by having a single hidden layer with 12 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First let's normalize the data\n",
    "## This aids the training of neural nets by providing numerical stability\n",
    "## Random Forest does not need this as it finds a split only, as opposed to performing matrix multiplications\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model \n",
    "# Input size is 8-dimensional\n",
    "# 1 hidden layer, 12 hidden nodes, sigmoid activation\n",
    "# Final layer has just one node with a sigmoid activation (standard for binary classification)\n",
    "\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(12,input_shape = (8,),activation = 'sigmoid'))\n",
    "model_1.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 12)                108       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  This is a nice tool to view the model you have created and count the parameters\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehension question:\n",
    "Why do we have 121 parameters?  Does that make sense?\n",
    "\n",
    "Let's fit our model for 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-25 10:06:17.814328: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 5ms/step - loss: 0.8416 - accuracy: 0.3299 - val_loss: 0.8310 - val_accuracy: 0.3490\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.8254 - accuracy: 0.3212 - val_loss: 0.8159 - val_accuracy: 0.3594\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.8108 - accuracy: 0.3281 - val_loss: 0.8022 - val_accuracy: 0.3594\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7976 - accuracy: 0.3229 - val_loss: 0.7898 - val_accuracy: 0.3854\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7856 - accuracy: 0.3177 - val_loss: 0.7786 - val_accuracy: 0.3802\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7747 - accuracy: 0.3247 - val_loss: 0.7685 - val_accuracy: 0.3698\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7649 - accuracy: 0.3281 - val_loss: 0.7594 - val_accuracy: 0.3490\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7560 - accuracy: 0.3299 - val_loss: 0.7511 - val_accuracy: 0.3177\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7479 - accuracy: 0.3385 - val_loss: 0.7437 - val_accuracy: 0.3333\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7406 - accuracy: 0.3455 - val_loss: 0.7370 - val_accuracy: 0.3542\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7340 - accuracy: 0.3611 - val_loss: 0.7309 - val_accuracy: 0.3542\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7280 - accuracy: 0.3750 - val_loss: 0.7254 - val_accuracy: 0.3854\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7225 - accuracy: 0.3854 - val_loss: 0.7204 - val_accuracy: 0.4062\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7175 - accuracy: 0.4149 - val_loss: 0.7159 - val_accuracy: 0.4115\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7130 - accuracy: 0.4462 - val_loss: 0.7117 - val_accuracy: 0.4323\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7088 - accuracy: 0.4705 - val_loss: 0.7080 - val_accuracy: 0.4531\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7050 - accuracy: 0.4983 - val_loss: 0.7045 - val_accuracy: 0.4792\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7015 - accuracy: 0.5365 - val_loss: 0.7014 - val_accuracy: 0.4896\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6983 - accuracy: 0.5399 - val_loss: 0.6985 - val_accuracy: 0.5208\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6953 - accuracy: 0.5660 - val_loss: 0.6958 - val_accuracy: 0.5469\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6926 - accuracy: 0.5920 - val_loss: 0.6934 - val_accuracy: 0.5729\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6901 - accuracy: 0.6111 - val_loss: 0.6912 - val_accuracy: 0.5938\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6878 - accuracy: 0.6233 - val_loss: 0.6891 - val_accuracy: 0.5990\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6856 - accuracy: 0.6267 - val_loss: 0.6871 - val_accuracy: 0.6146\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6836 - accuracy: 0.6319 - val_loss: 0.6853 - val_accuracy: 0.6146\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6817 - accuracy: 0.6337 - val_loss: 0.6837 - val_accuracy: 0.6302\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6799 - accuracy: 0.6389 - val_loss: 0.6821 - val_accuracy: 0.6354\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6783 - accuracy: 0.6424 - val_loss: 0.6806 - val_accuracy: 0.6406\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6767 - accuracy: 0.6424 - val_loss: 0.6792 - val_accuracy: 0.6458\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6752 - accuracy: 0.6476 - val_loss: 0.6779 - val_accuracy: 0.6458\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6738 - accuracy: 0.6510 - val_loss: 0.6767 - val_accuracy: 0.6458\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6725 - accuracy: 0.6510 - val_loss: 0.6755 - val_accuracy: 0.6406\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 0s 999us/step - loss: 0.6712 - accuracy: 0.6528 - val_loss: 0.6744 - val_accuracy: 0.6406\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6700 - accuracy: 0.6545 - val_loss: 0.6733 - val_accuracy: 0.6406\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6688 - accuracy: 0.6562 - val_loss: 0.6723 - val_accuracy: 0.6406\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6677 - accuracy: 0.6562 - val_loss: 0.6713 - val_accuracy: 0.6406\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6667 - accuracy: 0.6580 - val_loss: 0.6703 - val_accuracy: 0.6406\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6657 - accuracy: 0.6580 - val_loss: 0.6694 - val_accuracy: 0.6406\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6646 - accuracy: 0.6580 - val_loss: 0.6685 - val_accuracy: 0.6406\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6637 - accuracy: 0.6545 - val_loss: 0.6676 - val_accuracy: 0.6406\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6627 - accuracy: 0.6545 - val_loss: 0.6668 - val_accuracy: 0.6406\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6619 - accuracy: 0.6545 - val_loss: 0.6660 - val_accuracy: 0.6406\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6610 - accuracy: 0.6545 - val_loss: 0.6652 - val_accuracy: 0.6406\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6601 - accuracy: 0.6545 - val_loss: 0.6644 - val_accuracy: 0.6406\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6592 - accuracy: 0.6545 - val_loss: 0.6636 - val_accuracy: 0.6406\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6584 - accuracy: 0.6545 - val_loss: 0.6629 - val_accuracy: 0.6406\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6576 - accuracy: 0.6545 - val_loss: 0.6621 - val_accuracy: 0.6406\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6568 - accuracy: 0.6545 - val_loss: 0.6614 - val_accuracy: 0.6406\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6560 - accuracy: 0.6545 - val_loss: 0.6607 - val_accuracy: 0.6406\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6553 - accuracy: 0.6545 - val_loss: 0.6600 - val_accuracy: 0.6406\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6545 - accuracy: 0.6545 - val_loss: 0.6593 - val_accuracy: 0.6406\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6538 - accuracy: 0.6545 - val_loss: 0.6586 - val_accuracy: 0.6406\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6530 - accuracy: 0.6545 - val_loss: 0.6579 - val_accuracy: 0.6406\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6523 - accuracy: 0.6545 - val_loss: 0.6572 - val_accuracy: 0.6406\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6516 - accuracy: 0.6545 - val_loss: 0.6566 - val_accuracy: 0.6406\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6509 - accuracy: 0.6545 - val_loss: 0.6559 - val_accuracy: 0.6406\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6502 - accuracy: 0.6545 - val_loss: 0.6552 - val_accuracy: 0.6406\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6495 - accuracy: 0.6545 - val_loss: 0.6546 - val_accuracy: 0.6406\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6488 - accuracy: 0.6545 - val_loss: 0.6539 - val_accuracy: 0.6406\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6481 - accuracy: 0.6545 - val_loss: 0.6533 - val_accuracy: 0.6406\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6474 - accuracy: 0.6545 - val_loss: 0.6527 - val_accuracy: 0.6406\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6468 - accuracy: 0.6545 - val_loss: 0.6520 - val_accuracy: 0.6406\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6461 - accuracy: 0.6545 - val_loss: 0.6514 - val_accuracy: 0.6406\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6455 - accuracy: 0.6545 - val_loss: 0.6508 - val_accuracy: 0.6406\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6448 - accuracy: 0.6545 - val_loss: 0.6501 - val_accuracy: 0.6406\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6442 - accuracy: 0.6545 - val_loss: 0.6495 - val_accuracy: 0.6406\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6435 - accuracy: 0.6545 - val_loss: 0.6489 - val_accuracy: 0.6406\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6429 - accuracy: 0.6545 - val_loss: 0.6483 - val_accuracy: 0.6406\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6422 - accuracy: 0.6545 - val_loss: 0.6477 - val_accuracy: 0.6406\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6416 - accuracy: 0.6545 - val_loss: 0.6471 - val_accuracy: 0.6406\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6409 - accuracy: 0.6545 - val_loss: 0.6465 - val_accuracy: 0.6406\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6403 - accuracy: 0.6545 - val_loss: 0.6458 - val_accuracy: 0.6406\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6397 - accuracy: 0.6545 - val_loss: 0.6452 - val_accuracy: 0.6406\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6391 - accuracy: 0.6545 - val_loss: 0.6446 - val_accuracy: 0.6406\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6385 - accuracy: 0.6545 - val_loss: 0.6440 - val_accuracy: 0.6406\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6378 - accuracy: 0.6545 - val_loss: 0.6435 - val_accuracy: 0.6406\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6372 - accuracy: 0.6545 - val_loss: 0.6429 - val_accuracy: 0.6406\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6366 - accuracy: 0.6545 - val_loss: 0.6423 - val_accuracy: 0.6406\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6360 - accuracy: 0.6545 - val_loss: 0.6417 - val_accuracy: 0.6406\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6354 - accuracy: 0.6545 - val_loss: 0.6411 - val_accuracy: 0.6406\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6348 - accuracy: 0.6545 - val_loss: 0.6405 - val_accuracy: 0.6406\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6342 - accuracy: 0.6545 - val_loss: 0.6399 - val_accuracy: 0.6406\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6336 - accuracy: 0.6545 - val_loss: 0.6393 - val_accuracy: 0.6406\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6330 - accuracy: 0.6545 - val_loss: 0.6388 - val_accuracy: 0.6406\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 0s 999us/step - loss: 0.6324 - accuracy: 0.6545 - val_loss: 0.6382 - val_accuracy: 0.6406\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6318 - accuracy: 0.6545 - val_loss: 0.6376 - val_accuracy: 0.6406\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6312 - accuracy: 0.6545 - val_loss: 0.6370 - val_accuracy: 0.6406\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6307 - accuracy: 0.6545 - val_loss: 0.6365 - val_accuracy: 0.6406\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6301 - accuracy: 0.6545 - val_loss: 0.6359 - val_accuracy: 0.6406\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6294 - accuracy: 0.6545 - val_loss: 0.6353 - val_accuracy: 0.6406\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6289 - accuracy: 0.6545 - val_loss: 0.6348 - val_accuracy: 0.6406\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6283 - accuracy: 0.6545 - val_loss: 0.6342 - val_accuracy: 0.6406\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6277 - accuracy: 0.6545 - val_loss: 0.6336 - val_accuracy: 0.6406\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6272 - accuracy: 0.6545 - val_loss: 0.6331 - val_accuracy: 0.6406\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6266 - accuracy: 0.6545 - val_loss: 0.6325 - val_accuracy: 0.6406\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 0s 988us/step - loss: 0.6260 - accuracy: 0.6545 - val_loss: 0.6319 - val_accuracy: 0.6406\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6255 - accuracy: 0.6545 - val_loss: 0.6314 - val_accuracy: 0.6406\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6249 - accuracy: 0.6545 - val_loss: 0.6308 - val_accuracy: 0.6406\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6243 - accuracy: 0.6545 - val_loss: 0.6303 - val_accuracy: 0.6406\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6238 - accuracy: 0.6545 - val_loss: 0.6297 - val_accuracy: 0.6406\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6232 - accuracy: 0.6545 - val_loss: 0.6292 - val_accuracy: 0.6406\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6226 - accuracy: 0.6545 - val_loss: 0.6286 - val_accuracy: 0.6406\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6221 - accuracy: 0.6545 - val_loss: 0.6281 - val_accuracy: 0.6406\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6215 - accuracy: 0.6545 - val_loss: 0.6275 - val_accuracy: 0.6406\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6210 - accuracy: 0.6545 - val_loss: 0.6270 - val_accuracy: 0.6406\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6204 - accuracy: 0.6545 - val_loss: 0.6265 - val_accuracy: 0.6406\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6199 - accuracy: 0.6545 - val_loss: 0.6259 - val_accuracy: 0.6406\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6193 - accuracy: 0.6545 - val_loss: 0.6254 - val_accuracy: 0.6406\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6187 - accuracy: 0.6545 - val_loss: 0.6248 - val_accuracy: 0.6406\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6182 - accuracy: 0.6545 - val_loss: 0.6243 - val_accuracy: 0.6406\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6177 - accuracy: 0.6545 - val_loss: 0.6238 - val_accuracy: 0.6406\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 0s 935us/step - loss: 0.6171 - accuracy: 0.6545 - val_loss: 0.6232 - val_accuracy: 0.6406\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6166 - accuracy: 0.6545 - val_loss: 0.6227 - val_accuracy: 0.6406\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6160 - accuracy: 0.6545 - val_loss: 0.6222 - val_accuracy: 0.6406\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6155 - accuracy: 0.6545 - val_loss: 0.6216 - val_accuracy: 0.6406\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6150 - accuracy: 0.6545 - val_loss: 0.6211 - val_accuracy: 0.6406\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6144 - accuracy: 0.6545 - val_loss: 0.6206 - val_accuracy: 0.6406\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6139 - accuracy: 0.6545 - val_loss: 0.6201 - val_accuracy: 0.6406\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6134 - accuracy: 0.6545 - val_loss: 0.6195 - val_accuracy: 0.6406\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6128 - accuracy: 0.6545 - val_loss: 0.6190 - val_accuracy: 0.6406\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6124 - accuracy: 0.6545 - val_loss: 0.6185 - val_accuracy: 0.6406\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6118 - accuracy: 0.6545 - val_loss: 0.6180 - val_accuracy: 0.6406\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6112 - accuracy: 0.6545 - val_loss: 0.6175 - val_accuracy: 0.6406\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6107 - accuracy: 0.6562 - val_loss: 0.6170 - val_accuracy: 0.6458\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6102 - accuracy: 0.6562 - val_loss: 0.6164 - val_accuracy: 0.6458\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6097 - accuracy: 0.6562 - val_loss: 0.6159 - val_accuracy: 0.6458\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6092 - accuracy: 0.6562 - val_loss: 0.6154 - val_accuracy: 0.6458\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6087 - accuracy: 0.6562 - val_loss: 0.6149 - val_accuracy: 0.6458\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6081 - accuracy: 0.6580 - val_loss: 0.6144 - val_accuracy: 0.6458\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6076 - accuracy: 0.6580 - val_loss: 0.6139 - val_accuracy: 0.6458\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6071 - accuracy: 0.6580 - val_loss: 0.6134 - val_accuracy: 0.6458\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6066 - accuracy: 0.6580 - val_loss: 0.6129 - val_accuracy: 0.6458\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6061 - accuracy: 0.6580 - val_loss: 0.6124 - val_accuracy: 0.6458\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6056 - accuracy: 0.6580 - val_loss: 0.6119 - val_accuracy: 0.6458\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6051 - accuracy: 0.6580 - val_loss: 0.6114 - val_accuracy: 0.6458\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6045 - accuracy: 0.6580 - val_loss: 0.6109 - val_accuracy: 0.6458\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6040 - accuracy: 0.6580 - val_loss: 0.6104 - val_accuracy: 0.6458\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6035 - accuracy: 0.6580 - val_loss: 0.6099 - val_accuracy: 0.6458\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6030 - accuracy: 0.6580 - val_loss: 0.6094 - val_accuracy: 0.6458\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6025 - accuracy: 0.6580 - val_loss: 0.6089 - val_accuracy: 0.6458\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6020 - accuracy: 0.6580 - val_loss: 0.6084 - val_accuracy: 0.6458\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6015 - accuracy: 0.6580 - val_loss: 0.6079 - val_accuracy: 0.6458\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 0s 992us/step - loss: 0.6010 - accuracy: 0.6580 - val_loss: 0.6074 - val_accuracy: 0.6458\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6005 - accuracy: 0.6580 - val_loss: 0.6069 - val_accuracy: 0.6458\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6000 - accuracy: 0.6580 - val_loss: 0.6064 - val_accuracy: 0.6458\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 0s 975us/step - loss: 0.5995 - accuracy: 0.6580 - val_loss: 0.6059 - val_accuracy: 0.6458\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5990 - accuracy: 0.6580 - val_loss: 0.6055 - val_accuracy: 0.6458\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5986 - accuracy: 0.6597 - val_loss: 0.6050 - val_accuracy: 0.6458\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5980 - accuracy: 0.6615 - val_loss: 0.6045 - val_accuracy: 0.6458\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5976 - accuracy: 0.6615 - val_loss: 0.6040 - val_accuracy: 0.6458\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5971 - accuracy: 0.6615 - val_loss: 0.6035 - val_accuracy: 0.6458\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 0s 990us/step - loss: 0.5966 - accuracy: 0.6615 - val_loss: 0.6031 - val_accuracy: 0.6458\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5961 - accuracy: 0.6615 - val_loss: 0.6026 - val_accuracy: 0.6510\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5956 - accuracy: 0.6615 - val_loss: 0.6021 - val_accuracy: 0.6510\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5951 - accuracy: 0.6632 - val_loss: 0.6016 - val_accuracy: 0.6510\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5946 - accuracy: 0.6632 - val_loss: 0.6011 - val_accuracy: 0.6510\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5942 - accuracy: 0.6632 - val_loss: 0.6007 - val_accuracy: 0.6510\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5937 - accuracy: 0.6632 - val_loss: 0.6002 - val_accuracy: 0.6510\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5932 - accuracy: 0.6649 - val_loss: 0.5997 - val_accuracy: 0.6510\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5927 - accuracy: 0.6649 - val_loss: 0.5993 - val_accuracy: 0.6510\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5922 - accuracy: 0.6649 - val_loss: 0.5988 - val_accuracy: 0.6510\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5918 - accuracy: 0.6649 - val_loss: 0.5983 - val_accuracy: 0.6510\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5913 - accuracy: 0.6649 - val_loss: 0.5979 - val_accuracy: 0.6510\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5908 - accuracy: 0.6667 - val_loss: 0.5974 - val_accuracy: 0.6510\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5903 - accuracy: 0.6684 - val_loss: 0.5969 - val_accuracy: 0.6510\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5898 - accuracy: 0.6684 - val_loss: 0.5965 - val_accuracy: 0.6510\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5894 - accuracy: 0.6684 - val_loss: 0.5960 - val_accuracy: 0.6510\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5889 - accuracy: 0.6684 - val_loss: 0.5956 - val_accuracy: 0.6510\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5884 - accuracy: 0.6684 - val_loss: 0.5951 - val_accuracy: 0.6510\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5880 - accuracy: 0.6684 - val_loss: 0.5946 - val_accuracy: 0.6510\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5875 - accuracy: 0.6684 - val_loss: 0.5942 - val_accuracy: 0.6510\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5871 - accuracy: 0.6684 - val_loss: 0.5937 - val_accuracy: 0.6510\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5866 - accuracy: 0.6684 - val_loss: 0.5933 - val_accuracy: 0.6562\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5861 - accuracy: 0.6701 - val_loss: 0.5928 - val_accuracy: 0.6562\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5856 - accuracy: 0.6701 - val_loss: 0.5924 - val_accuracy: 0.6562\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5852 - accuracy: 0.6719 - val_loss: 0.5919 - val_accuracy: 0.6615\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5847 - accuracy: 0.6719 - val_loss: 0.5915 - val_accuracy: 0.6615\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 0s 979us/step - loss: 0.5843 - accuracy: 0.6719 - val_loss: 0.5910 - val_accuracy: 0.6615\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5838 - accuracy: 0.6701 - val_loss: 0.5906 - val_accuracy: 0.6615\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5834 - accuracy: 0.6719 - val_loss: 0.5901 - val_accuracy: 0.6615\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5829 - accuracy: 0.6701 - val_loss: 0.5897 - val_accuracy: 0.6615\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5825 - accuracy: 0.6719 - val_loss: 0.5892 - val_accuracy: 0.6615\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5820 - accuracy: 0.6701 - val_loss: 0.5888 - val_accuracy: 0.6615\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5816 - accuracy: 0.6701 - val_loss: 0.5884 - val_accuracy: 0.6615\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5811 - accuracy: 0.6701 - val_loss: 0.5879 - val_accuracy: 0.6615\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5806 - accuracy: 0.6701 - val_loss: 0.5875 - val_accuracy: 0.6615\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5802 - accuracy: 0.6719 - val_loss: 0.5870 - val_accuracy: 0.6615\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5798 - accuracy: 0.6701 - val_loss: 0.5866 - val_accuracy: 0.6615\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5793 - accuracy: 0.6701 - val_loss: 0.5862 - val_accuracy: 0.6615\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5789 - accuracy: 0.6701 - val_loss: 0.5857 - val_accuracy: 0.6615\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5784 - accuracy: 0.6719 - val_loss: 0.5853 - val_accuracy: 0.6615\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5780 - accuracy: 0.6719 - val_loss: 0.5849 - val_accuracy: 0.6615\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5775 - accuracy: 0.6719 - val_loss: 0.5844 - val_accuracy: 0.6667\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5771 - accuracy: 0.6719 - val_loss: 0.5840 - val_accuracy: 0.6667\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5766 - accuracy: 0.6719 - val_loss: 0.5836 - val_accuracy: 0.6667\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5762 - accuracy: 0.6667 - val_loss: 0.5832 - val_accuracy: 0.6667\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5758 - accuracy: 0.6667 - val_loss: 0.5827 - val_accuracy: 0.6667\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5753 - accuracy: 0.6667 - val_loss: 0.5823 - val_accuracy: 0.6667\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5749 - accuracy: 0.6667 - val_loss: 0.5819 - val_accuracy: 0.6667\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5745 - accuracy: 0.6667 - val_loss: 0.5815 - val_accuracy: 0.6719\n"
     ]
    }
   ],
   "source": [
    "# Fit(Train) the Model\n",
    "\n",
    "# Compile the model with Optimizer, Loss Function and Metrics\n",
    "# Roc-Auc is not available in Keras as an off the shelf metric yet, so we will skip it here.\n",
    "\n",
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)\n",
    "# the fit function returns the run history. \n",
    "# It is very convenient, as it contains information about the model fit, iterations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 457us/step\n"
     ]
    }
   ],
   "source": [
    "## Like we did for the Random Forest, we generate two kinds of predictions\n",
    "#  One is a hard decision, the other is a probabilitistic score.\n",
    "\n",
    "#y_pred_class_nn_1 = model_1.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test_norm)\n",
    "y_pred_class_nn_1=np.argmax(y_pred_prob_nn_1,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the outputs to get a feel for how keras apis work.\n",
    "y_pred_class_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3617097 ],\n",
       "       [0.48226565],\n",
       "       [0.37115505],\n",
       "       [0.38737896],\n",
       "       [0.31091258],\n",
       "       [0.3819592 ],\n",
       "       [0.24529819],\n",
       "       [0.38427043],\n",
       "       [0.4694517 ],\n",
       "       [0.32004988]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.641\n",
      "roc-auc is 0.803\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print model performance and plot the roc curve\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_1, 'NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some variation in exact numbers due to randomness, but you should get results in the same ballpark as the Random Forest - between 75% and 85% accuracy, between .8 and .9 for AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `run_hist_1` object that was created, specifically its `history` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_hist_1.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training loss and the validation loss over the different epochs and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x288eb46d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the losses are still going down on both the training set and the validation set.  This suggests that the model might benefit from further training.  Let's train the model a little more and see what happens. Note that it will pick up from where it left off. Train for 1000 more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5740 - accuracy: 0.6684 - val_loss: 0.5810 - val_accuracy: 0.6719\n",
      "Epoch 2/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5736 - accuracy: 0.6684 - val_loss: 0.5806 - val_accuracy: 0.6719\n",
      "Epoch 3/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5732 - accuracy: 0.6684 - val_loss: 0.5802 - val_accuracy: 0.6771\n",
      "Epoch 4/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5727 - accuracy: 0.6701 - val_loss: 0.5798 - val_accuracy: 0.6771\n",
      "Epoch 5/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5723 - accuracy: 0.6701 - val_loss: 0.5794 - val_accuracy: 0.6771\n",
      "Epoch 6/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5719 - accuracy: 0.6701 - val_loss: 0.5789 - val_accuracy: 0.6771\n",
      "Epoch 7/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5714 - accuracy: 0.6701 - val_loss: 0.5785 - val_accuracy: 0.6771\n",
      "Epoch 8/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5710 - accuracy: 0.6684 - val_loss: 0.5781 - val_accuracy: 0.6771\n",
      "Epoch 9/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5706 - accuracy: 0.6667 - val_loss: 0.5777 - val_accuracy: 0.6771\n",
      "Epoch 10/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5702 - accuracy: 0.6684 - val_loss: 0.5773 - val_accuracy: 0.6771\n",
      "Epoch 11/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5698 - accuracy: 0.6684 - val_loss: 0.5769 - val_accuracy: 0.6771\n",
      "Epoch 12/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5694 - accuracy: 0.6701 - val_loss: 0.5765 - val_accuracy: 0.6823\n",
      "Epoch 13/1000\n",
      "18/18 [==============================] - 0s 995us/step - loss: 0.5689 - accuracy: 0.6684 - val_loss: 0.5761 - val_accuracy: 0.6823\n",
      "Epoch 14/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5685 - accuracy: 0.6701 - val_loss: 0.5757 - val_accuracy: 0.6823\n",
      "Epoch 15/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5681 - accuracy: 0.6701 - val_loss: 0.5753 - val_accuracy: 0.6875\n",
      "Epoch 16/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5676 - accuracy: 0.6701 - val_loss: 0.5749 - val_accuracy: 0.6875\n",
      "Epoch 17/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5672 - accuracy: 0.6719 - val_loss: 0.5744 - val_accuracy: 0.6875\n",
      "Epoch 18/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5668 - accuracy: 0.6719 - val_loss: 0.5740 - val_accuracy: 0.6875\n",
      "Epoch 19/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5664 - accuracy: 0.6719 - val_loss: 0.5736 - val_accuracy: 0.6875\n",
      "Epoch 20/1000\n",
      "18/18 [==============================] - 0s 978us/step - loss: 0.5660 - accuracy: 0.6736 - val_loss: 0.5732 - val_accuracy: 0.6875\n",
      "Epoch 21/1000\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.5656 - accuracy: 0.6719 - val_loss: 0.5729 - val_accuracy: 0.6927\n",
      "Epoch 22/1000\n",
      "18/18 [==============================] - 0s 979us/step - loss: 0.5652 - accuracy: 0.6719 - val_loss: 0.5725 - val_accuracy: 0.6927\n",
      "Epoch 23/1000\n",
      "18/18 [==============================] - 0s 988us/step - loss: 0.5648 - accuracy: 0.6771 - val_loss: 0.5721 - val_accuracy: 0.6927\n",
      "Epoch 24/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5644 - accuracy: 0.6771 - val_loss: 0.5717 - val_accuracy: 0.7031\n",
      "Epoch 25/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5640 - accuracy: 0.6771 - val_loss: 0.5713 - val_accuracy: 0.7083\n",
      "Epoch 26/1000\n",
      "18/18 [==============================] - 0s 988us/step - loss: 0.5636 - accuracy: 0.6788 - val_loss: 0.5709 - val_accuracy: 0.7083\n",
      "Epoch 27/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5631 - accuracy: 0.6788 - val_loss: 0.5705 - val_accuracy: 0.7083\n",
      "Epoch 28/1000\n",
      "18/18 [==============================] - 0s 943us/step - loss: 0.5628 - accuracy: 0.6806 - val_loss: 0.5701 - val_accuracy: 0.7083\n",
      "Epoch 29/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5624 - accuracy: 0.6823 - val_loss: 0.5697 - val_accuracy: 0.7083\n",
      "Epoch 30/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5619 - accuracy: 0.6823 - val_loss: 0.5693 - val_accuracy: 0.7135\n",
      "Epoch 31/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5616 - accuracy: 0.6823 - val_loss: 0.5689 - val_accuracy: 0.7188\n",
      "Epoch 32/1000\n",
      "18/18 [==============================] - 0s 969us/step - loss: 0.5611 - accuracy: 0.6823 - val_loss: 0.5686 - val_accuracy: 0.7188\n",
      "Epoch 33/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5608 - accuracy: 0.6823 - val_loss: 0.5682 - val_accuracy: 0.7240\n",
      "Epoch 34/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5603 - accuracy: 0.6823 - val_loss: 0.5678 - val_accuracy: 0.7240\n",
      "Epoch 35/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5600 - accuracy: 0.6840 - val_loss: 0.5674 - val_accuracy: 0.7240\n",
      "Epoch 36/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5596 - accuracy: 0.6840 - val_loss: 0.5670 - val_accuracy: 0.7240\n",
      "Epoch 37/1000\n",
      "18/18 [==============================] - 0s 933us/step - loss: 0.5592 - accuracy: 0.6875 - val_loss: 0.5666 - val_accuracy: 0.7240\n",
      "Epoch 38/1000\n",
      "18/18 [==============================] - 0s 986us/step - loss: 0.5588 - accuracy: 0.6875 - val_loss: 0.5663 - val_accuracy: 0.7240\n",
      "Epoch 39/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5584 - accuracy: 0.6875 - val_loss: 0.5659 - val_accuracy: 0.7240\n",
      "Epoch 40/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5580 - accuracy: 0.6892 - val_loss: 0.5655 - val_accuracy: 0.7240\n",
      "Epoch 41/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5576 - accuracy: 0.6875 - val_loss: 0.5651 - val_accuracy: 0.7240\n",
      "Epoch 42/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5572 - accuracy: 0.6892 - val_loss: 0.5648 - val_accuracy: 0.7240\n",
      "Epoch 43/1000\n",
      "18/18 [==============================] - 0s 974us/step - loss: 0.5568 - accuracy: 0.6910 - val_loss: 0.5644 - val_accuracy: 0.7240\n",
      "Epoch 44/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5564 - accuracy: 0.6910 - val_loss: 0.5640 - val_accuracy: 0.7240\n",
      "Epoch 45/1000\n",
      "18/18 [==============================] - 0s 987us/step - loss: 0.5561 - accuracy: 0.6944 - val_loss: 0.5637 - val_accuracy: 0.7240\n",
      "Epoch 46/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5557 - accuracy: 0.6944 - val_loss: 0.5633 - val_accuracy: 0.7240\n",
      "Epoch 47/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5553 - accuracy: 0.6944 - val_loss: 0.5629 - val_accuracy: 0.7292\n",
      "Epoch 48/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5549 - accuracy: 0.6979 - val_loss: 0.5626 - val_accuracy: 0.7292\n",
      "Epoch 49/1000\n",
      "18/18 [==============================] - 0s 952us/step - loss: 0.5545 - accuracy: 0.6997 - val_loss: 0.5622 - val_accuracy: 0.7292\n",
      "Epoch 50/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5542 - accuracy: 0.7014 - val_loss: 0.5618 - val_accuracy: 0.7292\n",
      "Epoch 51/1000\n",
      "18/18 [==============================] - 0s 994us/step - loss: 0.5538 - accuracy: 0.6997 - val_loss: 0.5615 - val_accuracy: 0.7292\n",
      "Epoch 52/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5534 - accuracy: 0.7014 - val_loss: 0.5611 - val_accuracy: 0.7292\n",
      "Epoch 53/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5530 - accuracy: 0.7014 - val_loss: 0.5607 - val_accuracy: 0.7292\n",
      "Epoch 54/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5526 - accuracy: 0.7014 - val_loss: 0.5604 - val_accuracy: 0.7292\n",
      "Epoch 55/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5523 - accuracy: 0.7031 - val_loss: 0.5600 - val_accuracy: 0.7292\n",
      "Epoch 56/1000\n",
      "18/18 [==============================] - 0s 985us/step - loss: 0.5519 - accuracy: 0.7014 - val_loss: 0.5597 - val_accuracy: 0.7292\n",
      "Epoch 57/1000\n",
      "18/18 [==============================] - 0s 980us/step - loss: 0.5515 - accuracy: 0.7014 - val_loss: 0.5593 - val_accuracy: 0.7292\n",
      "Epoch 58/1000\n",
      "18/18 [==============================] - 0s 982us/step - loss: 0.5511 - accuracy: 0.6997 - val_loss: 0.5590 - val_accuracy: 0.7292\n",
      "Epoch 59/1000\n",
      "18/18 [==============================] - 0s 978us/step - loss: 0.5508 - accuracy: 0.7014 - val_loss: 0.5586 - val_accuracy: 0.7292\n",
      "Epoch 60/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5504 - accuracy: 0.7014 - val_loss: 0.5583 - val_accuracy: 0.7292\n",
      "Epoch 61/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5501 - accuracy: 0.7066 - val_loss: 0.5579 - val_accuracy: 0.7292\n",
      "Epoch 62/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5497 - accuracy: 0.7049 - val_loss: 0.5576 - val_accuracy: 0.7292\n",
      "Epoch 63/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5493 - accuracy: 0.7049 - val_loss: 0.5572 - val_accuracy: 0.7292\n",
      "Epoch 64/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5490 - accuracy: 0.7083 - val_loss: 0.5569 - val_accuracy: 0.7292\n",
      "Epoch 65/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5486 - accuracy: 0.7083 - val_loss: 0.5565 - val_accuracy: 0.7292\n",
      "Epoch 66/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5483 - accuracy: 0.7083 - val_loss: 0.5562 - val_accuracy: 0.7292\n",
      "Epoch 67/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5479 - accuracy: 0.7083 - val_loss: 0.5558 - val_accuracy: 0.7292\n",
      "Epoch 68/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5475 - accuracy: 0.7083 - val_loss: 0.5555 - val_accuracy: 0.7292\n",
      "Epoch 69/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5472 - accuracy: 0.7083 - val_loss: 0.5551 - val_accuracy: 0.7292\n",
      "Epoch 70/1000\n",
      "18/18 [==============================] - 0s 985us/step - loss: 0.5468 - accuracy: 0.7083 - val_loss: 0.5548 - val_accuracy: 0.7292\n",
      "Epoch 71/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5464 - accuracy: 0.7083 - val_loss: 0.5545 - val_accuracy: 0.7292\n",
      "Epoch 72/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5461 - accuracy: 0.7101 - val_loss: 0.5541 - val_accuracy: 0.7240\n",
      "Epoch 73/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5457 - accuracy: 0.7101 - val_loss: 0.5538 - val_accuracy: 0.7240\n",
      "Epoch 74/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5454 - accuracy: 0.7066 - val_loss: 0.5534 - val_accuracy: 0.7240\n",
      "Epoch 75/1000\n",
      "18/18 [==============================] - 0s 999us/step - loss: 0.5450 - accuracy: 0.7066 - val_loss: 0.5531 - val_accuracy: 0.7240\n",
      "Epoch 76/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5447 - accuracy: 0.7066 - val_loss: 0.5528 - val_accuracy: 0.7240\n",
      "Epoch 77/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5443 - accuracy: 0.7066 - val_loss: 0.5525 - val_accuracy: 0.7240\n",
      "Epoch 78/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5440 - accuracy: 0.7066 - val_loss: 0.5521 - val_accuracy: 0.7240\n",
      "Epoch 79/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5436 - accuracy: 0.7049 - val_loss: 0.5518 - val_accuracy: 0.7292\n",
      "Epoch 80/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5433 - accuracy: 0.7083 - val_loss: 0.5515 - val_accuracy: 0.7292\n",
      "Epoch 81/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5429 - accuracy: 0.7083 - val_loss: 0.5511 - val_accuracy: 0.7344\n",
      "Epoch 82/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5426 - accuracy: 0.7101 - val_loss: 0.5508 - val_accuracy: 0.7344\n",
      "Epoch 83/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5423 - accuracy: 0.7101 - val_loss: 0.5505 - val_accuracy: 0.7344\n",
      "Epoch 84/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5419 - accuracy: 0.7101 - val_loss: 0.5502 - val_accuracy: 0.7344\n",
      "Epoch 85/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5416 - accuracy: 0.7135 - val_loss: 0.5498 - val_accuracy: 0.7344\n",
      "Epoch 86/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5412 - accuracy: 0.7135 - val_loss: 0.5495 - val_accuracy: 0.7344\n",
      "Epoch 87/1000\n",
      "18/18 [==============================] - 0s 991us/step - loss: 0.5409 - accuracy: 0.7118 - val_loss: 0.5492 - val_accuracy: 0.7344\n",
      "Epoch 88/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5406 - accuracy: 0.7135 - val_loss: 0.5489 - val_accuracy: 0.7344\n",
      "Epoch 89/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5402 - accuracy: 0.7153 - val_loss: 0.5486 - val_accuracy: 0.7396\n",
      "Epoch 90/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5399 - accuracy: 0.7170 - val_loss: 0.5482 - val_accuracy: 0.7396\n",
      "Epoch 91/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5395 - accuracy: 0.7170 - val_loss: 0.5479 - val_accuracy: 0.7396\n",
      "Epoch 92/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5392 - accuracy: 0.7170 - val_loss: 0.5476 - val_accuracy: 0.7344\n",
      "Epoch 93/1000\n",
      "18/18 [==============================] - 0s 975us/step - loss: 0.5389 - accuracy: 0.7188 - val_loss: 0.5473 - val_accuracy: 0.7344\n",
      "Epoch 94/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5386 - accuracy: 0.7170 - val_loss: 0.5470 - val_accuracy: 0.7344\n",
      "Epoch 95/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5382 - accuracy: 0.7188 - val_loss: 0.5467 - val_accuracy: 0.7344\n",
      "Epoch 96/1000\n",
      "18/18 [==============================] - 0s 973us/step - loss: 0.5379 - accuracy: 0.7222 - val_loss: 0.5464 - val_accuracy: 0.7344\n",
      "Epoch 97/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5376 - accuracy: 0.7240 - val_loss: 0.5461 - val_accuracy: 0.7344\n",
      "Epoch 98/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5372 - accuracy: 0.7274 - val_loss: 0.5458 - val_accuracy: 0.7344\n",
      "Epoch 99/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5369 - accuracy: 0.7274 - val_loss: 0.5455 - val_accuracy: 0.7396\n",
      "Epoch 100/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5366 - accuracy: 0.7292 - val_loss: 0.5451 - val_accuracy: 0.7396\n",
      "Epoch 101/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5363 - accuracy: 0.7309 - val_loss: 0.5448 - val_accuracy: 0.7396\n",
      "Epoch 102/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5360 - accuracy: 0.7309 - val_loss: 0.5445 - val_accuracy: 0.7396\n",
      "Epoch 103/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5356 - accuracy: 0.7309 - val_loss: 0.5442 - val_accuracy: 0.7396\n",
      "Epoch 104/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5353 - accuracy: 0.7309 - val_loss: 0.5439 - val_accuracy: 0.7396\n",
      "Epoch 105/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5350 - accuracy: 0.7344 - val_loss: 0.5436 - val_accuracy: 0.7448\n",
      "Epoch 106/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5347 - accuracy: 0.7344 - val_loss: 0.5433 - val_accuracy: 0.7500\n",
      "Epoch 107/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5344 - accuracy: 0.7361 - val_loss: 0.5431 - val_accuracy: 0.7500\n",
      "Epoch 108/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5341 - accuracy: 0.7361 - val_loss: 0.5428 - val_accuracy: 0.7500\n",
      "Epoch 109/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5337 - accuracy: 0.7361 - val_loss: 0.5425 - val_accuracy: 0.7500\n",
      "Epoch 110/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5334 - accuracy: 0.7361 - val_loss: 0.5422 - val_accuracy: 0.7552\n",
      "Epoch 111/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5331 - accuracy: 0.7396 - val_loss: 0.5419 - val_accuracy: 0.7552\n",
      "Epoch 112/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5328 - accuracy: 0.7378 - val_loss: 0.5416 - val_accuracy: 0.7552\n",
      "Epoch 113/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5325 - accuracy: 0.7378 - val_loss: 0.5413 - val_accuracy: 0.7552\n",
      "Epoch 114/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5322 - accuracy: 0.7396 - val_loss: 0.5410 - val_accuracy: 0.7552\n",
      "Epoch 115/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5319 - accuracy: 0.7396 - val_loss: 0.5407 - val_accuracy: 0.7552\n",
      "Epoch 116/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5316 - accuracy: 0.7396 - val_loss: 0.5404 - val_accuracy: 0.7552\n",
      "Epoch 117/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5313 - accuracy: 0.7413 - val_loss: 0.5402 - val_accuracy: 0.7552\n",
      "Epoch 118/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5310 - accuracy: 0.7413 - val_loss: 0.5399 - val_accuracy: 0.7552\n",
      "Epoch 119/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5307 - accuracy: 0.7413 - val_loss: 0.5396 - val_accuracy: 0.7552\n",
      "Epoch 120/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5303 - accuracy: 0.7413 - val_loss: 0.5393 - val_accuracy: 0.7552\n",
      "Epoch 121/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5301 - accuracy: 0.7413 - val_loss: 0.5390 - val_accuracy: 0.7552\n",
      "Epoch 122/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5297 - accuracy: 0.7413 - val_loss: 0.5388 - val_accuracy: 0.7552\n",
      "Epoch 123/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5295 - accuracy: 0.7413 - val_loss: 0.5385 - val_accuracy: 0.7552\n",
      "Epoch 124/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5292 - accuracy: 0.7448 - val_loss: 0.5382 - val_accuracy: 0.7552\n",
      "Epoch 125/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5289 - accuracy: 0.7448 - val_loss: 0.5379 - val_accuracy: 0.7552\n",
      "Epoch 126/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5286 - accuracy: 0.7448 - val_loss: 0.5376 - val_accuracy: 0.7552\n",
      "Epoch 127/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5283 - accuracy: 0.7448 - val_loss: 0.5374 - val_accuracy: 0.7552\n",
      "Epoch 128/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5280 - accuracy: 0.7448 - val_loss: 0.5371 - val_accuracy: 0.7552\n",
      "Epoch 129/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5277 - accuracy: 0.7448 - val_loss: 0.5368 - val_accuracy: 0.7604\n",
      "Epoch 130/1000\n",
      "18/18 [==============================] - 0s 989us/step - loss: 0.5274 - accuracy: 0.7448 - val_loss: 0.5366 - val_accuracy: 0.7604\n",
      "Epoch 131/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5271 - accuracy: 0.7448 - val_loss: 0.5363 - val_accuracy: 0.7604\n",
      "Epoch 132/1000\n",
      "18/18 [==============================] - 0s 991us/step - loss: 0.5268 - accuracy: 0.7448 - val_loss: 0.5360 - val_accuracy: 0.7604\n",
      "Epoch 133/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5265 - accuracy: 0.7448 - val_loss: 0.5358 - val_accuracy: 0.7604\n",
      "Epoch 134/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5263 - accuracy: 0.7448 - val_loss: 0.5355 - val_accuracy: 0.7604\n",
      "Epoch 135/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5260 - accuracy: 0.7448 - val_loss: 0.5352 - val_accuracy: 0.7604\n",
      "Epoch 136/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5257 - accuracy: 0.7448 - val_loss: 0.5350 - val_accuracy: 0.7604\n",
      "Epoch 137/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5254 - accuracy: 0.7465 - val_loss: 0.5347 - val_accuracy: 0.7604\n",
      "Epoch 138/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5251 - accuracy: 0.7465 - val_loss: 0.5344 - val_accuracy: 0.7604\n",
      "Epoch 139/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5249 - accuracy: 0.7465 - val_loss: 0.5342 - val_accuracy: 0.7604\n",
      "Epoch 140/1000\n",
      "18/18 [==============================] - 0s 996us/step - loss: 0.5246 - accuracy: 0.7448 - val_loss: 0.5339 - val_accuracy: 0.7604\n",
      "Epoch 141/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5243 - accuracy: 0.7465 - val_loss: 0.5337 - val_accuracy: 0.7604\n",
      "Epoch 142/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5240 - accuracy: 0.7483 - val_loss: 0.5334 - val_accuracy: 0.7604\n",
      "Epoch 143/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5237 - accuracy: 0.7500 - val_loss: 0.5332 - val_accuracy: 0.7604\n",
      "Epoch 144/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5234 - accuracy: 0.7500 - val_loss: 0.5329 - val_accuracy: 0.7604\n",
      "Epoch 145/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5231 - accuracy: 0.7535 - val_loss: 0.5327 - val_accuracy: 0.7604\n",
      "Epoch 146/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5229 - accuracy: 0.7535 - val_loss: 0.5324 - val_accuracy: 0.7604\n",
      "Epoch 147/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5226 - accuracy: 0.7535 - val_loss: 0.5322 - val_accuracy: 0.7604\n",
      "Epoch 148/1000\n",
      "18/18 [==============================] - 0s 961us/step - loss: 0.5223 - accuracy: 0.7535 - val_loss: 0.5319 - val_accuracy: 0.7604\n",
      "Epoch 149/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5221 - accuracy: 0.7552 - val_loss: 0.5317 - val_accuracy: 0.7604\n",
      "Epoch 150/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5218 - accuracy: 0.7552 - val_loss: 0.5314 - val_accuracy: 0.7604\n",
      "Epoch 151/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5215 - accuracy: 0.7517 - val_loss: 0.5312 - val_accuracy: 0.7604\n",
      "Epoch 152/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5212 - accuracy: 0.7569 - val_loss: 0.5309 - val_accuracy: 0.7604\n",
      "Epoch 153/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5210 - accuracy: 0.7552 - val_loss: 0.5307 - val_accuracy: 0.7604\n",
      "Epoch 154/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5207 - accuracy: 0.7535 - val_loss: 0.5304 - val_accuracy: 0.7604\n",
      "Epoch 155/1000\n",
      "18/18 [==============================] - 0s 993us/step - loss: 0.5205 - accuracy: 0.7535 - val_loss: 0.5302 - val_accuracy: 0.7604\n",
      "Epoch 156/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5202 - accuracy: 0.7517 - val_loss: 0.5299 - val_accuracy: 0.7552\n",
      "Epoch 157/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5199 - accuracy: 0.7517 - val_loss: 0.5297 - val_accuracy: 0.7604\n",
      "Epoch 158/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5197 - accuracy: 0.7535 - val_loss: 0.5295 - val_accuracy: 0.7552\n",
      "Epoch 159/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5194 - accuracy: 0.7552 - val_loss: 0.5292 - val_accuracy: 0.7552\n",
      "Epoch 160/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5191 - accuracy: 0.7552 - val_loss: 0.5290 - val_accuracy: 0.7552\n",
      "Epoch 161/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5189 - accuracy: 0.7552 - val_loss: 0.5288 - val_accuracy: 0.7500\n",
      "Epoch 162/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5186 - accuracy: 0.7552 - val_loss: 0.5285 - val_accuracy: 0.7500\n",
      "Epoch 163/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5184 - accuracy: 0.7552 - val_loss: 0.5283 - val_accuracy: 0.7500\n",
      "Epoch 164/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5182 - accuracy: 0.7552 - val_loss: 0.5281 - val_accuracy: 0.7500\n",
      "Epoch 165/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5179 - accuracy: 0.7552 - val_loss: 0.5278 - val_accuracy: 0.7552\n",
      "Epoch 166/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5176 - accuracy: 0.7552 - val_loss: 0.5276 - val_accuracy: 0.7604\n",
      "Epoch 167/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5174 - accuracy: 0.7552 - val_loss: 0.5274 - val_accuracy: 0.7604\n",
      "Epoch 168/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5171 - accuracy: 0.7552 - val_loss: 0.5271 - val_accuracy: 0.7604\n",
      "Epoch 169/1000\n",
      "18/18 [==============================] - 0s 945us/step - loss: 0.5168 - accuracy: 0.7535 - val_loss: 0.5269 - val_accuracy: 0.7604\n",
      "Epoch 170/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5166 - accuracy: 0.7535 - val_loss: 0.5267 - val_accuracy: 0.7604\n",
      "Epoch 171/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5164 - accuracy: 0.7535 - val_loss: 0.5265 - val_accuracy: 0.7604\n",
      "Epoch 172/1000\n",
      "18/18 [==============================] - 0s 990us/step - loss: 0.5161 - accuracy: 0.7535 - val_loss: 0.5262 - val_accuracy: 0.7604\n",
      "Epoch 173/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5159 - accuracy: 0.7535 - val_loss: 0.5260 - val_accuracy: 0.7604\n",
      "Epoch 174/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5156 - accuracy: 0.7535 - val_loss: 0.5258 - val_accuracy: 0.7604\n",
      "Epoch 175/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5154 - accuracy: 0.7535 - val_loss: 0.5256 - val_accuracy: 0.7604\n",
      "Epoch 176/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5151 - accuracy: 0.7535 - val_loss: 0.5254 - val_accuracy: 0.7604\n",
      "Epoch 177/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5149 - accuracy: 0.7552 - val_loss: 0.5251 - val_accuracy: 0.7552\n",
      "Epoch 178/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5146 - accuracy: 0.7552 - val_loss: 0.5249 - val_accuracy: 0.7604\n",
      "Epoch 179/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5144 - accuracy: 0.7552 - val_loss: 0.5247 - val_accuracy: 0.7604\n",
      "Epoch 180/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5141 - accuracy: 0.7569 - val_loss: 0.5245 - val_accuracy: 0.7604\n",
      "Epoch 181/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5139 - accuracy: 0.7569 - val_loss: 0.5243 - val_accuracy: 0.7656\n",
      "Epoch 182/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5137 - accuracy: 0.7552 - val_loss: 0.5241 - val_accuracy: 0.7656\n",
      "Epoch 183/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5134 - accuracy: 0.7569 - val_loss: 0.5238 - val_accuracy: 0.7656\n",
      "Epoch 184/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5132 - accuracy: 0.7569 - val_loss: 0.5236 - val_accuracy: 0.7656\n",
      "Epoch 185/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5130 - accuracy: 0.7622 - val_loss: 0.5234 - val_accuracy: 0.7656\n",
      "Epoch 186/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5127 - accuracy: 0.7604 - val_loss: 0.5232 - val_accuracy: 0.7708\n",
      "Epoch 187/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5125 - accuracy: 0.7604 - val_loss: 0.5230 - val_accuracy: 0.7708\n",
      "Epoch 188/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5123 - accuracy: 0.7569 - val_loss: 0.5228 - val_accuracy: 0.7708\n",
      "Epoch 189/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5120 - accuracy: 0.7569 - val_loss: 0.5226 - val_accuracy: 0.7708\n",
      "Epoch 190/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5118 - accuracy: 0.7622 - val_loss: 0.5224 - val_accuracy: 0.7708\n",
      "Epoch 191/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5116 - accuracy: 0.7569 - val_loss: 0.5222 - val_accuracy: 0.7708\n",
      "Epoch 192/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5113 - accuracy: 0.7569 - val_loss: 0.5220 - val_accuracy: 0.7708\n",
      "Epoch 193/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5111 - accuracy: 0.7552 - val_loss: 0.5218 - val_accuracy: 0.7708\n",
      "Epoch 194/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5109 - accuracy: 0.7552 - val_loss: 0.5216 - val_accuracy: 0.7708\n",
      "Epoch 195/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5107 - accuracy: 0.7552 - val_loss: 0.5214 - val_accuracy: 0.7760\n",
      "Epoch 196/1000\n",
      "18/18 [==============================] - 0s 949us/step - loss: 0.5105 - accuracy: 0.7552 - val_loss: 0.5212 - val_accuracy: 0.7760\n",
      "Epoch 197/1000\n",
      "18/18 [==============================] - 0s 998us/step - loss: 0.5102 - accuracy: 0.7552 - val_loss: 0.5210 - val_accuracy: 0.7708\n",
      "Epoch 198/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5100 - accuracy: 0.7587 - val_loss: 0.5208 - val_accuracy: 0.7708\n",
      "Epoch 199/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5098 - accuracy: 0.7552 - val_loss: 0.5206 - val_accuracy: 0.7708\n",
      "Epoch 200/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5095 - accuracy: 0.7587 - val_loss: 0.5204 - val_accuracy: 0.7708\n",
      "Epoch 201/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5093 - accuracy: 0.7587 - val_loss: 0.5202 - val_accuracy: 0.7708\n",
      "Epoch 202/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5091 - accuracy: 0.7587 - val_loss: 0.5200 - val_accuracy: 0.7708\n",
      "Epoch 203/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5089 - accuracy: 0.7587 - val_loss: 0.5198 - val_accuracy: 0.7708\n",
      "Epoch 204/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5087 - accuracy: 0.7552 - val_loss: 0.5196 - val_accuracy: 0.7708\n",
      "Epoch 205/1000\n",
      "18/18 [==============================] - 0s 977us/step - loss: 0.5085 - accuracy: 0.7569 - val_loss: 0.5194 - val_accuracy: 0.7708\n",
      "Epoch 206/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5082 - accuracy: 0.7569 - val_loss: 0.5192 - val_accuracy: 0.7708\n",
      "Epoch 207/1000\n",
      "18/18 [==============================] - 0s 999us/step - loss: 0.5080 - accuracy: 0.7552 - val_loss: 0.5190 - val_accuracy: 0.7708\n",
      "Epoch 208/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5078 - accuracy: 0.7552 - val_loss: 0.5189 - val_accuracy: 0.7708\n",
      "Epoch 209/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5076 - accuracy: 0.7569 - val_loss: 0.5187 - val_accuracy: 0.7708\n",
      "Epoch 210/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5074 - accuracy: 0.7569 - val_loss: 0.5185 - val_accuracy: 0.7656\n",
      "Epoch 211/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5072 - accuracy: 0.7587 - val_loss: 0.5183 - val_accuracy: 0.7708\n",
      "Epoch 212/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5070 - accuracy: 0.7587 - val_loss: 0.5181 - val_accuracy: 0.7708\n",
      "Epoch 213/1000\n",
      "18/18 [==============================] - 0s 966us/step - loss: 0.5068 - accuracy: 0.7604 - val_loss: 0.5179 - val_accuracy: 0.7708\n",
      "Epoch 214/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5065 - accuracy: 0.7604 - val_loss: 0.5177 - val_accuracy: 0.7708\n",
      "Epoch 215/1000\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.5063 - accuracy: 0.7604 - val_loss: 0.5176 - val_accuracy: 0.7708\n",
      "Epoch 216/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5061 - accuracy: 0.7604 - val_loss: 0.5174 - val_accuracy: 0.7708\n",
      "Epoch 217/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5059 - accuracy: 0.7604 - val_loss: 0.5172 - val_accuracy: 0.7708\n",
      "Epoch 218/1000\n",
      "18/18 [==============================] - 0s 957us/step - loss: 0.5057 - accuracy: 0.7604 - val_loss: 0.5170 - val_accuracy: 0.7708\n",
      "Epoch 219/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5055 - accuracy: 0.7604 - val_loss: 0.5168 - val_accuracy: 0.7708\n",
      "Epoch 220/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5053 - accuracy: 0.7604 - val_loss: 0.5167 - val_accuracy: 0.7708\n",
      "Epoch 221/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5051 - accuracy: 0.7604 - val_loss: 0.5165 - val_accuracy: 0.7760\n",
      "Epoch 222/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5049 - accuracy: 0.7587 - val_loss: 0.5163 - val_accuracy: 0.7760\n",
      "Epoch 223/1000\n",
      "18/18 [==============================] - 0s 960us/step - loss: 0.5047 - accuracy: 0.7569 - val_loss: 0.5161 - val_accuracy: 0.7760\n",
      "Epoch 224/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5045 - accuracy: 0.7569 - val_loss: 0.5160 - val_accuracy: 0.7812\n",
      "Epoch 225/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5043 - accuracy: 0.7569 - val_loss: 0.5158 - val_accuracy: 0.7812\n",
      "Epoch 226/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5041 - accuracy: 0.7569 - val_loss: 0.5156 - val_accuracy: 0.7812\n",
      "Epoch 227/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5039 - accuracy: 0.7569 - val_loss: 0.5154 - val_accuracy: 0.7812\n",
      "Epoch 228/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5037 - accuracy: 0.7569 - val_loss: 0.5153 - val_accuracy: 0.7812\n",
      "Epoch 229/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5035 - accuracy: 0.7569 - val_loss: 0.5151 - val_accuracy: 0.7812\n",
      "Epoch 230/1000\n",
      "18/18 [==============================] - 0s 986us/step - loss: 0.5033 - accuracy: 0.7569 - val_loss: 0.5149 - val_accuracy: 0.7812\n",
      "Epoch 231/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5031 - accuracy: 0.7569 - val_loss: 0.5148 - val_accuracy: 0.7812\n",
      "Epoch 232/1000\n",
      "18/18 [==============================] - 0s 999us/step - loss: 0.5029 - accuracy: 0.7569 - val_loss: 0.5146 - val_accuracy: 0.7760\n",
      "Epoch 233/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5027 - accuracy: 0.7569 - val_loss: 0.5144 - val_accuracy: 0.7760\n",
      "Epoch 234/1000\n",
      "18/18 [==============================] - 0s 946us/step - loss: 0.5025 - accuracy: 0.7569 - val_loss: 0.5143 - val_accuracy: 0.7760\n",
      "Epoch 235/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5023 - accuracy: 0.7569 - val_loss: 0.5141 - val_accuracy: 0.7760\n",
      "Epoch 236/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5022 - accuracy: 0.7569 - val_loss: 0.5139 - val_accuracy: 0.7760\n",
      "Epoch 237/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5019 - accuracy: 0.7569 - val_loss: 0.5138 - val_accuracy: 0.7812\n",
      "Epoch 238/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5018 - accuracy: 0.7569 - val_loss: 0.5136 - val_accuracy: 0.7812\n",
      "Epoch 239/1000\n",
      "18/18 [==============================] - 0s 936us/step - loss: 0.5016 - accuracy: 0.7569 - val_loss: 0.5134 - val_accuracy: 0.7812\n",
      "Epoch 240/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5014 - accuracy: 0.7587 - val_loss: 0.5133 - val_accuracy: 0.7812\n",
      "Epoch 241/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5012 - accuracy: 0.7587 - val_loss: 0.5131 - val_accuracy: 0.7812\n",
      "Epoch 242/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5010 - accuracy: 0.7604 - val_loss: 0.5130 - val_accuracy: 0.7812\n",
      "Epoch 243/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5008 - accuracy: 0.7604 - val_loss: 0.5128 - val_accuracy: 0.7812\n",
      "Epoch 244/1000\n",
      "18/18 [==============================] - 0s 955us/step - loss: 0.5007 - accuracy: 0.7587 - val_loss: 0.5127 - val_accuracy: 0.7812\n",
      "Epoch 245/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5004 - accuracy: 0.7587 - val_loss: 0.5125 - val_accuracy: 0.7812\n",
      "Epoch 246/1000\n",
      "18/18 [==============================] - 0s 994us/step - loss: 0.5003 - accuracy: 0.7604 - val_loss: 0.5123 - val_accuracy: 0.7812\n",
      "Epoch 247/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5001 - accuracy: 0.7622 - val_loss: 0.5122 - val_accuracy: 0.7812\n",
      "Epoch 248/1000\n",
      "18/18 [==============================] - 0s 989us/step - loss: 0.4999 - accuracy: 0.7639 - val_loss: 0.5120 - val_accuracy: 0.7812\n",
      "Epoch 249/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4997 - accuracy: 0.7639 - val_loss: 0.5119 - val_accuracy: 0.7812\n",
      "Epoch 250/1000\n",
      "18/18 [==============================] - 0s 937us/step - loss: 0.4995 - accuracy: 0.7622 - val_loss: 0.5117 - val_accuracy: 0.7812\n",
      "Epoch 251/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4994 - accuracy: 0.7639 - val_loss: 0.5116 - val_accuracy: 0.7812\n",
      "Epoch 252/1000\n",
      "18/18 [==============================] - 0s 995us/step - loss: 0.4992 - accuracy: 0.7639 - val_loss: 0.5114 - val_accuracy: 0.7812\n",
      "Epoch 253/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4990 - accuracy: 0.7639 - val_loss: 0.5113 - val_accuracy: 0.7812\n",
      "Epoch 254/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4989 - accuracy: 0.7622 - val_loss: 0.5111 - val_accuracy: 0.7812\n",
      "Epoch 255/1000\n",
      "18/18 [==============================] - 0s 935us/step - loss: 0.4987 - accuracy: 0.7639 - val_loss: 0.5110 - val_accuracy: 0.7812\n",
      "Epoch 256/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4985 - accuracy: 0.7639 - val_loss: 0.5108 - val_accuracy: 0.7812\n",
      "Epoch 257/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4983 - accuracy: 0.7622 - val_loss: 0.5107 - val_accuracy: 0.7812\n",
      "Epoch 258/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4981 - accuracy: 0.7622 - val_loss: 0.5105 - val_accuracy: 0.7812\n",
      "Epoch 259/1000\n",
      "18/18 [==============================] - 0s 975us/step - loss: 0.4980 - accuracy: 0.7622 - val_loss: 0.5104 - val_accuracy: 0.7812\n",
      "Epoch 260/1000\n",
      "18/18 [==============================] - 0s 976us/step - loss: 0.4978 - accuracy: 0.7622 - val_loss: 0.5102 - val_accuracy: 0.7812\n",
      "Epoch 261/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4976 - accuracy: 0.7622 - val_loss: 0.5101 - val_accuracy: 0.7812\n",
      "Epoch 262/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4974 - accuracy: 0.7622 - val_loss: 0.5100 - val_accuracy: 0.7812\n",
      "Epoch 263/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4973 - accuracy: 0.7622 - val_loss: 0.5098 - val_accuracy: 0.7812\n",
      "Epoch 264/1000\n",
      "18/18 [==============================] - 0s 925us/step - loss: 0.4971 - accuracy: 0.7622 - val_loss: 0.5097 - val_accuracy: 0.7760\n",
      "Epoch 265/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4969 - accuracy: 0.7622 - val_loss: 0.5095 - val_accuracy: 0.7760\n",
      "Epoch 266/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4968 - accuracy: 0.7639 - val_loss: 0.5094 - val_accuracy: 0.7760\n",
      "Epoch 267/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4966 - accuracy: 0.7656 - val_loss: 0.5093 - val_accuracy: 0.7760\n",
      "Epoch 268/1000\n",
      "18/18 [==============================] - 0s 994us/step - loss: 0.4964 - accuracy: 0.7656 - val_loss: 0.5091 - val_accuracy: 0.7760\n",
      "Epoch 269/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4963 - accuracy: 0.7656 - val_loss: 0.5090 - val_accuracy: 0.7760\n",
      "Epoch 270/1000\n",
      "18/18 [==============================] - 0s 948us/step - loss: 0.4961 - accuracy: 0.7656 - val_loss: 0.5088 - val_accuracy: 0.7760\n",
      "Epoch 271/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4959 - accuracy: 0.7656 - val_loss: 0.5087 - val_accuracy: 0.7760\n",
      "Epoch 272/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4958 - accuracy: 0.7656 - val_loss: 0.5086 - val_accuracy: 0.7760\n",
      "Epoch 273/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4956 - accuracy: 0.7656 - val_loss: 0.5084 - val_accuracy: 0.7760\n",
      "Epoch 274/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4954 - accuracy: 0.7656 - val_loss: 0.5083 - val_accuracy: 0.7760\n",
      "Epoch 275/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4953 - accuracy: 0.7656 - val_loss: 0.5082 - val_accuracy: 0.7760\n",
      "Epoch 276/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4951 - accuracy: 0.7639 - val_loss: 0.5080 - val_accuracy: 0.7760\n",
      "Epoch 277/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4950 - accuracy: 0.7656 - val_loss: 0.5079 - val_accuracy: 0.7760\n",
      "Epoch 278/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4948 - accuracy: 0.7656 - val_loss: 0.5078 - val_accuracy: 0.7760\n",
      "Epoch 279/1000\n",
      "18/18 [==============================] - 0s 999us/step - loss: 0.4946 - accuracy: 0.7639 - val_loss: 0.5076 - val_accuracy: 0.7760\n",
      "Epoch 280/1000\n",
      "18/18 [==============================] - 0s 942us/step - loss: 0.4945 - accuracy: 0.7639 - val_loss: 0.5075 - val_accuracy: 0.7760\n",
      "Epoch 281/1000\n",
      "18/18 [==============================] - 0s 996us/step - loss: 0.4943 - accuracy: 0.7656 - val_loss: 0.5074 - val_accuracy: 0.7760\n",
      "Epoch 282/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4942 - accuracy: 0.7656 - val_loss: 0.5072 - val_accuracy: 0.7760\n",
      "Epoch 283/1000\n",
      "18/18 [==============================] - 0s 980us/step - loss: 0.4940 - accuracy: 0.7639 - val_loss: 0.5071 - val_accuracy: 0.7760\n",
      "Epoch 284/1000\n",
      "18/18 [==============================] - 0s 999us/step - loss: 0.4938 - accuracy: 0.7639 - val_loss: 0.5070 - val_accuracy: 0.7760\n",
      "Epoch 285/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4937 - accuracy: 0.7656 - val_loss: 0.5069 - val_accuracy: 0.7760\n",
      "Epoch 286/1000\n",
      "18/18 [==============================] - 0s 930us/step - loss: 0.4936 - accuracy: 0.7674 - val_loss: 0.5067 - val_accuracy: 0.7760\n",
      "Epoch 287/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4934 - accuracy: 0.7656 - val_loss: 0.5066 - val_accuracy: 0.7760\n",
      "Epoch 288/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4932 - accuracy: 0.7674 - val_loss: 0.5065 - val_accuracy: 0.7760\n",
      "Epoch 289/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4931 - accuracy: 0.7674 - val_loss: 0.5064 - val_accuracy: 0.7760\n",
      "Epoch 290/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4929 - accuracy: 0.7674 - val_loss: 0.5062 - val_accuracy: 0.7760\n",
      "Epoch 291/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4928 - accuracy: 0.7674 - val_loss: 0.5061 - val_accuracy: 0.7760\n",
      "Epoch 292/1000\n",
      "18/18 [==============================] - 0s 978us/step - loss: 0.4926 - accuracy: 0.7691 - val_loss: 0.5060 - val_accuracy: 0.7760\n",
      "Epoch 293/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4925 - accuracy: 0.7691 - val_loss: 0.5059 - val_accuracy: 0.7760\n",
      "Epoch 294/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4923 - accuracy: 0.7691 - val_loss: 0.5057 - val_accuracy: 0.7760\n",
      "Epoch 295/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4922 - accuracy: 0.7691 - val_loss: 0.5056 - val_accuracy: 0.7760\n",
      "Epoch 296/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4920 - accuracy: 0.7691 - val_loss: 0.5055 - val_accuracy: 0.7760\n",
      "Epoch 297/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4919 - accuracy: 0.7691 - val_loss: 0.5054 - val_accuracy: 0.7760\n",
      "Epoch 298/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4917 - accuracy: 0.7691 - val_loss: 0.5053 - val_accuracy: 0.7760\n",
      "Epoch 299/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4916 - accuracy: 0.7691 - val_loss: 0.5051 - val_accuracy: 0.7760\n",
      "Epoch 300/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4914 - accuracy: 0.7691 - val_loss: 0.5050 - val_accuracy: 0.7760\n",
      "Epoch 301/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4913 - accuracy: 0.7691 - val_loss: 0.5049 - val_accuracy: 0.7760\n",
      "Epoch 302/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4911 - accuracy: 0.7691 - val_loss: 0.5048 - val_accuracy: 0.7760\n",
      "Epoch 303/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4910 - accuracy: 0.7691 - val_loss: 0.5047 - val_accuracy: 0.7760\n",
      "Epoch 304/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4909 - accuracy: 0.7691 - val_loss: 0.5046 - val_accuracy: 0.7760\n",
      "Epoch 305/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4907 - accuracy: 0.7691 - val_loss: 0.5045 - val_accuracy: 0.7760\n",
      "Epoch 306/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4906 - accuracy: 0.7691 - val_loss: 0.5043 - val_accuracy: 0.7760\n",
      "Epoch 307/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4904 - accuracy: 0.7691 - val_loss: 0.5042 - val_accuracy: 0.7760\n",
      "Epoch 308/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4903 - accuracy: 0.7691 - val_loss: 0.5041 - val_accuracy: 0.7760\n",
      "Epoch 309/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4901 - accuracy: 0.7691 - val_loss: 0.5040 - val_accuracy: 0.7760\n",
      "Epoch 310/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4900 - accuracy: 0.7691 - val_loss: 0.5039 - val_accuracy: 0.7760\n",
      "Epoch 311/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4899 - accuracy: 0.7691 - val_loss: 0.5038 - val_accuracy: 0.7760\n",
      "Epoch 312/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4897 - accuracy: 0.7691 - val_loss: 0.5037 - val_accuracy: 0.7760\n",
      "Epoch 313/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4896 - accuracy: 0.7691 - val_loss: 0.5036 - val_accuracy: 0.7760\n",
      "Epoch 314/1000\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.4895 - accuracy: 0.7691 - val_loss: 0.5035 - val_accuracy: 0.7760\n",
      "Epoch 315/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4893 - accuracy: 0.7656 - val_loss: 0.5033 - val_accuracy: 0.7760\n",
      "Epoch 316/1000\n",
      "18/18 [==============================] - 0s 998us/step - loss: 0.4892 - accuracy: 0.7691 - val_loss: 0.5032 - val_accuracy: 0.7760\n",
      "Epoch 317/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4891 - accuracy: 0.7674 - val_loss: 0.5031 - val_accuracy: 0.7760\n",
      "Epoch 318/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4889 - accuracy: 0.7656 - val_loss: 0.5030 - val_accuracy: 0.7760\n",
      "Epoch 319/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4888 - accuracy: 0.7674 - val_loss: 0.5029 - val_accuracy: 0.7760\n",
      "Epoch 320/1000\n",
      "18/18 [==============================] - 0s 999us/step - loss: 0.4886 - accuracy: 0.7674 - val_loss: 0.5028 - val_accuracy: 0.7760\n",
      "Epoch 321/1000\n",
      "18/18 [==============================] - 0s 991us/step - loss: 0.4885 - accuracy: 0.7691 - val_loss: 0.5027 - val_accuracy: 0.7760\n",
      "Epoch 322/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4884 - accuracy: 0.7674 - val_loss: 0.5026 - val_accuracy: 0.7760\n",
      "Epoch 323/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4882 - accuracy: 0.7674 - val_loss: 0.5025 - val_accuracy: 0.7760\n",
      "Epoch 324/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4881 - accuracy: 0.7691 - val_loss: 0.5024 - val_accuracy: 0.7760\n",
      "Epoch 325/1000\n",
      "18/18 [==============================] - 0s 982us/step - loss: 0.4880 - accuracy: 0.7674 - val_loss: 0.5023 - val_accuracy: 0.7760\n",
      "Epoch 326/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4879 - accuracy: 0.7674 - val_loss: 0.5022 - val_accuracy: 0.7760\n",
      "Epoch 327/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4877 - accuracy: 0.7674 - val_loss: 0.5021 - val_accuracy: 0.7760\n",
      "Epoch 328/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4876 - accuracy: 0.7674 - val_loss: 0.5020 - val_accuracy: 0.7760\n",
      "Epoch 329/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4875 - accuracy: 0.7674 - val_loss: 0.5019 - val_accuracy: 0.7760\n",
      "Epoch 330/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4873 - accuracy: 0.7674 - val_loss: 0.5018 - val_accuracy: 0.7760\n",
      "Epoch 331/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4872 - accuracy: 0.7656 - val_loss: 0.5017 - val_accuracy: 0.7760\n",
      "Epoch 332/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4871 - accuracy: 0.7674 - val_loss: 0.5016 - val_accuracy: 0.7760\n",
      "Epoch 333/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4869 - accuracy: 0.7674 - val_loss: 0.5015 - val_accuracy: 0.7760\n",
      "Epoch 334/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4868 - accuracy: 0.7674 - val_loss: 0.5014 - val_accuracy: 0.7760\n",
      "Epoch 335/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4867 - accuracy: 0.7656 - val_loss: 0.5013 - val_accuracy: 0.7760\n",
      "Epoch 336/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4866 - accuracy: 0.7656 - val_loss: 0.5012 - val_accuracy: 0.7760\n",
      "Epoch 337/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4865 - accuracy: 0.7656 - val_loss: 0.5011 - val_accuracy: 0.7760\n",
      "Epoch 338/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4863 - accuracy: 0.7656 - val_loss: 0.5010 - val_accuracy: 0.7760\n",
      "Epoch 339/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4862 - accuracy: 0.7656 - val_loss: 0.5009 - val_accuracy: 0.7760\n",
      "Epoch 340/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4861 - accuracy: 0.7656 - val_loss: 0.5008 - val_accuracy: 0.7760\n",
      "Epoch 341/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4860 - accuracy: 0.7656 - val_loss: 0.5007 - val_accuracy: 0.7760\n",
      "Epoch 342/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4858 - accuracy: 0.7656 - val_loss: 0.5006 - val_accuracy: 0.7760\n",
      "Epoch 343/1000\n",
      "18/18 [==============================] - 0s 1000us/step - loss: 0.4857 - accuracy: 0.7656 - val_loss: 0.5005 - val_accuracy: 0.7760\n",
      "Epoch 344/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4856 - accuracy: 0.7656 - val_loss: 0.5004 - val_accuracy: 0.7760\n",
      "Epoch 345/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4855 - accuracy: 0.7639 - val_loss: 0.5004 - val_accuracy: 0.7760\n",
      "Epoch 346/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4854 - accuracy: 0.7639 - val_loss: 0.5003 - val_accuracy: 0.7760\n",
      "Epoch 347/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4852 - accuracy: 0.7639 - val_loss: 0.5002 - val_accuracy: 0.7760\n",
      "Epoch 348/1000\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.4851 - accuracy: 0.7639 - val_loss: 0.5001 - val_accuracy: 0.7760\n",
      "Epoch 349/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4850 - accuracy: 0.7639 - val_loss: 0.5000 - val_accuracy: 0.7760\n",
      "Epoch 350/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4849 - accuracy: 0.7639 - val_loss: 0.4999 - val_accuracy: 0.7760\n",
      "Epoch 351/1000\n",
      "18/18 [==============================] - 0s 919us/step - loss: 0.4848 - accuracy: 0.7639 - val_loss: 0.4998 - val_accuracy: 0.7760\n",
      "Epoch 352/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4846 - accuracy: 0.7639 - val_loss: 0.4997 - val_accuracy: 0.7760\n",
      "Epoch 353/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4845 - accuracy: 0.7639 - val_loss: 0.4996 - val_accuracy: 0.7760\n",
      "Epoch 354/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4844 - accuracy: 0.7639 - val_loss: 0.4995 - val_accuracy: 0.7708\n",
      "Epoch 355/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4843 - accuracy: 0.7639 - val_loss: 0.4995 - val_accuracy: 0.7708\n",
      "Epoch 356/1000\n",
      "18/18 [==============================] - 0s 963us/step - loss: 0.4842 - accuracy: 0.7639 - val_loss: 0.4994 - val_accuracy: 0.7708\n",
      "Epoch 357/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4841 - accuracy: 0.7639 - val_loss: 0.4993 - val_accuracy: 0.7708\n",
      "Epoch 358/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4840 - accuracy: 0.7639 - val_loss: 0.4992 - val_accuracy: 0.7708\n",
      "Epoch 359/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4838 - accuracy: 0.7639 - val_loss: 0.4991 - val_accuracy: 0.7708\n",
      "Epoch 360/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4837 - accuracy: 0.7639 - val_loss: 0.4990 - val_accuracy: 0.7708\n",
      "Epoch 361/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4836 - accuracy: 0.7639 - val_loss: 0.4989 - val_accuracy: 0.7708\n",
      "Epoch 362/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4835 - accuracy: 0.7639 - val_loss: 0.4989 - val_accuracy: 0.7708\n",
      "Epoch 363/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4834 - accuracy: 0.7639 - val_loss: 0.4988 - val_accuracy: 0.7708\n",
      "Epoch 364/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4833 - accuracy: 0.7639 - val_loss: 0.4987 - val_accuracy: 0.7708\n",
      "Epoch 365/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4832 - accuracy: 0.7639 - val_loss: 0.4986 - val_accuracy: 0.7708\n",
      "Epoch 366/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4830 - accuracy: 0.7639 - val_loss: 0.4985 - val_accuracy: 0.7708\n",
      "Epoch 367/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4829 - accuracy: 0.7639 - val_loss: 0.4984 - val_accuracy: 0.7708\n",
      "Epoch 368/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4828 - accuracy: 0.7639 - val_loss: 0.4984 - val_accuracy: 0.7708\n",
      "Epoch 369/1000\n",
      "18/18 [==============================] - 0s 998us/step - loss: 0.4827 - accuracy: 0.7639 - val_loss: 0.4983 - val_accuracy: 0.7708\n",
      "Epoch 370/1000\n",
      "18/18 [==============================] - 0s 993us/step - loss: 0.4826 - accuracy: 0.7639 - val_loss: 0.4982 - val_accuracy: 0.7708\n",
      "Epoch 371/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4825 - accuracy: 0.7639 - val_loss: 0.4981 - val_accuracy: 0.7708\n",
      "Epoch 372/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4824 - accuracy: 0.7639 - val_loss: 0.4980 - val_accuracy: 0.7708\n",
      "Epoch 373/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4823 - accuracy: 0.7639 - val_loss: 0.4980 - val_accuracy: 0.7708\n",
      "Epoch 374/1000\n",
      "18/18 [==============================] - 0s 948us/step - loss: 0.4822 - accuracy: 0.7639 - val_loss: 0.4979 - val_accuracy: 0.7708\n",
      "Epoch 375/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4821 - accuracy: 0.7639 - val_loss: 0.4978 - val_accuracy: 0.7708\n",
      "Epoch 376/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4820 - accuracy: 0.7639 - val_loss: 0.4977 - val_accuracy: 0.7708\n",
      "Epoch 377/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4818 - accuracy: 0.7639 - val_loss: 0.4977 - val_accuracy: 0.7708\n",
      "Epoch 378/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4817 - accuracy: 0.7639 - val_loss: 0.4976 - val_accuracy: 0.7656\n",
      "Epoch 379/1000\n",
      "18/18 [==============================] - 0s 968us/step - loss: 0.4816 - accuracy: 0.7639 - val_loss: 0.4975 - val_accuracy: 0.7656\n",
      "Epoch 380/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4816 - accuracy: 0.7639 - val_loss: 0.4974 - val_accuracy: 0.7656\n",
      "Epoch 381/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4814 - accuracy: 0.7639 - val_loss: 0.4973 - val_accuracy: 0.7656\n",
      "Epoch 382/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4813 - accuracy: 0.7639 - val_loss: 0.4973 - val_accuracy: 0.7656\n",
      "Epoch 383/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4812 - accuracy: 0.7639 - val_loss: 0.4972 - val_accuracy: 0.7656\n",
      "Epoch 384/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4811 - accuracy: 0.7639 - val_loss: 0.4971 - val_accuracy: 0.7656\n",
      "Epoch 385/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4810 - accuracy: 0.7639 - val_loss: 0.4971 - val_accuracy: 0.7656\n",
      "Epoch 386/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4809 - accuracy: 0.7639 - val_loss: 0.4970 - val_accuracy: 0.7656\n",
      "Epoch 387/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4808 - accuracy: 0.7639 - val_loss: 0.4969 - val_accuracy: 0.7656\n",
      "Epoch 388/1000\n",
      "18/18 [==============================] - 0s 982us/step - loss: 0.4807 - accuracy: 0.7639 - val_loss: 0.4968 - val_accuracy: 0.7604\n",
      "Epoch 389/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4806 - accuracy: 0.7639 - val_loss: 0.4968 - val_accuracy: 0.7604\n",
      "Epoch 390/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4805 - accuracy: 0.7656 - val_loss: 0.4967 - val_accuracy: 0.7604\n",
      "Epoch 391/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4804 - accuracy: 0.7656 - val_loss: 0.4966 - val_accuracy: 0.7604\n",
      "Epoch 392/1000\n",
      "18/18 [==============================] - 0s 974us/step - loss: 0.4803 - accuracy: 0.7656 - val_loss: 0.4965 - val_accuracy: 0.7604\n",
      "Epoch 393/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4803 - accuracy: 0.7656 - val_loss: 0.4965 - val_accuracy: 0.7604\n",
      "Epoch 394/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4801 - accuracy: 0.7656 - val_loss: 0.4964 - val_accuracy: 0.7604\n",
      "Epoch 395/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4800 - accuracy: 0.7656 - val_loss: 0.4963 - val_accuracy: 0.7604\n",
      "Epoch 396/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4799 - accuracy: 0.7656 - val_loss: 0.4963 - val_accuracy: 0.7604\n",
      "Epoch 397/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4798 - accuracy: 0.7656 - val_loss: 0.4962 - val_accuracy: 0.7604\n",
      "Epoch 398/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4797 - accuracy: 0.7656 - val_loss: 0.4961 - val_accuracy: 0.7604\n",
      "Epoch 399/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4796 - accuracy: 0.7656 - val_loss: 0.4961 - val_accuracy: 0.7604\n",
      "Epoch 400/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4795 - accuracy: 0.7656 - val_loss: 0.4960 - val_accuracy: 0.7604\n",
      "Epoch 401/1000\n",
      "18/18 [==============================] - 0s 969us/step - loss: 0.4795 - accuracy: 0.7656 - val_loss: 0.4959 - val_accuracy: 0.7604\n",
      "Epoch 402/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4793 - accuracy: 0.7656 - val_loss: 0.4958 - val_accuracy: 0.7604\n",
      "Epoch 403/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4792 - accuracy: 0.7656 - val_loss: 0.4958 - val_accuracy: 0.7604\n",
      "Epoch 404/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4792 - accuracy: 0.7656 - val_loss: 0.4957 - val_accuracy: 0.7604\n",
      "Epoch 405/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4791 - accuracy: 0.7656 - val_loss: 0.4956 - val_accuracy: 0.7604\n",
      "Epoch 406/1000\n",
      "18/18 [==============================] - 0s 979us/step - loss: 0.4790 - accuracy: 0.7639 - val_loss: 0.4956 - val_accuracy: 0.7604\n",
      "Epoch 407/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4789 - accuracy: 0.7656 - val_loss: 0.4955 - val_accuracy: 0.7604\n",
      "Epoch 408/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4788 - accuracy: 0.7656 - val_loss: 0.4955 - val_accuracy: 0.7604\n",
      "Epoch 409/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4787 - accuracy: 0.7639 - val_loss: 0.4954 - val_accuracy: 0.7604\n",
      "Epoch 410/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4786 - accuracy: 0.7656 - val_loss: 0.4953 - val_accuracy: 0.7604\n",
      "Epoch 411/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4785 - accuracy: 0.7656 - val_loss: 0.4953 - val_accuracy: 0.7604\n",
      "Epoch 412/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4784 - accuracy: 0.7656 - val_loss: 0.4952 - val_accuracy: 0.7604\n",
      "Epoch 413/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4783 - accuracy: 0.7656 - val_loss: 0.4951 - val_accuracy: 0.7604\n",
      "Epoch 414/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4782 - accuracy: 0.7656 - val_loss: 0.4951 - val_accuracy: 0.7604\n",
      "Epoch 415/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4781 - accuracy: 0.7656 - val_loss: 0.4950 - val_accuracy: 0.7604\n",
      "Epoch 416/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4781 - accuracy: 0.7656 - val_loss: 0.4949 - val_accuracy: 0.7604\n",
      "Epoch 417/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4780 - accuracy: 0.7656 - val_loss: 0.4949 - val_accuracy: 0.7604\n",
      "Epoch 418/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4779 - accuracy: 0.7656 - val_loss: 0.4948 - val_accuracy: 0.7604\n",
      "Epoch 419/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4778 - accuracy: 0.7656 - val_loss: 0.4948 - val_accuracy: 0.7604\n",
      "Epoch 420/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4777 - accuracy: 0.7656 - val_loss: 0.4947 - val_accuracy: 0.7604\n",
      "Epoch 421/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4776 - accuracy: 0.7656 - val_loss: 0.4946 - val_accuracy: 0.7604\n",
      "Epoch 422/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4775 - accuracy: 0.7656 - val_loss: 0.4946 - val_accuracy: 0.7604\n",
      "Epoch 423/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4774 - accuracy: 0.7656 - val_loss: 0.4945 - val_accuracy: 0.7604\n",
      "Epoch 424/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4773 - accuracy: 0.7656 - val_loss: 0.4945 - val_accuracy: 0.7604\n",
      "Epoch 425/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4773 - accuracy: 0.7656 - val_loss: 0.4944 - val_accuracy: 0.7604\n",
      "Epoch 426/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4772 - accuracy: 0.7656 - val_loss: 0.4943 - val_accuracy: 0.7604\n",
      "Epoch 427/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4771 - accuracy: 0.7656 - val_loss: 0.4943 - val_accuracy: 0.7604\n",
      "Epoch 428/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4770 - accuracy: 0.7656 - val_loss: 0.4942 - val_accuracy: 0.7604\n",
      "Epoch 429/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4769 - accuracy: 0.7656 - val_loss: 0.4942 - val_accuracy: 0.7604\n",
      "Epoch 430/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4768 - accuracy: 0.7656 - val_loss: 0.4941 - val_accuracy: 0.7604\n",
      "Epoch 431/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4767 - accuracy: 0.7656 - val_loss: 0.4940 - val_accuracy: 0.7604\n",
      "Epoch 432/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4766 - accuracy: 0.7656 - val_loss: 0.4940 - val_accuracy: 0.7604\n",
      "Epoch 433/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4766 - accuracy: 0.7656 - val_loss: 0.4939 - val_accuracy: 0.7604\n",
      "Epoch 434/1000\n",
      "18/18 [==============================] - 0s 950us/step - loss: 0.4765 - accuracy: 0.7674 - val_loss: 0.4939 - val_accuracy: 0.7604\n",
      "Epoch 435/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4764 - accuracy: 0.7639 - val_loss: 0.4938 - val_accuracy: 0.7604\n",
      "Epoch 436/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4763 - accuracy: 0.7674 - val_loss: 0.4938 - val_accuracy: 0.7604\n",
      "Epoch 437/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4762 - accuracy: 0.7674 - val_loss: 0.4937 - val_accuracy: 0.7604\n",
      "Epoch 438/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4762 - accuracy: 0.7656 - val_loss: 0.4937 - val_accuracy: 0.7604\n",
      "Epoch 439/1000\n",
      "18/18 [==============================] - 0s 970us/step - loss: 0.4761 - accuracy: 0.7656 - val_loss: 0.4936 - val_accuracy: 0.7604\n",
      "Epoch 440/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4760 - accuracy: 0.7656 - val_loss: 0.4935 - val_accuracy: 0.7604\n",
      "Epoch 441/1000\n",
      "18/18 [==============================] - 0s 959us/step - loss: 0.4759 - accuracy: 0.7656 - val_loss: 0.4935 - val_accuracy: 0.7604\n",
      "Epoch 442/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4758 - accuracy: 0.7656 - val_loss: 0.4934 - val_accuracy: 0.7604\n",
      "Epoch 443/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4757 - accuracy: 0.7656 - val_loss: 0.4934 - val_accuracy: 0.7604\n",
      "Epoch 444/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4757 - accuracy: 0.7656 - val_loss: 0.4933 - val_accuracy: 0.7604\n",
      "Epoch 445/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4756 - accuracy: 0.7656 - val_loss: 0.4933 - val_accuracy: 0.7604\n",
      "Epoch 446/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4755 - accuracy: 0.7656 - val_loss: 0.4932 - val_accuracy: 0.7604\n",
      "Epoch 447/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4754 - accuracy: 0.7656 - val_loss: 0.4932 - val_accuracy: 0.7604\n",
      "Epoch 448/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4753 - accuracy: 0.7656 - val_loss: 0.4931 - val_accuracy: 0.7604\n",
      "Epoch 449/1000\n",
      "18/18 [==============================] - 0s 962us/step - loss: 0.4753 - accuracy: 0.7656 - val_loss: 0.4931 - val_accuracy: 0.7604\n",
      "Epoch 450/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4752 - accuracy: 0.7656 - val_loss: 0.4930 - val_accuracy: 0.7604\n",
      "Epoch 451/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4751 - accuracy: 0.7656 - val_loss: 0.4930 - val_accuracy: 0.7604\n",
      "Epoch 452/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4750 - accuracy: 0.7656 - val_loss: 0.4929 - val_accuracy: 0.7604\n",
      "Epoch 453/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4750 - accuracy: 0.7656 - val_loss: 0.4929 - val_accuracy: 0.7604\n",
      "Epoch 454/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4749 - accuracy: 0.7656 - val_loss: 0.4928 - val_accuracy: 0.7604\n",
      "Epoch 455/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4748 - accuracy: 0.7656 - val_loss: 0.4928 - val_accuracy: 0.7604\n",
      "Epoch 456/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4747 - accuracy: 0.7656 - val_loss: 0.4927 - val_accuracy: 0.7604\n",
      "Epoch 457/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4746 - accuracy: 0.7656 - val_loss: 0.4927 - val_accuracy: 0.7604\n",
      "Epoch 458/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4746 - accuracy: 0.7656 - val_loss: 0.4926 - val_accuracy: 0.7604\n",
      "Epoch 459/1000\n",
      "18/18 [==============================] - 0s 1000us/step - loss: 0.4745 - accuracy: 0.7656 - val_loss: 0.4926 - val_accuracy: 0.7604\n",
      "Epoch 460/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4744 - accuracy: 0.7656 - val_loss: 0.4925 - val_accuracy: 0.7604\n",
      "Epoch 461/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4743 - accuracy: 0.7656 - val_loss: 0.4925 - val_accuracy: 0.7604\n",
      "Epoch 462/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4742 - accuracy: 0.7656 - val_loss: 0.4924 - val_accuracy: 0.7604\n",
      "Epoch 463/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4742 - accuracy: 0.7674 - val_loss: 0.4924 - val_accuracy: 0.7604\n",
      "Epoch 464/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4741 - accuracy: 0.7656 - val_loss: 0.4923 - val_accuracy: 0.7604\n",
      "Epoch 465/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4741 - accuracy: 0.7674 - val_loss: 0.4923 - val_accuracy: 0.7604\n",
      "Epoch 466/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4740 - accuracy: 0.7656 - val_loss: 0.4922 - val_accuracy: 0.7604\n",
      "Epoch 467/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4739 - accuracy: 0.7674 - val_loss: 0.4922 - val_accuracy: 0.7552\n",
      "Epoch 468/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4738 - accuracy: 0.7656 - val_loss: 0.4921 - val_accuracy: 0.7552\n",
      "Epoch 469/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4737 - accuracy: 0.7656 - val_loss: 0.4921 - val_accuracy: 0.7552\n",
      "Epoch 470/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4737 - accuracy: 0.7656 - val_loss: 0.4920 - val_accuracy: 0.7552\n",
      "Epoch 471/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4736 - accuracy: 0.7674 - val_loss: 0.4920 - val_accuracy: 0.7552\n",
      "Epoch 472/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4735 - accuracy: 0.7674 - val_loss: 0.4919 - val_accuracy: 0.7552\n",
      "Epoch 473/1000\n",
      "18/18 [==============================] - 0s 958us/step - loss: 0.4735 - accuracy: 0.7674 - val_loss: 0.4919 - val_accuracy: 0.7552\n",
      "Epoch 474/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4734 - accuracy: 0.7674 - val_loss: 0.4919 - val_accuracy: 0.7552\n",
      "Epoch 475/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4733 - accuracy: 0.7674 - val_loss: 0.4918 - val_accuracy: 0.7552\n",
      "Epoch 476/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4732 - accuracy: 0.7674 - val_loss: 0.4918 - val_accuracy: 0.7552\n",
      "Epoch 477/1000\n",
      "18/18 [==============================] - 0s 940us/step - loss: 0.4732 - accuracy: 0.7674 - val_loss: 0.4917 - val_accuracy: 0.7552\n",
      "Epoch 478/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4731 - accuracy: 0.7674 - val_loss: 0.4917 - val_accuracy: 0.7552\n",
      "Epoch 479/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4730 - accuracy: 0.7674 - val_loss: 0.4916 - val_accuracy: 0.7552\n",
      "Epoch 480/1000\n",
      "18/18 [==============================] - 0s 992us/step - loss: 0.4730 - accuracy: 0.7674 - val_loss: 0.4916 - val_accuracy: 0.7552\n",
      "Epoch 481/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4729 - accuracy: 0.7674 - val_loss: 0.4915 - val_accuracy: 0.7552\n",
      "Epoch 482/1000\n",
      "18/18 [==============================] - 0s 965us/step - loss: 0.4728 - accuracy: 0.7691 - val_loss: 0.4915 - val_accuracy: 0.7552\n",
      "Epoch 483/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4727 - accuracy: 0.7674 - val_loss: 0.4915 - val_accuracy: 0.7552\n",
      "Epoch 484/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4727 - accuracy: 0.7691 - val_loss: 0.4914 - val_accuracy: 0.7552\n",
      "Epoch 485/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4726 - accuracy: 0.7691 - val_loss: 0.4914 - val_accuracy: 0.7552\n",
      "Epoch 486/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4726 - accuracy: 0.7691 - val_loss: 0.4913 - val_accuracy: 0.7552\n",
      "Epoch 487/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4725 - accuracy: 0.7691 - val_loss: 0.4913 - val_accuracy: 0.7552\n",
      "Epoch 488/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4724 - accuracy: 0.7691 - val_loss: 0.4912 - val_accuracy: 0.7552\n",
      "Epoch 489/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4724 - accuracy: 0.7691 - val_loss: 0.4912 - val_accuracy: 0.7552\n",
      "Epoch 490/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4723 - accuracy: 0.7691 - val_loss: 0.4912 - val_accuracy: 0.7552\n",
      "Epoch 491/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4722 - accuracy: 0.7691 - val_loss: 0.4911 - val_accuracy: 0.7552\n",
      "Epoch 492/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4722 - accuracy: 0.7691 - val_loss: 0.4911 - val_accuracy: 0.7552\n",
      "Epoch 493/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4720 - accuracy: 0.7691 - val_loss: 0.4910 - val_accuracy: 0.7552\n",
      "Epoch 494/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4720 - accuracy: 0.7656 - val_loss: 0.4910 - val_accuracy: 0.7552\n",
      "Epoch 495/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4720 - accuracy: 0.7656 - val_loss: 0.4910 - val_accuracy: 0.7552\n",
      "Epoch 496/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4719 - accuracy: 0.7691 - val_loss: 0.4909 - val_accuracy: 0.7552\n",
      "Epoch 497/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4718 - accuracy: 0.7656 - val_loss: 0.4909 - val_accuracy: 0.7552\n",
      "Epoch 498/1000\n",
      "18/18 [==============================] - 0s 983us/step - loss: 0.4717 - accuracy: 0.7656 - val_loss: 0.4908 - val_accuracy: 0.7552\n",
      "Epoch 499/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4717 - accuracy: 0.7656 - val_loss: 0.4908 - val_accuracy: 0.7552\n",
      "Epoch 500/1000\n",
      "18/18 [==============================] - 0s 989us/step - loss: 0.4716 - accuracy: 0.7656 - val_loss: 0.4908 - val_accuracy: 0.7552\n",
      "Epoch 501/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4715 - accuracy: 0.7656 - val_loss: 0.4907 - val_accuracy: 0.7552\n",
      "Epoch 502/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4715 - accuracy: 0.7656 - val_loss: 0.4907 - val_accuracy: 0.7552\n",
      "Epoch 503/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4714 - accuracy: 0.7656 - val_loss: 0.4906 - val_accuracy: 0.7552\n",
      "Epoch 504/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4714 - accuracy: 0.7656 - val_loss: 0.4906 - val_accuracy: 0.7552\n",
      "Epoch 505/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4713 - accuracy: 0.7656 - val_loss: 0.4906 - val_accuracy: 0.7500\n",
      "Epoch 506/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4712 - accuracy: 0.7656 - val_loss: 0.4905 - val_accuracy: 0.7500\n",
      "Epoch 507/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4712 - accuracy: 0.7656 - val_loss: 0.4905 - val_accuracy: 0.7500\n",
      "Epoch 508/1000\n",
      "18/18 [==============================] - 0s 979us/step - loss: 0.4711 - accuracy: 0.7656 - val_loss: 0.4905 - val_accuracy: 0.7500\n",
      "Epoch 509/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4710 - accuracy: 0.7656 - val_loss: 0.4904 - val_accuracy: 0.7500\n",
      "Epoch 510/1000\n",
      "18/18 [==============================] - 0s 976us/step - loss: 0.4710 - accuracy: 0.7656 - val_loss: 0.4904 - val_accuracy: 0.7500\n",
      "Epoch 511/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4709 - accuracy: 0.7656 - val_loss: 0.4903 - val_accuracy: 0.7500\n",
      "Epoch 512/1000\n",
      "18/18 [==============================] - 0s 981us/step - loss: 0.4709 - accuracy: 0.7656 - val_loss: 0.4903 - val_accuracy: 0.7500\n",
      "Epoch 513/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4708 - accuracy: 0.7656 - val_loss: 0.4903 - val_accuracy: 0.7500\n",
      "Epoch 514/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4707 - accuracy: 0.7656 - val_loss: 0.4902 - val_accuracy: 0.7500\n",
      "Epoch 515/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4707 - accuracy: 0.7656 - val_loss: 0.4902 - val_accuracy: 0.7500\n",
      "Epoch 516/1000\n",
      "18/18 [==============================] - 0s 960us/step - loss: 0.4706 - accuracy: 0.7656 - val_loss: 0.4902 - val_accuracy: 0.7500\n",
      "Epoch 517/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4705 - accuracy: 0.7656 - val_loss: 0.4901 - val_accuracy: 0.7500\n",
      "Epoch 518/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4705 - accuracy: 0.7656 - val_loss: 0.4901 - val_accuracy: 0.7500\n",
      "Epoch 519/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4704 - accuracy: 0.7656 - val_loss: 0.4901 - val_accuracy: 0.7500\n",
      "Epoch 520/1000\n",
      "18/18 [==============================] - 0s 993us/step - loss: 0.4704 - accuracy: 0.7656 - val_loss: 0.4900 - val_accuracy: 0.7500\n",
      "Epoch 521/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4703 - accuracy: 0.7656 - val_loss: 0.4900 - val_accuracy: 0.7500\n",
      "Epoch 522/1000\n",
      "18/18 [==============================] - 0s 982us/step - loss: 0.4702 - accuracy: 0.7656 - val_loss: 0.4900 - val_accuracy: 0.7500\n",
      "Epoch 523/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4702 - accuracy: 0.7656 - val_loss: 0.4899 - val_accuracy: 0.7552\n",
      "Epoch 524/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4701 - accuracy: 0.7656 - val_loss: 0.4899 - val_accuracy: 0.7552\n",
      "Epoch 525/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4701 - accuracy: 0.7656 - val_loss: 0.4899 - val_accuracy: 0.7552\n",
      "Epoch 526/1000\n",
      "18/18 [==============================] - 0s 992us/step - loss: 0.4700 - accuracy: 0.7656 - val_loss: 0.4898 - val_accuracy: 0.7552\n",
      "Epoch 527/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4699 - accuracy: 0.7656 - val_loss: 0.4898 - val_accuracy: 0.7552\n",
      "Epoch 528/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4699 - accuracy: 0.7656 - val_loss: 0.4898 - val_accuracy: 0.7552\n",
      "Epoch 529/1000\n",
      "18/18 [==============================] - 0s 987us/step - loss: 0.4698 - accuracy: 0.7656 - val_loss: 0.4897 - val_accuracy: 0.7552\n",
      "Epoch 530/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4698 - accuracy: 0.7656 - val_loss: 0.4897 - val_accuracy: 0.7552\n",
      "Epoch 531/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4697 - accuracy: 0.7656 - val_loss: 0.4897 - val_accuracy: 0.7552\n",
      "Epoch 532/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4696 - accuracy: 0.7656 - val_loss: 0.4896 - val_accuracy: 0.7552\n",
      "Epoch 533/1000\n",
      "18/18 [==============================] - 0s 965us/step - loss: 0.4696 - accuracy: 0.7656 - val_loss: 0.4896 - val_accuracy: 0.7552\n",
      "Epoch 534/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4695 - accuracy: 0.7656 - val_loss: 0.4896 - val_accuracy: 0.7552\n",
      "Epoch 535/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4695 - accuracy: 0.7656 - val_loss: 0.4895 - val_accuracy: 0.7552\n",
      "Epoch 536/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4694 - accuracy: 0.7656 - val_loss: 0.4895 - val_accuracy: 0.7552\n",
      "Epoch 537/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4694 - accuracy: 0.7656 - val_loss: 0.4895 - val_accuracy: 0.7552\n",
      "Epoch 538/1000\n",
      "18/18 [==============================] - 0s 995us/step - loss: 0.4693 - accuracy: 0.7656 - val_loss: 0.4894 - val_accuracy: 0.7552\n",
      "Epoch 539/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4692 - accuracy: 0.7656 - val_loss: 0.4894 - val_accuracy: 0.7552\n",
      "Epoch 540/1000\n",
      "18/18 [==============================] - 0s 994us/step - loss: 0.4692 - accuracy: 0.7656 - val_loss: 0.4894 - val_accuracy: 0.7552\n",
      "Epoch 541/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4691 - accuracy: 0.7656 - val_loss: 0.4893 - val_accuracy: 0.7552\n",
      "Epoch 542/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4691 - accuracy: 0.7656 - val_loss: 0.4893 - val_accuracy: 0.7552\n",
      "Epoch 543/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4690 - accuracy: 0.7656 - val_loss: 0.4893 - val_accuracy: 0.7552\n",
      "Epoch 544/1000\n",
      "18/18 [==============================] - 0s 915us/step - loss: 0.4690 - accuracy: 0.7656 - val_loss: 0.4892 - val_accuracy: 0.7552\n",
      "Epoch 545/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4689 - accuracy: 0.7656 - val_loss: 0.4892 - val_accuracy: 0.7552\n",
      "Epoch 546/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4689 - accuracy: 0.7656 - val_loss: 0.4892 - val_accuracy: 0.7552\n",
      "Epoch 547/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4688 - accuracy: 0.7656 - val_loss: 0.4891 - val_accuracy: 0.7552\n",
      "Epoch 548/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4687 - accuracy: 0.7656 - val_loss: 0.4891 - val_accuracy: 0.7552\n",
      "Epoch 549/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4687 - accuracy: 0.7656 - val_loss: 0.4891 - val_accuracy: 0.7552\n",
      "Epoch 550/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4686 - accuracy: 0.7656 - val_loss: 0.4891 - val_accuracy: 0.7552\n",
      "Epoch 551/1000\n",
      "18/18 [==============================] - 0s 961us/step - loss: 0.4686 - accuracy: 0.7674 - val_loss: 0.4890 - val_accuracy: 0.7552\n",
      "Epoch 552/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4685 - accuracy: 0.7656 - val_loss: 0.4890 - val_accuracy: 0.7552\n",
      "Epoch 553/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4685 - accuracy: 0.7656 - val_loss: 0.4890 - val_accuracy: 0.7552\n",
      "Epoch 554/1000\n",
      "18/18 [==============================] - 0s 986us/step - loss: 0.4684 - accuracy: 0.7656 - val_loss: 0.4889 - val_accuracy: 0.7552\n",
      "Epoch 555/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4684 - accuracy: 0.7656 - val_loss: 0.4889 - val_accuracy: 0.7552\n",
      "Epoch 556/1000\n",
      "18/18 [==============================] - 0s 974us/step - loss: 0.4683 - accuracy: 0.7674 - val_loss: 0.4889 - val_accuracy: 0.7552\n",
      "Epoch 557/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4683 - accuracy: 0.7674 - val_loss: 0.4889 - val_accuracy: 0.7552\n",
      "Epoch 558/1000\n",
      "18/18 [==============================] - 0s 938us/step - loss: 0.4682 - accuracy: 0.7674 - val_loss: 0.4888 - val_accuracy: 0.7552\n",
      "Epoch 559/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4682 - accuracy: 0.7691 - val_loss: 0.4888 - val_accuracy: 0.7552\n",
      "Epoch 560/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4681 - accuracy: 0.7691 - val_loss: 0.4888 - val_accuracy: 0.7552\n",
      "Epoch 561/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4681 - accuracy: 0.7691 - val_loss: 0.4887 - val_accuracy: 0.7552\n",
      "Epoch 562/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4680 - accuracy: 0.7691 - val_loss: 0.4887 - val_accuracy: 0.7552\n",
      "Epoch 563/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4680 - accuracy: 0.7691 - val_loss: 0.4887 - val_accuracy: 0.7552\n",
      "Epoch 564/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4679 - accuracy: 0.7691 - val_loss: 0.4887 - val_accuracy: 0.7552\n",
      "Epoch 565/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4679 - accuracy: 0.7691 - val_loss: 0.4886 - val_accuracy: 0.7552\n",
      "Epoch 566/1000\n",
      "18/18 [==============================] - 0s 970us/step - loss: 0.4678 - accuracy: 0.7691 - val_loss: 0.4886 - val_accuracy: 0.7552\n",
      "Epoch 567/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4678 - accuracy: 0.7691 - val_loss: 0.4886 - val_accuracy: 0.7552\n",
      "Epoch 568/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4677 - accuracy: 0.7691 - val_loss: 0.4886 - val_accuracy: 0.7552\n",
      "Epoch 569/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4677 - accuracy: 0.7691 - val_loss: 0.4885 - val_accuracy: 0.7552\n",
      "Epoch 570/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4676 - accuracy: 0.7691 - val_loss: 0.4885 - val_accuracy: 0.7552\n",
      "Epoch 571/1000\n",
      "18/18 [==============================] - 0s 969us/step - loss: 0.4676 - accuracy: 0.7691 - val_loss: 0.4885 - val_accuracy: 0.7552\n",
      "Epoch 572/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4675 - accuracy: 0.7691 - val_loss: 0.4884 - val_accuracy: 0.7552\n",
      "Epoch 573/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4674 - accuracy: 0.7691 - val_loss: 0.4884 - val_accuracy: 0.7552\n",
      "Epoch 574/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4674 - accuracy: 0.7691 - val_loss: 0.4884 - val_accuracy: 0.7552\n",
      "Epoch 575/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4674 - accuracy: 0.7691 - val_loss: 0.4884 - val_accuracy: 0.7552\n",
      "Epoch 576/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4673 - accuracy: 0.7691 - val_loss: 0.4883 - val_accuracy: 0.7552\n",
      "Epoch 577/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4673 - accuracy: 0.7691 - val_loss: 0.4883 - val_accuracy: 0.7552\n",
      "Epoch 578/1000\n",
      "18/18 [==============================] - 0s 994us/step - loss: 0.4672 - accuracy: 0.7691 - val_loss: 0.4883 - val_accuracy: 0.7552\n",
      "Epoch 579/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4672 - accuracy: 0.7691 - val_loss: 0.4883 - val_accuracy: 0.7552\n",
      "Epoch 580/1000\n",
      "18/18 [==============================] - 0s 978us/step - loss: 0.4671 - accuracy: 0.7691 - val_loss: 0.4882 - val_accuracy: 0.7552\n",
      "Epoch 581/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4670 - accuracy: 0.7691 - val_loss: 0.4882 - val_accuracy: 0.7552\n",
      "Epoch 582/1000\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.4670 - accuracy: 0.7691 - val_loss: 0.4882 - val_accuracy: 0.7552\n",
      "Epoch 583/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4670 - accuracy: 0.7691 - val_loss: 0.4882 - val_accuracy: 0.7552\n",
      "Epoch 584/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4669 - accuracy: 0.7691 - val_loss: 0.4881 - val_accuracy: 0.7552\n",
      "Epoch 585/1000\n",
      "18/18 [==============================] - 0s 971us/step - loss: 0.4669 - accuracy: 0.7691 - val_loss: 0.4881 - val_accuracy: 0.7552\n",
      "Epoch 586/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4668 - accuracy: 0.7691 - val_loss: 0.4881 - val_accuracy: 0.7552\n",
      "Epoch 587/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4668 - accuracy: 0.7691 - val_loss: 0.4881 - val_accuracy: 0.7552\n",
      "Epoch 588/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4667 - accuracy: 0.7691 - val_loss: 0.4880 - val_accuracy: 0.7552\n",
      "Epoch 589/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4667 - accuracy: 0.7691 - val_loss: 0.4880 - val_accuracy: 0.7552\n",
      "Epoch 590/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4666 - accuracy: 0.7691 - val_loss: 0.4880 - val_accuracy: 0.7552\n",
      "Epoch 591/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4666 - accuracy: 0.7691 - val_loss: 0.4880 - val_accuracy: 0.7552\n",
      "Epoch 592/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4665 - accuracy: 0.7691 - val_loss: 0.4880 - val_accuracy: 0.7552\n",
      "Epoch 593/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4665 - accuracy: 0.7691 - val_loss: 0.4879 - val_accuracy: 0.7552\n",
      "Epoch 594/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4664 - accuracy: 0.7691 - val_loss: 0.4879 - val_accuracy: 0.7552\n",
      "Epoch 595/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4664 - accuracy: 0.7691 - val_loss: 0.4879 - val_accuracy: 0.7552\n",
      "Epoch 596/1000\n",
      "18/18 [==============================] - 0s 947us/step - loss: 0.4663 - accuracy: 0.7691 - val_loss: 0.4879 - val_accuracy: 0.7552\n",
      "Epoch 597/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4663 - accuracy: 0.7691 - val_loss: 0.4878 - val_accuracy: 0.7552\n",
      "Epoch 598/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4663 - accuracy: 0.7691 - val_loss: 0.4878 - val_accuracy: 0.7552\n",
      "Epoch 599/1000\n",
      "18/18 [==============================] - 0s 969us/step - loss: 0.4662 - accuracy: 0.7691 - val_loss: 0.4878 - val_accuracy: 0.7552\n",
      "Epoch 600/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4662 - accuracy: 0.7691 - val_loss: 0.4878 - val_accuracy: 0.7552\n",
      "Epoch 601/1000\n",
      "18/18 [==============================] - 0s 990us/step - loss: 0.4661 - accuracy: 0.7691 - val_loss: 0.4878 - val_accuracy: 0.7552\n",
      "Epoch 602/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4661 - accuracy: 0.7691 - val_loss: 0.4877 - val_accuracy: 0.7552\n",
      "Epoch 603/1000\n",
      "18/18 [==============================] - 0s 916us/step - loss: 0.4660 - accuracy: 0.7691 - val_loss: 0.4877 - val_accuracy: 0.7552\n",
      "Epoch 604/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4660 - accuracy: 0.7691 - val_loss: 0.4877 - val_accuracy: 0.7552\n",
      "Epoch 605/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4660 - accuracy: 0.7691 - val_loss: 0.4877 - val_accuracy: 0.7552\n",
      "Epoch 606/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4659 - accuracy: 0.7691 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 607/1000\n",
      "18/18 [==============================] - 0s 957us/step - loss: 0.4659 - accuracy: 0.7691 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 608/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4658 - accuracy: 0.7691 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 609/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4658 - accuracy: 0.7691 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 610/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4657 - accuracy: 0.7708 - val_loss: 0.4876 - val_accuracy: 0.7552\n",
      "Epoch 611/1000\n",
      "18/18 [==============================] - 0s 928us/step - loss: 0.4657 - accuracy: 0.7691 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 612/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4657 - accuracy: 0.7691 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 613/1000\n",
      "18/18 [==============================] - 0s 999us/step - loss: 0.4656 - accuracy: 0.7691 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 614/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4656 - accuracy: 0.7691 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 615/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4655 - accuracy: 0.7691 - val_loss: 0.4875 - val_accuracy: 0.7552\n",
      "Epoch 616/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4655 - accuracy: 0.7691 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 617/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4654 - accuracy: 0.7691 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 618/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4654 - accuracy: 0.7691 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 619/1000\n",
      "18/18 [==============================] - 0s 985us/step - loss: 0.4653 - accuracy: 0.7691 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 620/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4653 - accuracy: 0.7691 - val_loss: 0.4874 - val_accuracy: 0.7552\n",
      "Epoch 621/1000\n",
      "18/18 [==============================] - 0s 989us/step - loss: 0.4653 - accuracy: 0.7708 - val_loss: 0.4873 - val_accuracy: 0.7552\n",
      "Epoch 622/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4652 - accuracy: 0.7708 - val_loss: 0.4873 - val_accuracy: 0.7552\n",
      "Epoch 623/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4652 - accuracy: 0.7708 - val_loss: 0.4873 - val_accuracy: 0.7552\n",
      "Epoch 624/1000\n",
      "18/18 [==============================] - 0s 954us/step - loss: 0.4651 - accuracy: 0.7691 - val_loss: 0.4873 - val_accuracy: 0.7552\n",
      "Epoch 625/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4651 - accuracy: 0.7708 - val_loss: 0.4873 - val_accuracy: 0.7552\n",
      "Epoch 626/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4651 - accuracy: 0.7708 - val_loss: 0.4872 - val_accuracy: 0.7552\n",
      "Epoch 627/1000\n",
      "18/18 [==============================] - 0s 946us/step - loss: 0.4650 - accuracy: 0.7708 - val_loss: 0.4872 - val_accuracy: 0.7552\n",
      "Epoch 628/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4650 - accuracy: 0.7708 - val_loss: 0.4872 - val_accuracy: 0.7552\n",
      "Epoch 629/1000\n",
      "18/18 [==============================] - 0s 991us/step - loss: 0.4649 - accuracy: 0.7708 - val_loss: 0.4872 - val_accuracy: 0.7552\n",
      "Epoch 630/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4649 - accuracy: 0.7708 - val_loss: 0.4872 - val_accuracy: 0.7552\n",
      "Epoch 631/1000\n",
      "18/18 [==============================] - 0s 974us/step - loss: 0.4649 - accuracy: 0.7708 - val_loss: 0.4871 - val_accuracy: 0.7552\n",
      "Epoch 632/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4648 - accuracy: 0.7708 - val_loss: 0.4871 - val_accuracy: 0.7552\n",
      "Epoch 633/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4648 - accuracy: 0.7708 - val_loss: 0.4871 - val_accuracy: 0.7552\n",
      "Epoch 634/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4647 - accuracy: 0.7726 - val_loss: 0.4871 - val_accuracy: 0.7552\n",
      "Epoch 635/1000\n",
      "18/18 [==============================] - 0s 955us/step - loss: 0.4647 - accuracy: 0.7708 - val_loss: 0.4871 - val_accuracy: 0.7552\n",
      "Epoch 636/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4646 - accuracy: 0.7726 - val_loss: 0.4871 - val_accuracy: 0.7552\n",
      "Epoch 637/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4646 - accuracy: 0.7708 - val_loss: 0.4870 - val_accuracy: 0.7552\n",
      "Epoch 638/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4646 - accuracy: 0.7708 - val_loss: 0.4870 - val_accuracy: 0.7552\n",
      "Epoch 639/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4645 - accuracy: 0.7726 - val_loss: 0.4870 - val_accuracy: 0.7552\n",
      "Epoch 640/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4645 - accuracy: 0.7708 - val_loss: 0.4870 - val_accuracy: 0.7552\n",
      "Epoch 641/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4645 - accuracy: 0.7708 - val_loss: 0.4870 - val_accuracy: 0.7552\n",
      "Epoch 642/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4644 - accuracy: 0.7726 - val_loss: 0.4869 - val_accuracy: 0.7552\n",
      "Epoch 643/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4644 - accuracy: 0.7691 - val_loss: 0.4869 - val_accuracy: 0.7552\n",
      "Epoch 644/1000\n",
      "18/18 [==============================] - 0s 953us/step - loss: 0.4643 - accuracy: 0.7726 - val_loss: 0.4869 - val_accuracy: 0.7552\n",
      "Epoch 645/1000\n",
      "18/18 [==============================] - 0s 996us/step - loss: 0.4643 - accuracy: 0.7726 - val_loss: 0.4869 - val_accuracy: 0.7552\n",
      "Epoch 646/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4643 - accuracy: 0.7726 - val_loss: 0.4869 - val_accuracy: 0.7552\n",
      "Epoch 647/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4642 - accuracy: 0.7708 - val_loss: 0.4869 - val_accuracy: 0.7552\n",
      "Epoch 648/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4642 - accuracy: 0.7726 - val_loss: 0.4868 - val_accuracy: 0.7552\n",
      "Epoch 649/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4642 - accuracy: 0.7726 - val_loss: 0.4868 - val_accuracy: 0.7552\n",
      "Epoch 650/1000\n",
      "18/18 [==============================] - 0s 934us/step - loss: 0.4641 - accuracy: 0.7726 - val_loss: 0.4868 - val_accuracy: 0.7552\n",
      "Epoch 651/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4641 - accuracy: 0.7726 - val_loss: 0.4868 - val_accuracy: 0.7552\n",
      "Epoch 652/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4640 - accuracy: 0.7726 - val_loss: 0.4868 - val_accuracy: 0.7552\n",
      "Epoch 653/1000\n",
      "18/18 [==============================] - 0s 946us/step - loss: 0.4640 - accuracy: 0.7726 - val_loss: 0.4868 - val_accuracy: 0.7552\n",
      "Epoch 654/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4640 - accuracy: 0.7726 - val_loss: 0.4867 - val_accuracy: 0.7552\n",
      "Epoch 655/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4639 - accuracy: 0.7726 - val_loss: 0.4867 - val_accuracy: 0.7552\n",
      "Epoch 656/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4639 - accuracy: 0.7726 - val_loss: 0.4867 - val_accuracy: 0.7552\n",
      "Epoch 657/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4638 - accuracy: 0.7708 - val_loss: 0.4867 - val_accuracy: 0.7552\n",
      "Epoch 658/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4638 - accuracy: 0.7708 - val_loss: 0.4867 - val_accuracy: 0.7552\n",
      "Epoch 659/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4638 - accuracy: 0.7726 - val_loss: 0.4867 - val_accuracy: 0.7552\n",
      "Epoch 660/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4637 - accuracy: 0.7726 - val_loss: 0.4866 - val_accuracy: 0.7552\n",
      "Epoch 661/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4637 - accuracy: 0.7726 - val_loss: 0.4866 - val_accuracy: 0.7552\n",
      "Epoch 662/1000\n",
      "18/18 [==============================] - 0s 969us/step - loss: 0.4637 - accuracy: 0.7726 - val_loss: 0.4866 - val_accuracy: 0.7552\n",
      "Epoch 663/1000\n",
      "18/18 [==============================] - 0s 996us/step - loss: 0.4636 - accuracy: 0.7726 - val_loss: 0.4866 - val_accuracy: 0.7552\n",
      "Epoch 664/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4636 - accuracy: 0.7708 - val_loss: 0.4866 - val_accuracy: 0.7552\n",
      "Epoch 665/1000\n",
      "18/18 [==============================] - 0s 942us/step - loss: 0.4636 - accuracy: 0.7726 - val_loss: 0.4866 - val_accuracy: 0.7552\n",
      "Epoch 666/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4635 - accuracy: 0.7726 - val_loss: 0.4866 - val_accuracy: 0.7552\n",
      "Epoch 667/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4635 - accuracy: 0.7726 - val_loss: 0.4865 - val_accuracy: 0.7552\n",
      "Epoch 668/1000\n",
      "18/18 [==============================] - 0s 965us/step - loss: 0.4634 - accuracy: 0.7726 - val_loss: 0.4865 - val_accuracy: 0.7552\n",
      "Epoch 669/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4634 - accuracy: 0.7708 - val_loss: 0.4865 - val_accuracy: 0.7552\n",
      "Epoch 670/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4634 - accuracy: 0.7726 - val_loss: 0.4865 - val_accuracy: 0.7552\n",
      "Epoch 671/1000\n",
      "18/18 [==============================] - 0s 934us/step - loss: 0.4633 - accuracy: 0.7708 - val_loss: 0.4865 - val_accuracy: 0.7552\n",
      "Epoch 672/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4633 - accuracy: 0.7726 - val_loss: 0.4865 - val_accuracy: 0.7552\n",
      "Epoch 673/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4633 - accuracy: 0.7708 - val_loss: 0.4865 - val_accuracy: 0.7552\n",
      "Epoch 674/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4632 - accuracy: 0.7708 - val_loss: 0.4864 - val_accuracy: 0.7552\n",
      "Epoch 675/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4632 - accuracy: 0.7708 - val_loss: 0.4864 - val_accuracy: 0.7552\n",
      "Epoch 676/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4631 - accuracy: 0.7708 - val_loss: 0.4864 - val_accuracy: 0.7552\n",
      "Epoch 677/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4631 - accuracy: 0.7708 - val_loss: 0.4864 - val_accuracy: 0.7552\n",
      "Epoch 678/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4631 - accuracy: 0.7708 - val_loss: 0.4864 - val_accuracy: 0.7552\n",
      "Epoch 679/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4631 - accuracy: 0.7708 - val_loss: 0.4864 - val_accuracy: 0.7552\n",
      "Epoch 680/1000\n",
      "18/18 [==============================] - 0s 992us/step - loss: 0.4630 - accuracy: 0.7708 - val_loss: 0.4864 - val_accuracy: 0.7552\n",
      "Epoch 681/1000\n",
      "18/18 [==============================] - 0s 998us/step - loss: 0.4630 - accuracy: 0.7708 - val_loss: 0.4863 - val_accuracy: 0.7552\n",
      "Epoch 682/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4630 - accuracy: 0.7708 - val_loss: 0.4863 - val_accuracy: 0.7552\n",
      "Epoch 683/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4629 - accuracy: 0.7708 - val_loss: 0.4863 - val_accuracy: 0.7552\n",
      "Epoch 684/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4629 - accuracy: 0.7708 - val_loss: 0.4863 - val_accuracy: 0.7552\n",
      "Epoch 685/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4629 - accuracy: 0.7708 - val_loss: 0.4863 - val_accuracy: 0.7552\n",
      "Epoch 686/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4628 - accuracy: 0.7708 - val_loss: 0.4863 - val_accuracy: 0.7552\n",
      "Epoch 687/1000\n",
      "18/18 [==============================] - 0s 981us/step - loss: 0.4628 - accuracy: 0.7708 - val_loss: 0.4863 - val_accuracy: 0.7552\n",
      "Epoch 688/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4628 - accuracy: 0.7708 - val_loss: 0.4862 - val_accuracy: 0.7552\n",
      "Epoch 689/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4627 - accuracy: 0.7708 - val_loss: 0.4862 - val_accuracy: 0.7552\n",
      "Epoch 690/1000\n",
      "18/18 [==============================] - 0s 979us/step - loss: 0.4627 - accuracy: 0.7708 - val_loss: 0.4862 - val_accuracy: 0.7552\n",
      "Epoch 691/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4626 - accuracy: 0.7708 - val_loss: 0.4862 - val_accuracy: 0.7552\n",
      "Epoch 692/1000\n",
      "18/18 [==============================] - 0s 985us/step - loss: 0.4626 - accuracy: 0.7708 - val_loss: 0.4862 - val_accuracy: 0.7552\n",
      "Epoch 693/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4626 - accuracy: 0.7708 - val_loss: 0.4862 - val_accuracy: 0.7552\n",
      "Epoch 694/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4625 - accuracy: 0.7708 - val_loss: 0.4862 - val_accuracy: 0.7552\n",
      "Epoch 695/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4625 - accuracy: 0.7708 - val_loss: 0.4862 - val_accuracy: 0.7552\n",
      "Epoch 696/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4625 - accuracy: 0.7708 - val_loss: 0.4861 - val_accuracy: 0.7552\n",
      "Epoch 697/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4625 - accuracy: 0.7708 - val_loss: 0.4861 - val_accuracy: 0.7552\n",
      "Epoch 698/1000\n",
      "18/18 [==============================] - 0s 956us/step - loss: 0.4624 - accuracy: 0.7708 - val_loss: 0.4861 - val_accuracy: 0.7552\n",
      "Epoch 699/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4624 - accuracy: 0.7708 - val_loss: 0.4861 - val_accuracy: 0.7552\n",
      "Epoch 700/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4623 - accuracy: 0.7708 - val_loss: 0.4861 - val_accuracy: 0.7552\n",
      "Epoch 701/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4623 - accuracy: 0.7708 - val_loss: 0.4861 - val_accuracy: 0.7552\n",
      "Epoch 702/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4623 - accuracy: 0.7708 - val_loss: 0.4861 - val_accuracy: 0.7552\n",
      "Epoch 703/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4623 - accuracy: 0.7708 - val_loss: 0.4861 - val_accuracy: 0.7552\n",
      "Epoch 704/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4622 - accuracy: 0.7708 - val_loss: 0.4860 - val_accuracy: 0.7552\n",
      "Epoch 705/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4622 - accuracy: 0.7708 - val_loss: 0.4860 - val_accuracy: 0.7552\n",
      "Epoch 706/1000\n",
      "18/18 [==============================] - 0s 980us/step - loss: 0.4622 - accuracy: 0.7708 - val_loss: 0.4860 - val_accuracy: 0.7552\n",
      "Epoch 707/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4621 - accuracy: 0.7708 - val_loss: 0.4860 - val_accuracy: 0.7552\n",
      "Epoch 708/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4621 - accuracy: 0.7708 - val_loss: 0.4860 - val_accuracy: 0.7552\n",
      "Epoch 709/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4621 - accuracy: 0.7708 - val_loss: 0.4860 - val_accuracy: 0.7552\n",
      "Epoch 710/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4620 - accuracy: 0.7708 - val_loss: 0.4860 - val_accuracy: 0.7552\n",
      "Epoch 711/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4620 - accuracy: 0.7708 - val_loss: 0.4860 - val_accuracy: 0.7552\n",
      "Epoch 712/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4620 - accuracy: 0.7708 - val_loss: 0.4860 - val_accuracy: 0.7552\n",
      "Epoch 713/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4620 - accuracy: 0.7708 - val_loss: 0.4859 - val_accuracy: 0.7552\n",
      "Epoch 714/1000\n",
      "18/18 [==============================] - 0s 996us/step - loss: 0.4619 - accuracy: 0.7708 - val_loss: 0.4859 - val_accuracy: 0.7552\n",
      "Epoch 715/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4619 - accuracy: 0.7708 - val_loss: 0.4859 - val_accuracy: 0.7552\n",
      "Epoch 716/1000\n",
      "18/18 [==============================] - 0s 993us/step - loss: 0.4619 - accuracy: 0.7708 - val_loss: 0.4859 - val_accuracy: 0.7552\n",
      "Epoch 717/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4619 - accuracy: 0.7708 - val_loss: 0.4859 - val_accuracy: 0.7552\n",
      "Epoch 718/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4618 - accuracy: 0.7708 - val_loss: 0.4859 - val_accuracy: 0.7552\n",
      "Epoch 719/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4618 - accuracy: 0.7708 - val_loss: 0.4859 - val_accuracy: 0.7552\n",
      "Epoch 720/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4617 - accuracy: 0.7708 - val_loss: 0.4859 - val_accuracy: 0.7552\n",
      "Epoch 721/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4617 - accuracy: 0.7708 - val_loss: 0.4859 - val_accuracy: 0.7552\n",
      "Epoch 722/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4617 - accuracy: 0.7708 - val_loss: 0.4858 - val_accuracy: 0.7552\n",
      "Epoch 723/1000\n",
      "18/18 [==============================] - 0s 945us/step - loss: 0.4616 - accuracy: 0.7708 - val_loss: 0.4858 - val_accuracy: 0.7552\n",
      "Epoch 724/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4616 - accuracy: 0.7708 - val_loss: 0.4858 - val_accuracy: 0.7552\n",
      "Epoch 725/1000\n",
      "18/18 [==============================] - 0s 968us/step - loss: 0.4616 - accuracy: 0.7708 - val_loss: 0.4858 - val_accuracy: 0.7552\n",
      "Epoch 726/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4615 - accuracy: 0.7708 - val_loss: 0.4858 - val_accuracy: 0.7552\n",
      "Epoch 727/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4615 - accuracy: 0.7708 - val_loss: 0.4858 - val_accuracy: 0.7552\n",
      "Epoch 728/1000\n",
      "18/18 [==============================] - 0s 936us/step - loss: 0.4615 - accuracy: 0.7708 - val_loss: 0.4858 - val_accuracy: 0.7552\n",
      "Epoch 729/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4615 - accuracy: 0.7708 - val_loss: 0.4858 - val_accuracy: 0.7552\n",
      "Epoch 730/1000\n",
      "18/18 [==============================] - 0s 977us/step - loss: 0.4614 - accuracy: 0.7708 - val_loss: 0.4858 - val_accuracy: 0.7552\n",
      "Epoch 731/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4614 - accuracy: 0.7708 - val_loss: 0.4857 - val_accuracy: 0.7552\n",
      "Epoch 732/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4614 - accuracy: 0.7708 - val_loss: 0.4857 - val_accuracy: 0.7500\n",
      "Epoch 733/1000\n",
      "18/18 [==============================] - 0s 972us/step - loss: 0.4614 - accuracy: 0.7708 - val_loss: 0.4857 - val_accuracy: 0.7500\n",
      "Epoch 734/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4613 - accuracy: 0.7708 - val_loss: 0.4857 - val_accuracy: 0.7500\n",
      "Epoch 735/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4613 - accuracy: 0.7708 - val_loss: 0.4857 - val_accuracy: 0.7500\n",
      "Epoch 736/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4612 - accuracy: 0.7708 - val_loss: 0.4857 - val_accuracy: 0.7500\n",
      "Epoch 737/1000\n",
      "18/18 [==============================] - 0s 959us/step - loss: 0.4612 - accuracy: 0.7708 - val_loss: 0.4857 - val_accuracy: 0.7500\n",
      "Epoch 738/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4612 - accuracy: 0.7708 - val_loss: 0.4857 - val_accuracy: 0.7500\n",
      "Epoch 739/1000\n",
      "18/18 [==============================] - 0s 974us/step - loss: 0.4612 - accuracy: 0.7708 - val_loss: 0.4857 - val_accuracy: 0.7500\n",
      "Epoch 740/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4612 - accuracy: 0.7708 - val_loss: 0.4857 - val_accuracy: 0.7500\n",
      "Epoch 741/1000\n",
      "18/18 [==============================] - 0s 937us/step - loss: 0.4611 - accuracy: 0.7708 - val_loss: 0.4857 - val_accuracy: 0.7500\n",
      "Epoch 742/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4611 - accuracy: 0.7708 - val_loss: 0.4856 - val_accuracy: 0.7500\n",
      "Epoch 743/1000\n",
      "18/18 [==============================] - 0s 992us/step - loss: 0.4611 - accuracy: 0.7708 - val_loss: 0.4856 - val_accuracy: 0.7500\n",
      "Epoch 744/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4610 - accuracy: 0.7708 - val_loss: 0.4856 - val_accuracy: 0.7500\n",
      "Epoch 745/1000\n",
      "18/18 [==============================] - 0s 956us/step - loss: 0.4610 - accuracy: 0.7708 - val_loss: 0.4856 - val_accuracy: 0.7500\n",
      "Epoch 746/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4610 - accuracy: 0.7708 - val_loss: 0.4856 - val_accuracy: 0.7500\n",
      "Epoch 747/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4609 - accuracy: 0.7708 - val_loss: 0.4856 - val_accuracy: 0.7500\n",
      "Epoch 748/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4609 - accuracy: 0.7708 - val_loss: 0.4856 - val_accuracy: 0.7500\n",
      "Epoch 749/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4609 - accuracy: 0.7708 - val_loss: 0.4856 - val_accuracy: 0.7500\n",
      "Epoch 750/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4609 - accuracy: 0.7708 - val_loss: 0.4856 - val_accuracy: 0.7500\n",
      "Epoch 751/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4608 - accuracy: 0.7708 - val_loss: 0.4856 - val_accuracy: 0.7500\n",
      "Epoch 752/1000\n",
      "18/18 [==============================] - 0s 942us/step - loss: 0.4608 - accuracy: 0.7708 - val_loss: 0.4856 - val_accuracy: 0.7500\n",
      "Epoch 753/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4608 - accuracy: 0.7708 - val_loss: 0.4855 - val_accuracy: 0.7500\n",
      "Epoch 754/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4608 - accuracy: 0.7708 - val_loss: 0.4855 - val_accuracy: 0.7500\n",
      "Epoch 755/1000\n",
      "18/18 [==============================] - 0s 966us/step - loss: 0.4607 - accuracy: 0.7708 - val_loss: 0.4855 - val_accuracy: 0.7500\n",
      "Epoch 756/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4607 - accuracy: 0.7708 - val_loss: 0.4855 - val_accuracy: 0.7500\n",
      "Epoch 757/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4607 - accuracy: 0.7708 - val_loss: 0.4855 - val_accuracy: 0.7500\n",
      "Epoch 758/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4606 - accuracy: 0.7708 - val_loss: 0.4855 - val_accuracy: 0.7500\n",
      "Epoch 759/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4606 - accuracy: 0.7708 - val_loss: 0.4855 - val_accuracy: 0.7500\n",
      "Epoch 760/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4606 - accuracy: 0.7708 - val_loss: 0.4855 - val_accuracy: 0.7500\n",
      "Epoch 761/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4606 - accuracy: 0.7708 - val_loss: 0.4855 - val_accuracy: 0.7552\n",
      "Epoch 762/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4605 - accuracy: 0.7708 - val_loss: 0.4855 - val_accuracy: 0.7552\n",
      "Epoch 763/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4605 - accuracy: 0.7708 - val_loss: 0.4855 - val_accuracy: 0.7552\n",
      "Epoch 764/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4605 - accuracy: 0.7708 - val_loss: 0.4854 - val_accuracy: 0.7552\n",
      "Epoch 765/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4605 - accuracy: 0.7708 - val_loss: 0.4854 - val_accuracy: 0.7552\n",
      "Epoch 766/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4604 - accuracy: 0.7708 - val_loss: 0.4854 - val_accuracy: 0.7552\n",
      "Epoch 767/1000\n",
      "18/18 [==============================] - 0s 946us/step - loss: 0.4604 - accuracy: 0.7708 - val_loss: 0.4854 - val_accuracy: 0.7552\n",
      "Epoch 768/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4604 - accuracy: 0.7708 - val_loss: 0.4854 - val_accuracy: 0.7552\n",
      "Epoch 769/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4604 - accuracy: 0.7708 - val_loss: 0.4854 - val_accuracy: 0.7552\n",
      "Epoch 770/1000\n",
      "18/18 [==============================] - 0s 942us/step - loss: 0.4603 - accuracy: 0.7708 - val_loss: 0.4854 - val_accuracy: 0.7552\n",
      "Epoch 771/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4603 - accuracy: 0.7708 - val_loss: 0.4854 - val_accuracy: 0.7552\n",
      "Epoch 772/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4603 - accuracy: 0.7708 - val_loss: 0.4854 - val_accuracy: 0.7552\n",
      "Epoch 773/1000\n",
      "18/18 [==============================] - 0s 987us/step - loss: 0.4603 - accuracy: 0.7708 - val_loss: 0.4854 - val_accuracy: 0.7552\n",
      "Epoch 774/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4602 - accuracy: 0.7708 - val_loss: 0.4854 - val_accuracy: 0.7552\n",
      "Epoch 775/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4602 - accuracy: 0.7708 - val_loss: 0.4854 - val_accuracy: 0.7552\n",
      "Epoch 776/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4602 - accuracy: 0.7708 - val_loss: 0.4854 - val_accuracy: 0.7552\n",
      "Epoch 777/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4601 - accuracy: 0.7708 - val_loss: 0.4853 - val_accuracy: 0.7552\n",
      "Epoch 778/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4601 - accuracy: 0.7708 - val_loss: 0.4853 - val_accuracy: 0.7552\n",
      "Epoch 779/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4601 - accuracy: 0.7708 - val_loss: 0.4853 - val_accuracy: 0.7552\n",
      "Epoch 780/1000\n",
      "18/18 [==============================] - 0s 950us/step - loss: 0.4601 - accuracy: 0.7708 - val_loss: 0.4853 - val_accuracy: 0.7552\n",
      "Epoch 781/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4601 - accuracy: 0.7708 - val_loss: 0.4853 - val_accuracy: 0.7552\n",
      "Epoch 782/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4600 - accuracy: 0.7708 - val_loss: 0.4853 - val_accuracy: 0.7552\n",
      "Epoch 783/1000\n",
      "18/18 [==============================] - 0s 938us/step - loss: 0.4600 - accuracy: 0.7708 - val_loss: 0.4853 - val_accuracy: 0.7552\n",
      "Epoch 784/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4600 - accuracy: 0.7708 - val_loss: 0.4853 - val_accuracy: 0.7552\n",
      "Epoch 785/1000\n",
      "18/18 [==============================] - 0s 978us/step - loss: 0.4600 - accuracy: 0.7708 - val_loss: 0.4853 - val_accuracy: 0.7552\n",
      "Epoch 786/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4599 - accuracy: 0.7708 - val_loss: 0.4853 - val_accuracy: 0.7552\n",
      "Epoch 787/1000\n",
      "18/18 [==============================] - 0s 947us/step - loss: 0.4599 - accuracy: 0.7708 - val_loss: 0.4853 - val_accuracy: 0.7552\n",
      "Epoch 788/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4599 - accuracy: 0.7708 - val_loss: 0.4853 - val_accuracy: 0.7552\n",
      "Epoch 789/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4599 - accuracy: 0.7708 - val_loss: 0.4853 - val_accuracy: 0.7552\n",
      "Epoch 790/1000\n",
      "18/18 [==============================] - 0s 960us/step - loss: 0.4598 - accuracy: 0.7708 - val_loss: 0.4853 - val_accuracy: 0.7552\n",
      "Epoch 791/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4598 - accuracy: 0.7708 - val_loss: 0.4852 - val_accuracy: 0.7552\n",
      "Epoch 792/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4598 - accuracy: 0.7708 - val_loss: 0.4852 - val_accuracy: 0.7552\n",
      "Epoch 793/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4598 - accuracy: 0.7708 - val_loss: 0.4852 - val_accuracy: 0.7552\n",
      "Epoch 794/1000\n",
      "18/18 [==============================] - 0s 990us/step - loss: 0.4598 - accuracy: 0.7708 - val_loss: 0.4852 - val_accuracy: 0.7552\n",
      "Epoch 795/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4597 - accuracy: 0.7708 - val_loss: 0.4852 - val_accuracy: 0.7552\n",
      "Epoch 796/1000\n",
      "18/18 [==============================] - 0s 932us/step - loss: 0.4597 - accuracy: 0.7708 - val_loss: 0.4852 - val_accuracy: 0.7552\n",
      "Epoch 797/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4597 - accuracy: 0.7708 - val_loss: 0.4852 - val_accuracy: 0.7552\n",
      "Epoch 798/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4596 - accuracy: 0.7708 - val_loss: 0.4852 - val_accuracy: 0.7552\n",
      "Epoch 799/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4596 - accuracy: 0.7708 - val_loss: 0.4852 - val_accuracy: 0.7552\n",
      "Epoch 800/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4596 - accuracy: 0.7708 - val_loss: 0.4852 - val_accuracy: 0.7552\n",
      "Epoch 801/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4596 - accuracy: 0.7708 - val_loss: 0.4852 - val_accuracy: 0.7552\n",
      "Epoch 802/1000\n",
      "18/18 [==============================] - 0s 987us/step - loss: 0.4596 - accuracy: 0.7708 - val_loss: 0.4852 - val_accuracy: 0.7552\n",
      "Epoch 803/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4595 - accuracy: 0.7708 - val_loss: 0.4852 - val_accuracy: 0.7552\n",
      "Epoch 804/1000\n",
      "18/18 [==============================] - 0s 917us/step - loss: 0.4595 - accuracy: 0.7708 - val_loss: 0.4852 - val_accuracy: 0.7552\n",
      "Epoch 805/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.7708 - val_loss: 0.4852 - val_accuracy: 0.7552\n",
      "Epoch 806/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.7708 - val_loss: 0.4851 - val_accuracy: 0.7552\n",
      "Epoch 807/1000\n",
      "18/18 [==============================] - 0s 995us/step - loss: 0.4594 - accuracy: 0.7691 - val_loss: 0.4851 - val_accuracy: 0.7552\n",
      "Epoch 808/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4594 - accuracy: 0.7708 - val_loss: 0.4851 - val_accuracy: 0.7552\n",
      "Epoch 809/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4594 - accuracy: 0.7691 - val_loss: 0.4851 - val_accuracy: 0.7552\n",
      "Epoch 810/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4594 - accuracy: 0.7691 - val_loss: 0.4851 - val_accuracy: 0.7552\n",
      "Epoch 811/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4593 - accuracy: 0.7691 - val_loss: 0.4851 - val_accuracy: 0.7552\n",
      "Epoch 812/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4593 - accuracy: 0.7691 - val_loss: 0.4851 - val_accuracy: 0.7552\n",
      "Epoch 813/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4593 - accuracy: 0.7691 - val_loss: 0.4851 - val_accuracy: 0.7552\n",
      "Epoch 814/1000\n",
      "18/18 [==============================] - 0s 988us/step - loss: 0.4593 - accuracy: 0.7691 - val_loss: 0.4851 - val_accuracy: 0.7552\n",
      "Epoch 815/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4593 - accuracy: 0.7691 - val_loss: 0.4851 - val_accuracy: 0.7552\n",
      "Epoch 816/1000\n",
      "18/18 [==============================] - 0s 946us/step - loss: 0.4592 - accuracy: 0.7691 - val_loss: 0.4851 - val_accuracy: 0.7552\n",
      "Epoch 817/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4592 - accuracy: 0.7691 - val_loss: 0.4851 - val_accuracy: 0.7552\n",
      "Epoch 818/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4592 - accuracy: 0.7691 - val_loss: 0.4851 - val_accuracy: 0.7552\n",
      "Epoch 819/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.7691 - val_loss: 0.4851 - val_accuracy: 0.7552\n",
      "Epoch 820/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4591 - accuracy: 0.7691 - val_loss: 0.4851 - val_accuracy: 0.7552\n",
      "Epoch 821/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4591 - accuracy: 0.7691 - val_loss: 0.4851 - val_accuracy: 0.7552\n",
      "Epoch 822/1000\n",
      "18/18 [==============================] - 0s 970us/step - loss: 0.4591 - accuracy: 0.7691 - val_loss: 0.4851 - val_accuracy: 0.7552\n",
      "Epoch 823/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4591 - accuracy: 0.7708 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 824/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4591 - accuracy: 0.7691 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 825/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4590 - accuracy: 0.7691 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 826/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4590 - accuracy: 0.7691 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 827/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4590 - accuracy: 0.7691 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 828/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4590 - accuracy: 0.7691 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 829/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4589 - accuracy: 0.7691 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 830/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4589 - accuracy: 0.7691 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 831/1000\n",
      "18/18 [==============================] - 0s 909us/step - loss: 0.4589 - accuracy: 0.7691 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 832/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4589 - accuracy: 0.7691 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 833/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4589 - accuracy: 0.7691 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 834/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4588 - accuracy: 0.7691 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 835/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4588 - accuracy: 0.7691 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 836/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4588 - accuracy: 0.7691 - val_loss: 0.4850 - val_accuracy: 0.7552\n",
      "Epoch 837/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4588 - accuracy: 0.7691 - val_loss: 0.4850 - val_accuracy: 0.7604\n",
      "Epoch 838/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4587 - accuracy: 0.7691 - val_loss: 0.4850 - val_accuracy: 0.7604\n",
      "Epoch 839/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4587 - accuracy: 0.7691 - val_loss: 0.4850 - val_accuracy: 0.7604\n",
      "Epoch 840/1000\n",
      "18/18 [==============================] - 0s 933us/step - loss: 0.4587 - accuracy: 0.7691 - val_loss: 0.4850 - val_accuracy: 0.7604\n",
      "Epoch 841/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4587 - accuracy: 0.7708 - val_loss: 0.4850 - val_accuracy: 0.7604\n",
      "Epoch 842/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4587 - accuracy: 0.7708 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 843/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4587 - accuracy: 0.7708 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 844/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4586 - accuracy: 0.7708 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 845/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4586 - accuracy: 0.7708 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 846/1000\n",
      "18/18 [==============================] - 0s 967us/step - loss: 0.4586 - accuracy: 0.7708 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 847/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4586 - accuracy: 0.7708 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 848/1000\n",
      "18/18 [==============================] - 0s 944us/step - loss: 0.4585 - accuracy: 0.7708 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 849/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4585 - accuracy: 0.7708 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 850/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4585 - accuracy: 0.7708 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 851/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4585 - accuracy: 0.7726 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 852/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4585 - accuracy: 0.7708 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 853/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4585 - accuracy: 0.7726 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 854/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4584 - accuracy: 0.7726 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 855/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4584 - accuracy: 0.7708 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 856/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4584 - accuracy: 0.7726 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 857/1000\n",
      "18/18 [==============================] - 0s 963us/step - loss: 0.4583 - accuracy: 0.7726 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 858/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4583 - accuracy: 0.7726 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 859/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4583 - accuracy: 0.7743 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 860/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4583 - accuracy: 0.7743 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 861/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4583 - accuracy: 0.7743 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 862/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4583 - accuracy: 0.7726 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 863/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4582 - accuracy: 0.7743 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 864/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4582 - accuracy: 0.7726 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 865/1000\n",
      "18/18 [==============================] - 0s 998us/step - loss: 0.4582 - accuracy: 0.7708 - val_loss: 0.4849 - val_accuracy: 0.7604\n",
      "Epoch 866/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4582 - accuracy: 0.7726 - val_loss: 0.4848 - val_accuracy: 0.7604\n",
      "Epoch 867/1000\n",
      "18/18 [==============================] - 0s 993us/step - loss: 0.4582 - accuracy: 0.7726 - val_loss: 0.4848 - val_accuracy: 0.7604\n",
      "Epoch 868/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4581 - accuracy: 0.7726 - val_loss: 0.4848 - val_accuracy: 0.7604\n",
      "Epoch 869/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4581 - accuracy: 0.7726 - val_loss: 0.4848 - val_accuracy: 0.7604\n",
      "Epoch 870/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4581 - accuracy: 0.7743 - val_loss: 0.4848 - val_accuracy: 0.7604\n",
      "Epoch 871/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4581 - accuracy: 0.7743 - val_loss: 0.4848 - val_accuracy: 0.7604\n",
      "Epoch 872/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4581 - accuracy: 0.7726 - val_loss: 0.4848 - val_accuracy: 0.7604\n",
      "Epoch 873/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4580 - accuracy: 0.7726 - val_loss: 0.4848 - val_accuracy: 0.7604\n",
      "Epoch 874/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4580 - accuracy: 0.7743 - val_loss: 0.4848 - val_accuracy: 0.7604\n",
      "Epoch 875/1000\n",
      "18/18 [==============================] - 0s 977us/step - loss: 0.4580 - accuracy: 0.7726 - val_loss: 0.4848 - val_accuracy: 0.7604\n",
      "Epoch 876/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4580 - accuracy: 0.7726 - val_loss: 0.4848 - val_accuracy: 0.7604\n",
      "Epoch 877/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4580 - accuracy: 0.7726 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 878/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4579 - accuracy: 0.7743 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 879/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4579 - accuracy: 0.7743 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 880/1000\n",
      "18/18 [==============================] - 0s 977us/step - loss: 0.4579 - accuracy: 0.7743 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 881/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4579 - accuracy: 0.7743 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 882/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4579 - accuracy: 0.7726 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 883/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4578 - accuracy: 0.7743 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 884/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4578 - accuracy: 0.7743 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 885/1000\n",
      "18/18 [==============================] - 0s 995us/step - loss: 0.4578 - accuracy: 0.7743 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 886/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4578 - accuracy: 0.7743 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 887/1000\n",
      "18/18 [==============================] - 0s 958us/step - loss: 0.4578 - accuracy: 0.7743 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 888/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4577 - accuracy: 0.7743 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 889/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4578 - accuracy: 0.7743 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 890/1000\n",
      "18/18 [==============================] - 0s 958us/step - loss: 0.4577 - accuracy: 0.7743 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 891/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4577 - accuracy: 0.7743 - val_loss: 0.4848 - val_accuracy: 0.7552\n",
      "Epoch 892/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4577 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 893/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4577 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 894/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4576 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 895/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4576 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 896/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4576 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 897/1000\n",
      "18/18 [==============================] - 0s 941us/step - loss: 0.4576 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 898/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4576 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 899/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4576 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 900/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4575 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 901/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4575 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 902/1000\n",
      "18/18 [==============================] - 0s 964us/step - loss: 0.4575 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 903/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4575 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 904/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4575 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 905/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4575 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 906/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4574 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 907/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4574 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 908/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4574 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 909/1000\n",
      "18/18 [==============================] - 0s 984us/step - loss: 0.4574 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7552\n",
      "Epoch 910/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4574 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7604\n",
      "Epoch 911/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4574 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7604\n",
      "Epoch 912/1000\n",
      "18/18 [==============================] - 0s 993us/step - loss: 0.4573 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7604\n",
      "Epoch 913/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4573 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7604\n",
      "Epoch 914/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4573 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7604\n",
      "Epoch 915/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4573 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7604\n",
      "Epoch 916/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4573 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7604\n",
      "Epoch 917/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4572 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7604\n",
      "Epoch 918/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4572 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7604\n",
      "Epoch 919/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4572 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7604\n",
      "Epoch 920/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4572 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7604\n",
      "Epoch 921/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4572 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7604\n",
      "Epoch 922/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4572 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7604\n",
      "Epoch 923/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4571 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7604\n",
      "Epoch 924/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4571 - accuracy: 0.7760 - val_loss: 0.4847 - val_accuracy: 0.7604\n",
      "Epoch 925/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4571 - accuracy: 0.7743 - val_loss: 0.4847 - val_accuracy: 0.7604\n",
      "Epoch 926/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4571 - accuracy: 0.7760 - val_loss: 0.4847 - val_accuracy: 0.7604\n",
      "Epoch 927/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4571 - accuracy: 0.7760 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 928/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4570 - accuracy: 0.7760 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 929/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4570 - accuracy: 0.7743 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 930/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4570 - accuracy: 0.7743 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 931/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4570 - accuracy: 0.7743 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 932/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4570 - accuracy: 0.7760 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 933/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4570 - accuracy: 0.7760 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 934/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4570 - accuracy: 0.7760 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 935/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4569 - accuracy: 0.7760 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 936/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4569 - accuracy: 0.7778 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 937/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4569 - accuracy: 0.7778 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 938/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4569 - accuracy: 0.7778 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 939/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4569 - accuracy: 0.7778 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 940/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4568 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 941/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4568 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 942/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4568 - accuracy: 0.7778 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 943/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4568 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 944/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4568 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 945/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4568 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 946/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4568 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 947/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4567 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 948/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4567 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 949/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4567 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 950/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4567 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 951/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4567 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 952/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4567 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 953/1000\n",
      "18/18 [==============================] - 0s 956us/step - loss: 0.4567 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 954/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4566 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 955/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4566 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 956/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4566 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 957/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4566 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 958/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4566 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 959/1000\n",
      "18/18 [==============================] - 0s 935us/step - loss: 0.4566 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 960/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4566 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 961/1000\n",
      "18/18 [==============================] - 0s 987us/step - loss: 0.4565 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 962/1000\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4565 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 963/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4565 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 964/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4565 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 965/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4565 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 966/1000\n",
      "18/18 [==============================] - 0s 965us/step - loss: 0.4564 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 967/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4564 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 968/1000\n",
      "18/18 [==============================] - 0s 998us/step - loss: 0.4564 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 969/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4564 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 970/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4564 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 971/1000\n",
      "18/18 [==============================] - 0s 929us/step - loss: 0.4564 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 972/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4564 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 973/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4563 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 974/1000\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4563 - accuracy: 0.7795 - val_loss: 0.4846 - val_accuracy: 0.7604\n",
      "Epoch 975/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4563 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 976/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4563 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 977/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4563 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 978/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4563 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 979/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4563 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 980/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4562 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 981/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4562 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 982/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4562 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 983/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4562 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 984/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4562 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 985/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4562 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 986/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4562 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 987/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4561 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 988/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4561 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 989/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4561 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 990/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4561 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 991/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4561 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 992/1000\n",
      "18/18 [==============================] - 0s 991us/step - loss: 0.4561 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 993/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4560 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 994/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4560 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 995/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4560 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 996/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4560 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 997/1000\n",
      "18/18 [==============================] - 0s 965us/step - loss: 0.4560 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 998/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4560 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 999/1000\n",
      "18/18 [==============================] - 0s 949us/step - loss: 0.4560 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n",
      "Epoch 1000/1000\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4560 - accuracy: 0.7795 - val_loss: 0.4845 - val_accuracy: 0.7604\n"
     ]
    }
   ],
   "source": [
    "## Note that when we call \"fit\" again, it picks up where it left off\n",
    "run_hist_1b = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x177920280>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(run_hist_1.history[\"loss\"])\n",
    "m = len(run_hist_1b.history['loss'])\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"loss\"], 'hotpink', marker='.', label=\"Train Loss - Run 2\")\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"val_loss\"], 'LightSkyBlue', marker='.',  label=\"Validation Loss - Run 2\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this graph begins where the other left off.  While the training loss is still going down, it looks like the validation loss has stabilized (or even gotten worse!).  This suggests that our network will not benefit from further training.  What is the appropriate number of epochs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "For this exercise, do the following in the cells below:\n",
    "- Build a model with two hidden layers, each with 6 nodes\n",
    "- Use the \"relu\" activation function for the hidden layers, and \"sigmoid\" for the final layer\n",
    "- Use a learning rate of .003 and train for 1500 epochs\n",
    "- Graph the trajectory of the loss functions, accuracy on both train and test set\n",
    "- Plot the roc curve for the predictions\n",
    "\n",
    "Experiment with different learning rates, numbers of epochs, and network structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6384 - accuracy: 0.6545 - val_loss: 0.6655 - val_accuracy: 0.6406\n",
      "Epoch 2/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6369 - accuracy: 0.6545 - val_loss: 0.6641 - val_accuracy: 0.6406\n",
      "Epoch 3/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6354 - accuracy: 0.6545 - val_loss: 0.6627 - val_accuracy: 0.6406\n",
      "Epoch 4/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6340 - accuracy: 0.6545 - val_loss: 0.6613 - val_accuracy: 0.6406\n",
      "Epoch 5/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6326 - accuracy: 0.6545 - val_loss: 0.6600 - val_accuracy: 0.6406\n",
      "Epoch 6/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6312 - accuracy: 0.6545 - val_loss: 0.6587 - val_accuracy: 0.6406\n",
      "Epoch 7/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6298 - accuracy: 0.6545 - val_loss: 0.6575 - val_accuracy: 0.6406\n",
      "Epoch 8/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6285 - accuracy: 0.6545 - val_loss: 0.6562 - val_accuracy: 0.6406\n",
      "Epoch 9/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6271 - accuracy: 0.6545 - val_loss: 0.6550 - val_accuracy: 0.6406\n",
      "Epoch 10/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6258 - accuracy: 0.6545 - val_loss: 0.6538 - val_accuracy: 0.6406\n",
      "Epoch 11/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6245 - accuracy: 0.6545 - val_loss: 0.6527 - val_accuracy: 0.6406\n",
      "Epoch 12/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6232 - accuracy: 0.6545 - val_loss: 0.6515 - val_accuracy: 0.6406\n",
      "Epoch 13/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6219 - accuracy: 0.6545 - val_loss: 0.6504 - val_accuracy: 0.6406\n",
      "Epoch 14/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6206 - accuracy: 0.6545 - val_loss: 0.6492 - val_accuracy: 0.6406\n",
      "Epoch 15/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6194 - accuracy: 0.6545 - val_loss: 0.6482 - val_accuracy: 0.6406\n",
      "Epoch 16/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6181 - accuracy: 0.6545 - val_loss: 0.6471 - val_accuracy: 0.6406\n",
      "Epoch 17/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6169 - accuracy: 0.6545 - val_loss: 0.6461 - val_accuracy: 0.6406\n",
      "Epoch 18/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6157 - accuracy: 0.6545 - val_loss: 0.6450 - val_accuracy: 0.6406\n",
      "Epoch 19/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6145 - accuracy: 0.6545 - val_loss: 0.6440 - val_accuracy: 0.6406\n",
      "Epoch 20/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6133 - accuracy: 0.6545 - val_loss: 0.6430 - val_accuracy: 0.6406\n",
      "Epoch 21/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6121 - accuracy: 0.6545 - val_loss: 0.6420 - val_accuracy: 0.6406\n",
      "Epoch 22/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6109 - accuracy: 0.6545 - val_loss: 0.6410 - val_accuracy: 0.6406\n",
      "Epoch 23/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6097 - accuracy: 0.6545 - val_loss: 0.6400 - val_accuracy: 0.6406\n",
      "Epoch 24/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6085 - accuracy: 0.6545 - val_loss: 0.6390 - val_accuracy: 0.6406\n",
      "Epoch 25/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6073 - accuracy: 0.6545 - val_loss: 0.6380 - val_accuracy: 0.6406\n",
      "Epoch 26/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6062 - accuracy: 0.6545 - val_loss: 0.6371 - val_accuracy: 0.6406\n",
      "Epoch 27/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6050 - accuracy: 0.6545 - val_loss: 0.6361 - val_accuracy: 0.6406\n",
      "Epoch 28/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6039 - accuracy: 0.6545 - val_loss: 0.6352 - val_accuracy: 0.6406\n",
      "Epoch 29/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6028 - accuracy: 0.6545 - val_loss: 0.6343 - val_accuracy: 0.6406\n",
      "Epoch 30/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6017 - accuracy: 0.6545 - val_loss: 0.6334 - val_accuracy: 0.6406\n",
      "Epoch 31/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6006 - accuracy: 0.6562 - val_loss: 0.6325 - val_accuracy: 0.6406\n",
      "Epoch 32/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5995 - accuracy: 0.6545 - val_loss: 0.6316 - val_accuracy: 0.6406\n",
      "Epoch 33/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5984 - accuracy: 0.6580 - val_loss: 0.6307 - val_accuracy: 0.6302\n",
      "Epoch 34/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5973 - accuracy: 0.6580 - val_loss: 0.6299 - val_accuracy: 0.6302\n",
      "Epoch 35/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5963 - accuracy: 0.6580 - val_loss: 0.6290 - val_accuracy: 0.6302\n",
      "Epoch 36/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5952 - accuracy: 0.6580 - val_loss: 0.6282 - val_accuracy: 0.6302\n",
      "Epoch 37/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5941 - accuracy: 0.6580 - val_loss: 0.6273 - val_accuracy: 0.6302\n",
      "Epoch 38/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5930 - accuracy: 0.6580 - val_loss: 0.6265 - val_accuracy: 0.6302\n",
      "Epoch 39/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5920 - accuracy: 0.6580 - val_loss: 0.6257 - val_accuracy: 0.6302\n",
      "Epoch 40/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5909 - accuracy: 0.6580 - val_loss: 0.6248 - val_accuracy: 0.6302\n",
      "Epoch 41/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5898 - accuracy: 0.6580 - val_loss: 0.6240 - val_accuracy: 0.6302\n",
      "Epoch 42/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5888 - accuracy: 0.6580 - val_loss: 0.6232 - val_accuracy: 0.6302\n",
      "Epoch 43/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5877 - accuracy: 0.6580 - val_loss: 0.6224 - val_accuracy: 0.6302\n",
      "Epoch 44/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5867 - accuracy: 0.6580 - val_loss: 0.6216 - val_accuracy: 0.6302\n",
      "Epoch 45/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5857 - accuracy: 0.6580 - val_loss: 0.6208 - val_accuracy: 0.6302\n",
      "Epoch 46/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5846 - accuracy: 0.6580 - val_loss: 0.6200 - val_accuracy: 0.6302\n",
      "Epoch 47/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5835 - accuracy: 0.6580 - val_loss: 0.6192 - val_accuracy: 0.6250\n",
      "Epoch 48/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5825 - accuracy: 0.6580 - val_loss: 0.6184 - val_accuracy: 0.6250\n",
      "Epoch 49/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5814 - accuracy: 0.6580 - val_loss: 0.6176 - val_accuracy: 0.6250\n",
      "Epoch 50/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5804 - accuracy: 0.6580 - val_loss: 0.6168 - val_accuracy: 0.6250\n",
      "Epoch 51/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5794 - accuracy: 0.6597 - val_loss: 0.6160 - val_accuracy: 0.6250\n",
      "Epoch 52/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5784 - accuracy: 0.6597 - val_loss: 0.6152 - val_accuracy: 0.6250\n",
      "Epoch 53/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5773 - accuracy: 0.6597 - val_loss: 0.6145 - val_accuracy: 0.6250\n",
      "Epoch 54/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5763 - accuracy: 0.6597 - val_loss: 0.6137 - val_accuracy: 0.6250\n",
      "Epoch 55/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5753 - accuracy: 0.6597 - val_loss: 0.6130 - val_accuracy: 0.6250\n",
      "Epoch 56/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5744 - accuracy: 0.6597 - val_loss: 0.6122 - val_accuracy: 0.6250\n",
      "Epoch 57/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5734 - accuracy: 0.6597 - val_loss: 0.6115 - val_accuracy: 0.6302\n",
      "Epoch 58/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5725 - accuracy: 0.6597 - val_loss: 0.6108 - val_accuracy: 0.6302\n",
      "Epoch 59/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5715 - accuracy: 0.6597 - val_loss: 0.6101 - val_accuracy: 0.6302\n",
      "Epoch 60/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5705 - accuracy: 0.6615 - val_loss: 0.6094 - val_accuracy: 0.6302\n",
      "Epoch 61/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5696 - accuracy: 0.6580 - val_loss: 0.6087 - val_accuracy: 0.6354\n",
      "Epoch 62/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5687 - accuracy: 0.6597 - val_loss: 0.6080 - val_accuracy: 0.6406\n",
      "Epoch 63/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5678 - accuracy: 0.6597 - val_loss: 0.6073 - val_accuracy: 0.6458\n",
      "Epoch 64/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5668 - accuracy: 0.6597 - val_loss: 0.6066 - val_accuracy: 0.6458\n",
      "Epoch 65/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5659 - accuracy: 0.6562 - val_loss: 0.6059 - val_accuracy: 0.6458\n",
      "Epoch 66/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5650 - accuracy: 0.6562 - val_loss: 0.6053 - val_accuracy: 0.6458\n",
      "Epoch 67/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5641 - accuracy: 0.6580 - val_loss: 0.6046 - val_accuracy: 0.6458\n",
      "Epoch 68/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5632 - accuracy: 0.6580 - val_loss: 0.6040 - val_accuracy: 0.6458\n",
      "Epoch 69/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5624 - accuracy: 0.6580 - val_loss: 0.6033 - val_accuracy: 0.6458\n",
      "Epoch 70/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5615 - accuracy: 0.6632 - val_loss: 0.6026 - val_accuracy: 0.6458\n",
      "Epoch 71/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5606 - accuracy: 0.6632 - val_loss: 0.6020 - val_accuracy: 0.6510\n",
      "Epoch 72/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5597 - accuracy: 0.6632 - val_loss: 0.6013 - val_accuracy: 0.6562\n",
      "Epoch 73/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5588 - accuracy: 0.6632 - val_loss: 0.6006 - val_accuracy: 0.6510\n",
      "Epoch 74/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5580 - accuracy: 0.6632 - val_loss: 0.6000 - val_accuracy: 0.6510\n",
      "Epoch 75/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5571 - accuracy: 0.6667 - val_loss: 0.5993 - val_accuracy: 0.6615\n",
      "Epoch 76/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5562 - accuracy: 0.6667 - val_loss: 0.5987 - val_accuracy: 0.6562\n",
      "Epoch 77/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5553 - accuracy: 0.6649 - val_loss: 0.5981 - val_accuracy: 0.6562\n",
      "Epoch 78/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5545 - accuracy: 0.6649 - val_loss: 0.5975 - val_accuracy: 0.6562\n",
      "Epoch 79/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5536 - accuracy: 0.6649 - val_loss: 0.5968 - val_accuracy: 0.6562\n",
      "Epoch 80/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5528 - accuracy: 0.6632 - val_loss: 0.5962 - val_accuracy: 0.6615\n",
      "Epoch 81/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5520 - accuracy: 0.6649 - val_loss: 0.5957 - val_accuracy: 0.6615\n",
      "Epoch 82/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5511 - accuracy: 0.6649 - val_loss: 0.5951 - val_accuracy: 0.6615\n",
      "Epoch 83/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5503 - accuracy: 0.6667 - val_loss: 0.5945 - val_accuracy: 0.6615\n",
      "Epoch 84/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5495 - accuracy: 0.6649 - val_loss: 0.5940 - val_accuracy: 0.6615\n",
      "Epoch 85/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5487 - accuracy: 0.6667 - val_loss: 0.5934 - val_accuracy: 0.6615\n",
      "Epoch 86/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5480 - accuracy: 0.6719 - val_loss: 0.5929 - val_accuracy: 0.6615\n",
      "Epoch 87/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5471 - accuracy: 0.6719 - val_loss: 0.5924 - val_accuracy: 0.6615\n",
      "Epoch 88/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5464 - accuracy: 0.6736 - val_loss: 0.5918 - val_accuracy: 0.6615\n",
      "Epoch 89/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5456 - accuracy: 0.6771 - val_loss: 0.5913 - val_accuracy: 0.6667\n",
      "Epoch 90/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5448 - accuracy: 0.6806 - val_loss: 0.5908 - val_accuracy: 0.6667\n",
      "Epoch 91/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5440 - accuracy: 0.6806 - val_loss: 0.5903 - val_accuracy: 0.6667\n",
      "Epoch 92/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5433 - accuracy: 0.6858 - val_loss: 0.5898 - val_accuracy: 0.6667\n",
      "Epoch 93/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5425 - accuracy: 0.6910 - val_loss: 0.5893 - val_accuracy: 0.6667\n",
      "Epoch 94/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5418 - accuracy: 0.6962 - val_loss: 0.5888 - val_accuracy: 0.6667\n",
      "Epoch 95/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5410 - accuracy: 0.7014 - val_loss: 0.5884 - val_accuracy: 0.6667\n",
      "Epoch 96/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5403 - accuracy: 0.7031 - val_loss: 0.5879 - val_accuracy: 0.6667\n",
      "Epoch 97/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5396 - accuracy: 0.6979 - val_loss: 0.5874 - val_accuracy: 0.6771\n",
      "Epoch 98/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5388 - accuracy: 0.6997 - val_loss: 0.5870 - val_accuracy: 0.6771\n",
      "Epoch 99/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5382 - accuracy: 0.7014 - val_loss: 0.5865 - val_accuracy: 0.6719\n",
      "Epoch 100/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5374 - accuracy: 0.7031 - val_loss: 0.5860 - val_accuracy: 0.6719\n",
      "Epoch 101/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5367 - accuracy: 0.7049 - val_loss: 0.5856 - val_accuracy: 0.6719\n",
      "Epoch 102/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5360 - accuracy: 0.7049 - val_loss: 0.5851 - val_accuracy: 0.6771\n",
      "Epoch 103/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5353 - accuracy: 0.7066 - val_loss: 0.5847 - val_accuracy: 0.6771\n",
      "Epoch 104/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5346 - accuracy: 0.7083 - val_loss: 0.5843 - val_accuracy: 0.6771\n",
      "Epoch 105/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5339 - accuracy: 0.7101 - val_loss: 0.5838 - val_accuracy: 0.6771\n",
      "Epoch 106/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5333 - accuracy: 0.7083 - val_loss: 0.5834 - val_accuracy: 0.6875\n",
      "Epoch 107/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5326 - accuracy: 0.7083 - val_loss: 0.5830 - val_accuracy: 0.6875\n",
      "Epoch 108/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5319 - accuracy: 0.7101 - val_loss: 0.5826 - val_accuracy: 0.6875\n",
      "Epoch 109/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5312 - accuracy: 0.7066 - val_loss: 0.5821 - val_accuracy: 0.6875\n",
      "Epoch 110/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5306 - accuracy: 0.7066 - val_loss: 0.5817 - val_accuracy: 0.6823\n",
      "Epoch 111/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5299 - accuracy: 0.7101 - val_loss: 0.5813 - val_accuracy: 0.6823\n",
      "Epoch 112/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5292 - accuracy: 0.7101 - val_loss: 0.5809 - val_accuracy: 0.6667\n",
      "Epoch 113/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5286 - accuracy: 0.7135 - val_loss: 0.5805 - val_accuracy: 0.6719\n",
      "Epoch 114/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5279 - accuracy: 0.7153 - val_loss: 0.5801 - val_accuracy: 0.6667\n",
      "Epoch 115/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5273 - accuracy: 0.7170 - val_loss: 0.5797 - val_accuracy: 0.6719\n",
      "Epoch 116/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5267 - accuracy: 0.7188 - val_loss: 0.5793 - val_accuracy: 0.6719\n",
      "Epoch 117/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5260 - accuracy: 0.7205 - val_loss: 0.5789 - val_accuracy: 0.6771\n",
      "Epoch 118/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5254 - accuracy: 0.7205 - val_loss: 0.5785 - val_accuracy: 0.6771\n",
      "Epoch 119/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5248 - accuracy: 0.7257 - val_loss: 0.5781 - val_accuracy: 0.6771\n",
      "Epoch 120/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5242 - accuracy: 0.7257 - val_loss: 0.5778 - val_accuracy: 0.6771\n",
      "Epoch 121/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5236 - accuracy: 0.7292 - val_loss: 0.5774 - val_accuracy: 0.6771\n",
      "Epoch 122/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5230 - accuracy: 0.7274 - val_loss: 0.5771 - val_accuracy: 0.6771\n",
      "Epoch 123/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5224 - accuracy: 0.7274 - val_loss: 0.5768 - val_accuracy: 0.6771\n",
      "Epoch 124/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5218 - accuracy: 0.7274 - val_loss: 0.5764 - val_accuracy: 0.6771\n",
      "Epoch 125/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5212 - accuracy: 0.7292 - val_loss: 0.5761 - val_accuracy: 0.6771\n",
      "Epoch 126/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5206 - accuracy: 0.7309 - val_loss: 0.5758 - val_accuracy: 0.6771\n",
      "Epoch 127/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5200 - accuracy: 0.7309 - val_loss: 0.5755 - val_accuracy: 0.6771\n",
      "Epoch 128/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5194 - accuracy: 0.7309 - val_loss: 0.5751 - val_accuracy: 0.6771\n",
      "Epoch 129/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5189 - accuracy: 0.7326 - val_loss: 0.5748 - val_accuracy: 0.6823\n",
      "Epoch 130/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5183 - accuracy: 0.7344 - val_loss: 0.5745 - val_accuracy: 0.6875\n",
      "Epoch 131/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5178 - accuracy: 0.7344 - val_loss: 0.5742 - val_accuracy: 0.6875\n",
      "Epoch 132/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5172 - accuracy: 0.7378 - val_loss: 0.5738 - val_accuracy: 0.6875\n",
      "Epoch 133/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5166 - accuracy: 0.7378 - val_loss: 0.5735 - val_accuracy: 0.6927\n",
      "Epoch 134/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5161 - accuracy: 0.7378 - val_loss: 0.5732 - val_accuracy: 0.6927\n",
      "Epoch 135/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5155 - accuracy: 0.7378 - val_loss: 0.5729 - val_accuracy: 0.6927\n",
      "Epoch 136/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5149 - accuracy: 0.7413 - val_loss: 0.5726 - val_accuracy: 0.6927\n",
      "Epoch 137/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5144 - accuracy: 0.7431 - val_loss: 0.5723 - val_accuracy: 0.6927\n",
      "Epoch 138/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5139 - accuracy: 0.7431 - val_loss: 0.5720 - val_accuracy: 0.6979\n",
      "Epoch 139/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5133 - accuracy: 0.7413 - val_loss: 0.5716 - val_accuracy: 0.6979\n",
      "Epoch 140/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5128 - accuracy: 0.7431 - val_loss: 0.5713 - val_accuracy: 0.6979\n",
      "Epoch 141/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5122 - accuracy: 0.7431 - val_loss: 0.5710 - val_accuracy: 0.6979\n",
      "Epoch 142/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5117 - accuracy: 0.7431 - val_loss: 0.5707 - val_accuracy: 0.6927\n",
      "Epoch 143/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5112 - accuracy: 0.7431 - val_loss: 0.5704 - val_accuracy: 0.6927\n",
      "Epoch 144/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5106 - accuracy: 0.7465 - val_loss: 0.5701 - val_accuracy: 0.6927\n",
      "Epoch 145/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5101 - accuracy: 0.7465 - val_loss: 0.5699 - val_accuracy: 0.6927\n",
      "Epoch 146/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5096 - accuracy: 0.7483 - val_loss: 0.5696 - val_accuracy: 0.6927\n",
      "Epoch 147/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5091 - accuracy: 0.7465 - val_loss: 0.5693 - val_accuracy: 0.6979\n",
      "Epoch 148/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5086 - accuracy: 0.7465 - val_loss: 0.5690 - val_accuracy: 0.6979\n",
      "Epoch 149/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5081 - accuracy: 0.7465 - val_loss: 0.5687 - val_accuracy: 0.6979\n",
      "Epoch 150/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5076 - accuracy: 0.7465 - val_loss: 0.5685 - val_accuracy: 0.6979\n",
      "Epoch 151/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5071 - accuracy: 0.7465 - val_loss: 0.5682 - val_accuracy: 0.7031\n",
      "Epoch 152/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5066 - accuracy: 0.7500 - val_loss: 0.5679 - val_accuracy: 0.7031\n",
      "Epoch 153/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5061 - accuracy: 0.7517 - val_loss: 0.5677 - val_accuracy: 0.7031\n",
      "Epoch 154/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5056 - accuracy: 0.7517 - val_loss: 0.5674 - val_accuracy: 0.7031\n",
      "Epoch 155/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5052 - accuracy: 0.7517 - val_loss: 0.5671 - val_accuracy: 0.7031\n",
      "Epoch 156/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5047 - accuracy: 0.7517 - val_loss: 0.5669 - val_accuracy: 0.7031\n",
      "Epoch 157/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5042 - accuracy: 0.7535 - val_loss: 0.5666 - val_accuracy: 0.7031\n",
      "Epoch 158/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5037 - accuracy: 0.7552 - val_loss: 0.5664 - val_accuracy: 0.7031\n",
      "Epoch 159/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5033 - accuracy: 0.7552 - val_loss: 0.5662 - val_accuracy: 0.7031\n",
      "Epoch 160/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5028 - accuracy: 0.7552 - val_loss: 0.5659 - val_accuracy: 0.7031\n",
      "Epoch 161/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5023 - accuracy: 0.7552 - val_loss: 0.5657 - val_accuracy: 0.7031\n",
      "Epoch 162/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5019 - accuracy: 0.7569 - val_loss: 0.5654 - val_accuracy: 0.6979\n",
      "Epoch 163/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5014 - accuracy: 0.7587 - val_loss: 0.5652 - val_accuracy: 0.6927\n",
      "Epoch 164/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5010 - accuracy: 0.7587 - val_loss: 0.5650 - val_accuracy: 0.7031\n",
      "Epoch 165/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5005 - accuracy: 0.7587 - val_loss: 0.5647 - val_accuracy: 0.7031\n",
      "Epoch 166/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5001 - accuracy: 0.7569 - val_loss: 0.5645 - val_accuracy: 0.7031\n",
      "Epoch 167/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4996 - accuracy: 0.7604 - val_loss: 0.5643 - val_accuracy: 0.7031\n",
      "Epoch 168/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4992 - accuracy: 0.7604 - val_loss: 0.5641 - val_accuracy: 0.7031\n",
      "Epoch 169/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4988 - accuracy: 0.7622 - val_loss: 0.5638 - val_accuracy: 0.7031\n",
      "Epoch 170/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4983 - accuracy: 0.7622 - val_loss: 0.5636 - val_accuracy: 0.7031\n",
      "Epoch 171/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4979 - accuracy: 0.7604 - val_loss: 0.5634 - val_accuracy: 0.7031\n",
      "Epoch 172/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4975 - accuracy: 0.7604 - val_loss: 0.5632 - val_accuracy: 0.7031\n",
      "Epoch 173/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4971 - accuracy: 0.7622 - val_loss: 0.5630 - val_accuracy: 0.7031\n",
      "Epoch 174/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4966 - accuracy: 0.7622 - val_loss: 0.5628 - val_accuracy: 0.7083\n",
      "Epoch 175/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4962 - accuracy: 0.7622 - val_loss: 0.5625 - val_accuracy: 0.7135\n",
      "Epoch 176/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4958 - accuracy: 0.7656 - val_loss: 0.5623 - val_accuracy: 0.7135\n",
      "Epoch 177/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4954 - accuracy: 0.7639 - val_loss: 0.5621 - val_accuracy: 0.7135\n",
      "Epoch 178/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4950 - accuracy: 0.7639 - val_loss: 0.5619 - val_accuracy: 0.7135\n",
      "Epoch 179/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4946 - accuracy: 0.7639 - val_loss: 0.5617 - val_accuracy: 0.7135\n",
      "Epoch 180/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4941 - accuracy: 0.7656 - val_loss: 0.5615 - val_accuracy: 0.7135\n",
      "Epoch 181/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4938 - accuracy: 0.7656 - val_loss: 0.5613 - val_accuracy: 0.7188\n",
      "Epoch 182/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4934 - accuracy: 0.7656 - val_loss: 0.5611 - val_accuracy: 0.7188\n",
      "Epoch 183/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4930 - accuracy: 0.7656 - val_loss: 0.5609 - val_accuracy: 0.7188\n",
      "Epoch 184/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4926 - accuracy: 0.7656 - val_loss: 0.5607 - val_accuracy: 0.7188\n",
      "Epoch 185/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4922 - accuracy: 0.7656 - val_loss: 0.5605 - val_accuracy: 0.7188\n",
      "Epoch 186/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4919 - accuracy: 0.7674 - val_loss: 0.5603 - val_accuracy: 0.7188\n",
      "Epoch 187/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4915 - accuracy: 0.7674 - val_loss: 0.5601 - val_accuracy: 0.7188\n",
      "Epoch 188/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4911 - accuracy: 0.7674 - val_loss: 0.5599 - val_accuracy: 0.7188\n",
      "Epoch 189/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4907 - accuracy: 0.7674 - val_loss: 0.5597 - val_accuracy: 0.7188\n",
      "Epoch 190/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4904 - accuracy: 0.7691 - val_loss: 0.5596 - val_accuracy: 0.7188\n",
      "Epoch 191/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4900 - accuracy: 0.7691 - val_loss: 0.5594 - val_accuracy: 0.7240\n",
      "Epoch 192/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4896 - accuracy: 0.7691 - val_loss: 0.5592 - val_accuracy: 0.7240\n",
      "Epoch 193/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4892 - accuracy: 0.7674 - val_loss: 0.5590 - val_accuracy: 0.7240\n",
      "Epoch 194/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4888 - accuracy: 0.7708 - val_loss: 0.5588 - val_accuracy: 0.7240\n",
      "Epoch 195/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4885 - accuracy: 0.7743 - val_loss: 0.5586 - val_accuracy: 0.7240\n",
      "Epoch 196/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4881 - accuracy: 0.7743 - val_loss: 0.5584 - val_accuracy: 0.7240\n",
      "Epoch 197/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4877 - accuracy: 0.7743 - val_loss: 0.5583 - val_accuracy: 0.7240\n",
      "Epoch 198/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4874 - accuracy: 0.7760 - val_loss: 0.5581 - val_accuracy: 0.7240\n",
      "Epoch 199/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4870 - accuracy: 0.7760 - val_loss: 0.5579 - val_accuracy: 0.7240\n",
      "Epoch 200/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4867 - accuracy: 0.7760 - val_loss: 0.5577 - val_accuracy: 0.7240\n",
      "Epoch 201/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4863 - accuracy: 0.7760 - val_loss: 0.5576 - val_accuracy: 0.7240\n",
      "Epoch 202/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4860 - accuracy: 0.7743 - val_loss: 0.5574 - val_accuracy: 0.7240\n",
      "Epoch 203/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4856 - accuracy: 0.7743 - val_loss: 0.5573 - val_accuracy: 0.7240\n",
      "Epoch 204/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4853 - accuracy: 0.7760 - val_loss: 0.5571 - val_accuracy: 0.7240\n",
      "Epoch 205/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4850 - accuracy: 0.7778 - val_loss: 0.5569 - val_accuracy: 0.7240\n",
      "Epoch 206/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4846 - accuracy: 0.7778 - val_loss: 0.5568 - val_accuracy: 0.7240\n",
      "Epoch 207/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4843 - accuracy: 0.7778 - val_loss: 0.5566 - val_accuracy: 0.7240\n",
      "Epoch 208/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4840 - accuracy: 0.7760 - val_loss: 0.5565 - val_accuracy: 0.7188\n",
      "Epoch 209/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4836 - accuracy: 0.7778 - val_loss: 0.5563 - val_accuracy: 0.7135\n",
      "Epoch 210/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4833 - accuracy: 0.7778 - val_loss: 0.5562 - val_accuracy: 0.7135\n",
      "Epoch 211/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4830 - accuracy: 0.7778 - val_loss: 0.5561 - val_accuracy: 0.7135\n",
      "Epoch 212/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4826 - accuracy: 0.7760 - val_loss: 0.5559 - val_accuracy: 0.7135\n",
      "Epoch 213/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4824 - accuracy: 0.7760 - val_loss: 0.5558 - val_accuracy: 0.7135\n",
      "Epoch 214/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4820 - accuracy: 0.7778 - val_loss: 0.5557 - val_accuracy: 0.7083\n",
      "Epoch 215/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4817 - accuracy: 0.7743 - val_loss: 0.5556 - val_accuracy: 0.7135\n",
      "Epoch 216/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4814 - accuracy: 0.7743 - val_loss: 0.5554 - val_accuracy: 0.7135\n",
      "Epoch 217/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4811 - accuracy: 0.7743 - val_loss: 0.5553 - val_accuracy: 0.7188\n",
      "Epoch 218/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4808 - accuracy: 0.7743 - val_loss: 0.5552 - val_accuracy: 0.7188\n",
      "Epoch 219/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4805 - accuracy: 0.7743 - val_loss: 0.5551 - val_accuracy: 0.7188\n",
      "Epoch 220/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4802 - accuracy: 0.7743 - val_loss: 0.5550 - val_accuracy: 0.7188\n",
      "Epoch 221/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4799 - accuracy: 0.7743 - val_loss: 0.5549 - val_accuracy: 0.7135\n",
      "Epoch 222/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4796 - accuracy: 0.7760 - val_loss: 0.5547 - val_accuracy: 0.7135\n",
      "Epoch 223/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4794 - accuracy: 0.7760 - val_loss: 0.5546 - val_accuracy: 0.7135\n",
      "Epoch 224/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4791 - accuracy: 0.7760 - val_loss: 0.5545 - val_accuracy: 0.7135\n",
      "Epoch 225/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4788 - accuracy: 0.7760 - val_loss: 0.5544 - val_accuracy: 0.7135\n",
      "Epoch 226/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4785 - accuracy: 0.7760 - val_loss: 0.5543 - val_accuracy: 0.7135\n",
      "Epoch 227/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4782 - accuracy: 0.7760 - val_loss: 0.5542 - val_accuracy: 0.7135\n",
      "Epoch 228/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4780 - accuracy: 0.7760 - val_loss: 0.5541 - val_accuracy: 0.7135\n",
      "Epoch 229/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4777 - accuracy: 0.7812 - val_loss: 0.5540 - val_accuracy: 0.7135\n",
      "Epoch 230/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4774 - accuracy: 0.7812 - val_loss: 0.5539 - val_accuracy: 0.7135\n",
      "Epoch 231/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4772 - accuracy: 0.7812 - val_loss: 0.5538 - val_accuracy: 0.7135\n",
      "Epoch 232/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4769 - accuracy: 0.7812 - val_loss: 0.5537 - val_accuracy: 0.7135\n",
      "Epoch 233/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4766 - accuracy: 0.7812 - val_loss: 0.5536 - val_accuracy: 0.7135\n",
      "Epoch 234/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4764 - accuracy: 0.7812 - val_loss: 0.5535 - val_accuracy: 0.7135\n",
      "Epoch 235/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4761 - accuracy: 0.7812 - val_loss: 0.5534 - val_accuracy: 0.7135\n",
      "Epoch 236/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4758 - accuracy: 0.7812 - val_loss: 0.5533 - val_accuracy: 0.7135\n",
      "Epoch 237/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4756 - accuracy: 0.7830 - val_loss: 0.5532 - val_accuracy: 0.7135\n",
      "Epoch 238/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4753 - accuracy: 0.7812 - val_loss: 0.5531 - val_accuracy: 0.7083\n",
      "Epoch 239/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4751 - accuracy: 0.7812 - val_loss: 0.5530 - val_accuracy: 0.7083\n",
      "Epoch 240/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4749 - accuracy: 0.7795 - val_loss: 0.5529 - val_accuracy: 0.7083\n",
      "Epoch 241/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4746 - accuracy: 0.7778 - val_loss: 0.5528 - val_accuracy: 0.7083\n",
      "Epoch 242/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4743 - accuracy: 0.7760 - val_loss: 0.5527 - val_accuracy: 0.7083\n",
      "Epoch 243/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4741 - accuracy: 0.7760 - val_loss: 0.5526 - val_accuracy: 0.7083\n",
      "Epoch 244/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4738 - accuracy: 0.7760 - val_loss: 0.5525 - val_accuracy: 0.7083\n",
      "Epoch 245/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4736 - accuracy: 0.7760 - val_loss: 0.5524 - val_accuracy: 0.7083\n",
      "Epoch 246/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4733 - accuracy: 0.7778 - val_loss: 0.5523 - val_accuracy: 0.7083\n",
      "Epoch 247/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4731 - accuracy: 0.7778 - val_loss: 0.5522 - val_accuracy: 0.7083\n",
      "Epoch 248/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4729 - accuracy: 0.7778 - val_loss: 0.5521 - val_accuracy: 0.7083\n",
      "Epoch 249/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4726 - accuracy: 0.7778 - val_loss: 0.5520 - val_accuracy: 0.7083\n",
      "Epoch 250/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4723 - accuracy: 0.7778 - val_loss: 0.5519 - val_accuracy: 0.7083\n",
      "Epoch 251/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4721 - accuracy: 0.7795 - val_loss: 0.5518 - val_accuracy: 0.7031\n",
      "Epoch 252/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4718 - accuracy: 0.7795 - val_loss: 0.5517 - val_accuracy: 0.7083\n",
      "Epoch 253/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4716 - accuracy: 0.7778 - val_loss: 0.5516 - val_accuracy: 0.7083\n",
      "Epoch 254/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4713 - accuracy: 0.7778 - val_loss: 0.5515 - val_accuracy: 0.7083\n",
      "Epoch 255/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4711 - accuracy: 0.7795 - val_loss: 0.5514 - val_accuracy: 0.7083\n",
      "Epoch 256/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4708 - accuracy: 0.7795 - val_loss: 0.5513 - val_accuracy: 0.7083\n",
      "Epoch 257/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4706 - accuracy: 0.7778 - val_loss: 0.5512 - val_accuracy: 0.7083\n",
      "Epoch 258/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4703 - accuracy: 0.7795 - val_loss: 0.5511 - val_accuracy: 0.7083\n",
      "Epoch 259/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4701 - accuracy: 0.7795 - val_loss: 0.5510 - val_accuracy: 0.7083\n",
      "Epoch 260/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4698 - accuracy: 0.7778 - val_loss: 0.5509 - val_accuracy: 0.7083\n",
      "Epoch 261/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4696 - accuracy: 0.7778 - val_loss: 0.5508 - val_accuracy: 0.7083\n",
      "Epoch 262/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4693 - accuracy: 0.7778 - val_loss: 0.5506 - val_accuracy: 0.7083\n",
      "Epoch 263/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4691 - accuracy: 0.7778 - val_loss: 0.5505 - val_accuracy: 0.7083\n",
      "Epoch 264/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4689 - accuracy: 0.7778 - val_loss: 0.5504 - val_accuracy: 0.7083\n",
      "Epoch 265/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4686 - accuracy: 0.7778 - val_loss: 0.5503 - val_accuracy: 0.7083\n",
      "Epoch 266/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4684 - accuracy: 0.7778 - val_loss: 0.5502 - val_accuracy: 0.7083\n",
      "Epoch 267/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4682 - accuracy: 0.7778 - val_loss: 0.5501 - val_accuracy: 0.7083\n",
      "Epoch 268/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4679 - accuracy: 0.7778 - val_loss: 0.5500 - val_accuracy: 0.7083\n",
      "Epoch 269/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4677 - accuracy: 0.7795 - val_loss: 0.5499 - val_accuracy: 0.7083\n",
      "Epoch 270/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4674 - accuracy: 0.7795 - val_loss: 0.5498 - val_accuracy: 0.7188\n",
      "Epoch 271/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4672 - accuracy: 0.7795 - val_loss: 0.5497 - val_accuracy: 0.7188\n",
      "Epoch 272/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4669 - accuracy: 0.7795 - val_loss: 0.5497 - val_accuracy: 0.7188\n",
      "Epoch 273/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4667 - accuracy: 0.7795 - val_loss: 0.5496 - val_accuracy: 0.7188\n",
      "Epoch 274/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4664 - accuracy: 0.7795 - val_loss: 0.5495 - val_accuracy: 0.7188\n",
      "Epoch 275/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4662 - accuracy: 0.7812 - val_loss: 0.5494 - val_accuracy: 0.7188\n",
      "Epoch 276/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4660 - accuracy: 0.7812 - val_loss: 0.5493 - val_accuracy: 0.7188\n",
      "Epoch 277/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4657 - accuracy: 0.7812 - val_loss: 0.5492 - val_accuracy: 0.7188\n",
      "Epoch 278/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4655 - accuracy: 0.7830 - val_loss: 0.5491 - val_accuracy: 0.7188\n",
      "Epoch 279/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4653 - accuracy: 0.7830 - val_loss: 0.5490 - val_accuracy: 0.7188\n",
      "Epoch 280/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4651 - accuracy: 0.7830 - val_loss: 0.5489 - val_accuracy: 0.7188\n",
      "Epoch 281/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4648 - accuracy: 0.7830 - val_loss: 0.5488 - val_accuracy: 0.7188\n",
      "Epoch 282/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4646 - accuracy: 0.7847 - val_loss: 0.5488 - val_accuracy: 0.7188\n",
      "Epoch 283/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4644 - accuracy: 0.7830 - val_loss: 0.5487 - val_accuracy: 0.7188\n",
      "Epoch 284/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4642 - accuracy: 0.7865 - val_loss: 0.5486 - val_accuracy: 0.7188\n",
      "Epoch 285/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4640 - accuracy: 0.7865 - val_loss: 0.5485 - val_accuracy: 0.7240\n",
      "Epoch 286/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4638 - accuracy: 0.7882 - val_loss: 0.5484 - val_accuracy: 0.7240\n",
      "Epoch 287/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4636 - accuracy: 0.7865 - val_loss: 0.5483 - val_accuracy: 0.7240\n",
      "Epoch 288/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4634 - accuracy: 0.7882 - val_loss: 0.5482 - val_accuracy: 0.7240\n",
      "Epoch 289/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4632 - accuracy: 0.7882 - val_loss: 0.5482 - val_accuracy: 0.7240\n",
      "Epoch 290/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4630 - accuracy: 0.7882 - val_loss: 0.5481 - val_accuracy: 0.7240\n",
      "Epoch 291/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4628 - accuracy: 0.7865 - val_loss: 0.5480 - val_accuracy: 0.7240\n",
      "Epoch 292/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4626 - accuracy: 0.7865 - val_loss: 0.5479 - val_accuracy: 0.7240\n",
      "Epoch 293/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4625 - accuracy: 0.7865 - val_loss: 0.5478 - val_accuracy: 0.7240\n",
      "Epoch 294/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4623 - accuracy: 0.7865 - val_loss: 0.5477 - val_accuracy: 0.7240\n",
      "Epoch 295/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4621 - accuracy: 0.7865 - val_loss: 0.5477 - val_accuracy: 0.7240\n",
      "Epoch 296/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4619 - accuracy: 0.7865 - val_loss: 0.5476 - val_accuracy: 0.7240\n",
      "Epoch 297/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4617 - accuracy: 0.7847 - val_loss: 0.5475 - val_accuracy: 0.7240\n",
      "Epoch 298/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4616 - accuracy: 0.7865 - val_loss: 0.5474 - val_accuracy: 0.7240\n",
      "Epoch 299/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4614 - accuracy: 0.7865 - val_loss: 0.5473 - val_accuracy: 0.7292\n",
      "Epoch 300/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4612 - accuracy: 0.7847 - val_loss: 0.5472 - val_accuracy: 0.7292\n",
      "Epoch 301/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4611 - accuracy: 0.7847 - val_loss: 0.5471 - val_accuracy: 0.7292\n",
      "Epoch 302/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4609 - accuracy: 0.7847 - val_loss: 0.5471 - val_accuracy: 0.7292\n",
      "Epoch 303/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4607 - accuracy: 0.7847 - val_loss: 0.5470 - val_accuracy: 0.7292\n",
      "Epoch 304/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4606 - accuracy: 0.7847 - val_loss: 0.5469 - val_accuracy: 0.7292\n",
      "Epoch 305/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4604 - accuracy: 0.7830 - val_loss: 0.5468 - val_accuracy: 0.7292\n",
      "Epoch 306/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4602 - accuracy: 0.7847 - val_loss: 0.5467 - val_accuracy: 0.7344\n",
      "Epoch 307/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4601 - accuracy: 0.7865 - val_loss: 0.5467 - val_accuracy: 0.7292\n",
      "Epoch 308/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4599 - accuracy: 0.7847 - val_loss: 0.5466 - val_accuracy: 0.7292\n",
      "Epoch 309/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4598 - accuracy: 0.7865 - val_loss: 0.5465 - val_accuracy: 0.7292\n",
      "Epoch 310/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4596 - accuracy: 0.7865 - val_loss: 0.5465 - val_accuracy: 0.7292\n",
      "Epoch 311/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4594 - accuracy: 0.7865 - val_loss: 0.5464 - val_accuracy: 0.7292\n",
      "Epoch 312/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4593 - accuracy: 0.7847 - val_loss: 0.5463 - val_accuracy: 0.7292\n",
      "Epoch 313/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4591 - accuracy: 0.7847 - val_loss: 0.5462 - val_accuracy: 0.7292\n",
      "Epoch 314/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4589 - accuracy: 0.7847 - val_loss: 0.5462 - val_accuracy: 0.7292\n",
      "Epoch 315/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4588 - accuracy: 0.7847 - val_loss: 0.5461 - val_accuracy: 0.7292\n",
      "Epoch 316/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4587 - accuracy: 0.7830 - val_loss: 0.5460 - val_accuracy: 0.7292\n",
      "Epoch 317/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4585 - accuracy: 0.7830 - val_loss: 0.5460 - val_accuracy: 0.7292\n",
      "Epoch 318/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4584 - accuracy: 0.7830 - val_loss: 0.5459 - val_accuracy: 0.7292\n",
      "Epoch 319/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4582 - accuracy: 0.7830 - val_loss: 0.5458 - val_accuracy: 0.7292\n",
      "Epoch 320/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4581 - accuracy: 0.7830 - val_loss: 0.5457 - val_accuracy: 0.7344\n",
      "Epoch 321/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4579 - accuracy: 0.7830 - val_loss: 0.5456 - val_accuracy: 0.7292\n",
      "Epoch 322/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4578 - accuracy: 0.7812 - val_loss: 0.5456 - val_accuracy: 0.7292\n",
      "Epoch 323/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4577 - accuracy: 0.7830 - val_loss: 0.5455 - val_accuracy: 0.7240\n",
      "Epoch 324/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4575 - accuracy: 0.7812 - val_loss: 0.5454 - val_accuracy: 0.7240\n",
      "Epoch 325/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4574 - accuracy: 0.7812 - val_loss: 0.5453 - val_accuracy: 0.7240\n",
      "Epoch 326/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4572 - accuracy: 0.7830 - val_loss: 0.5453 - val_accuracy: 0.7240\n",
      "Epoch 327/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4571 - accuracy: 0.7812 - val_loss: 0.5452 - val_accuracy: 0.7240\n",
      "Epoch 328/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4570 - accuracy: 0.7830 - val_loss: 0.5451 - val_accuracy: 0.7240\n",
      "Epoch 329/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4568 - accuracy: 0.7830 - val_loss: 0.5451 - val_accuracy: 0.7240\n",
      "Epoch 330/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4567 - accuracy: 0.7830 - val_loss: 0.5450 - val_accuracy: 0.7240\n",
      "Epoch 331/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4566 - accuracy: 0.7830 - val_loss: 0.5449 - val_accuracy: 0.7240\n",
      "Epoch 332/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4564 - accuracy: 0.7830 - val_loss: 0.5449 - val_accuracy: 0.7240\n",
      "Epoch 333/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4563 - accuracy: 0.7830 - val_loss: 0.5448 - val_accuracy: 0.7240\n",
      "Epoch 334/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4562 - accuracy: 0.7830 - val_loss: 0.5447 - val_accuracy: 0.7240\n",
      "Epoch 335/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4560 - accuracy: 0.7830 - val_loss: 0.5447 - val_accuracy: 0.7240\n",
      "Epoch 336/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4559 - accuracy: 0.7847 - val_loss: 0.5446 - val_accuracy: 0.7292\n",
      "Epoch 337/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4558 - accuracy: 0.7830 - val_loss: 0.5445 - val_accuracy: 0.7292\n",
      "Epoch 338/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4556 - accuracy: 0.7830 - val_loss: 0.5445 - val_accuracy: 0.7292\n",
      "Epoch 339/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4555 - accuracy: 0.7812 - val_loss: 0.5444 - val_accuracy: 0.7292\n",
      "Epoch 340/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4554 - accuracy: 0.7812 - val_loss: 0.5443 - val_accuracy: 0.7292\n",
      "Epoch 341/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4553 - accuracy: 0.7812 - val_loss: 0.5443 - val_accuracy: 0.7292\n",
      "Epoch 342/1500\n",
      "18/18 [==============================] - 0s 998us/step - loss: 0.4552 - accuracy: 0.7812 - val_loss: 0.5442 - val_accuracy: 0.7292\n",
      "Epoch 343/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4550 - accuracy: 0.7795 - val_loss: 0.5442 - val_accuracy: 0.7292\n",
      "Epoch 344/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4549 - accuracy: 0.7812 - val_loss: 0.5441 - val_accuracy: 0.7292\n",
      "Epoch 345/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4548 - accuracy: 0.7812 - val_loss: 0.5440 - val_accuracy: 0.7292\n",
      "Epoch 346/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4547 - accuracy: 0.7795 - val_loss: 0.5440 - val_accuracy: 0.7292\n",
      "Epoch 347/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4546 - accuracy: 0.7795 - val_loss: 0.5439 - val_accuracy: 0.7292\n",
      "Epoch 348/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4544 - accuracy: 0.7795 - val_loss: 0.5439 - val_accuracy: 0.7292\n",
      "Epoch 349/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4544 - accuracy: 0.7795 - val_loss: 0.5438 - val_accuracy: 0.7292\n",
      "Epoch 350/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4542 - accuracy: 0.7795 - val_loss: 0.5438 - val_accuracy: 0.7292\n",
      "Epoch 351/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4541 - accuracy: 0.7795 - val_loss: 0.5437 - val_accuracy: 0.7292\n",
      "Epoch 352/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4540 - accuracy: 0.7795 - val_loss: 0.5437 - val_accuracy: 0.7292\n",
      "Epoch 353/1500\n",
      "18/18 [==============================] - 0s 942us/step - loss: 0.4539 - accuracy: 0.7795 - val_loss: 0.5436 - val_accuracy: 0.7292\n",
      "Epoch 354/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4538 - accuracy: 0.7795 - val_loss: 0.5436 - val_accuracy: 0.7292\n",
      "Epoch 355/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4537 - accuracy: 0.7795 - val_loss: 0.5435 - val_accuracy: 0.7292\n",
      "Epoch 356/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4536 - accuracy: 0.7795 - val_loss: 0.5434 - val_accuracy: 0.7292\n",
      "Epoch 357/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4535 - accuracy: 0.7795 - val_loss: 0.5434 - val_accuracy: 0.7292\n",
      "Epoch 358/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4534 - accuracy: 0.7812 - val_loss: 0.5433 - val_accuracy: 0.7292\n",
      "Epoch 359/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4533 - accuracy: 0.7812 - val_loss: 0.5433 - val_accuracy: 0.7292\n",
      "Epoch 360/1500\n",
      "18/18 [==============================] - 0s 999us/step - loss: 0.4532 - accuracy: 0.7812 - val_loss: 0.5432 - val_accuracy: 0.7292\n",
      "Epoch 361/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4531 - accuracy: 0.7812 - val_loss: 0.5432 - val_accuracy: 0.7292\n",
      "Epoch 362/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4530 - accuracy: 0.7812 - val_loss: 0.5431 - val_accuracy: 0.7292\n",
      "Epoch 363/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4529 - accuracy: 0.7812 - val_loss: 0.5431 - val_accuracy: 0.7292\n",
      "Epoch 364/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4528 - accuracy: 0.7830 - val_loss: 0.5430 - val_accuracy: 0.7292\n",
      "Epoch 365/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4527 - accuracy: 0.7812 - val_loss: 0.5429 - val_accuracy: 0.7292\n",
      "Epoch 366/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4526 - accuracy: 0.7830 - val_loss: 0.5429 - val_accuracy: 0.7292\n",
      "Epoch 367/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4525 - accuracy: 0.7830 - val_loss: 0.5428 - val_accuracy: 0.7292\n",
      "Epoch 368/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4524 - accuracy: 0.7830 - val_loss: 0.5428 - val_accuracy: 0.7292\n",
      "Epoch 369/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4523 - accuracy: 0.7830 - val_loss: 0.5427 - val_accuracy: 0.7292\n",
      "Epoch 370/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4522 - accuracy: 0.7830 - val_loss: 0.5426 - val_accuracy: 0.7292\n",
      "Epoch 371/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4521 - accuracy: 0.7830 - val_loss: 0.5426 - val_accuracy: 0.7292\n",
      "Epoch 372/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4520 - accuracy: 0.7830 - val_loss: 0.5425 - val_accuracy: 0.7292\n",
      "Epoch 373/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4519 - accuracy: 0.7847 - val_loss: 0.5425 - val_accuracy: 0.7292\n",
      "Epoch 374/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4518 - accuracy: 0.7847 - val_loss: 0.5424 - val_accuracy: 0.7292\n",
      "Epoch 375/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4517 - accuracy: 0.7847 - val_loss: 0.5423 - val_accuracy: 0.7240\n",
      "Epoch 376/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4517 - accuracy: 0.7847 - val_loss: 0.5423 - val_accuracy: 0.7240\n",
      "Epoch 377/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4516 - accuracy: 0.7847 - val_loss: 0.5422 - val_accuracy: 0.7240\n",
      "Epoch 378/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4515 - accuracy: 0.7847 - val_loss: 0.5422 - val_accuracy: 0.7240\n",
      "Epoch 379/1500\n",
      "18/18 [==============================] - 0s 977us/step - loss: 0.4514 - accuracy: 0.7847 - val_loss: 0.5421 - val_accuracy: 0.7240\n",
      "Epoch 380/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4513 - accuracy: 0.7847 - val_loss: 0.5420 - val_accuracy: 0.7240\n",
      "Epoch 381/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4513 - accuracy: 0.7847 - val_loss: 0.5420 - val_accuracy: 0.7240\n",
      "Epoch 382/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4511 - accuracy: 0.7847 - val_loss: 0.5419 - val_accuracy: 0.7240\n",
      "Epoch 383/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4510 - accuracy: 0.7847 - val_loss: 0.5419 - val_accuracy: 0.7240\n",
      "Epoch 384/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4510 - accuracy: 0.7847 - val_loss: 0.5418 - val_accuracy: 0.7240\n",
      "Epoch 385/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4509 - accuracy: 0.7847 - val_loss: 0.5418 - val_accuracy: 0.7240\n",
      "Epoch 386/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4508 - accuracy: 0.7847 - val_loss: 0.5417 - val_accuracy: 0.7240\n",
      "Epoch 387/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4507 - accuracy: 0.7847 - val_loss: 0.5417 - val_accuracy: 0.7240\n",
      "Epoch 388/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4506 - accuracy: 0.7847 - val_loss: 0.5416 - val_accuracy: 0.7240\n",
      "Epoch 389/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4506 - accuracy: 0.7830 - val_loss: 0.5416 - val_accuracy: 0.7240\n",
      "Epoch 390/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4505 - accuracy: 0.7847 - val_loss: 0.5415 - val_accuracy: 0.7240\n",
      "Epoch 391/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4504 - accuracy: 0.7830 - val_loss: 0.5415 - val_accuracy: 0.7240\n",
      "Epoch 392/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4503 - accuracy: 0.7847 - val_loss: 0.5414 - val_accuracy: 0.7240\n",
      "Epoch 393/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4502 - accuracy: 0.7830 - val_loss: 0.5414 - val_accuracy: 0.7240\n",
      "Epoch 394/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4502 - accuracy: 0.7830 - val_loss: 0.5413 - val_accuracy: 0.7240\n",
      "Epoch 395/1500\n",
      "18/18 [==============================] - 0s 992us/step - loss: 0.4501 - accuracy: 0.7830 - val_loss: 0.5413 - val_accuracy: 0.7240\n",
      "Epoch 396/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4500 - accuracy: 0.7830 - val_loss: 0.5412 - val_accuracy: 0.7240\n",
      "Epoch 397/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4500 - accuracy: 0.7830 - val_loss: 0.5412 - val_accuracy: 0.7240\n",
      "Epoch 398/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4498 - accuracy: 0.7830 - val_loss: 0.5411 - val_accuracy: 0.7240\n",
      "Epoch 399/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4498 - accuracy: 0.7830 - val_loss: 0.5411 - val_accuracy: 0.7240\n",
      "Epoch 400/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4497 - accuracy: 0.7830 - val_loss: 0.5410 - val_accuracy: 0.7240\n",
      "Epoch 401/1500\n",
      "18/18 [==============================] - 0s 990us/step - loss: 0.4496 - accuracy: 0.7830 - val_loss: 0.5410 - val_accuracy: 0.7240\n",
      "Epoch 402/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4496 - accuracy: 0.7830 - val_loss: 0.5410 - val_accuracy: 0.7240\n",
      "Epoch 403/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4495 - accuracy: 0.7812 - val_loss: 0.5409 - val_accuracy: 0.7240\n",
      "Epoch 404/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4495 - accuracy: 0.7830 - val_loss: 0.5409 - val_accuracy: 0.7240\n",
      "Epoch 405/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4493 - accuracy: 0.7830 - val_loss: 0.5408 - val_accuracy: 0.7240\n",
      "Epoch 406/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4493 - accuracy: 0.7830 - val_loss: 0.5408 - val_accuracy: 0.7240\n",
      "Epoch 407/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4492 - accuracy: 0.7830 - val_loss: 0.5407 - val_accuracy: 0.7240\n",
      "Epoch 408/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4491 - accuracy: 0.7830 - val_loss: 0.5407 - val_accuracy: 0.7240\n",
      "Epoch 409/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4491 - accuracy: 0.7847 - val_loss: 0.5407 - val_accuracy: 0.7240\n",
      "Epoch 410/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4490 - accuracy: 0.7830 - val_loss: 0.5406 - val_accuracy: 0.7240\n",
      "Epoch 411/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4489 - accuracy: 0.7847 - val_loss: 0.5406 - val_accuracy: 0.7240\n",
      "Epoch 412/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4489 - accuracy: 0.7847 - val_loss: 0.5405 - val_accuracy: 0.7240\n",
      "Epoch 413/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4488 - accuracy: 0.7847 - val_loss: 0.5405 - val_accuracy: 0.7240\n",
      "Epoch 414/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4487 - accuracy: 0.7847 - val_loss: 0.5404 - val_accuracy: 0.7240\n",
      "Epoch 415/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4486 - accuracy: 0.7847 - val_loss: 0.5404 - val_accuracy: 0.7240\n",
      "Epoch 416/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4486 - accuracy: 0.7847 - val_loss: 0.5404 - val_accuracy: 0.7240\n",
      "Epoch 417/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4485 - accuracy: 0.7847 - val_loss: 0.5403 - val_accuracy: 0.7240\n",
      "Epoch 418/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4485 - accuracy: 0.7847 - val_loss: 0.5403 - val_accuracy: 0.7240\n",
      "Epoch 419/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4484 - accuracy: 0.7847 - val_loss: 0.5402 - val_accuracy: 0.7240\n",
      "Epoch 420/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4483 - accuracy: 0.7847 - val_loss: 0.5402 - val_accuracy: 0.7240\n",
      "Epoch 421/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4482 - accuracy: 0.7847 - val_loss: 0.5401 - val_accuracy: 0.7240\n",
      "Epoch 422/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4482 - accuracy: 0.7847 - val_loss: 0.5401 - val_accuracy: 0.7240\n",
      "Epoch 423/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4481 - accuracy: 0.7847 - val_loss: 0.5401 - val_accuracy: 0.7240\n",
      "Epoch 424/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4481 - accuracy: 0.7830 - val_loss: 0.5400 - val_accuracy: 0.7240\n",
      "Epoch 425/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4480 - accuracy: 0.7847 - val_loss: 0.5400 - val_accuracy: 0.7240\n",
      "Epoch 426/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4480 - accuracy: 0.7830 - val_loss: 0.5400 - val_accuracy: 0.7240\n",
      "Epoch 427/1500\n",
      "18/18 [==============================] - 0s 984us/step - loss: 0.4479 - accuracy: 0.7830 - val_loss: 0.5399 - val_accuracy: 0.7240\n",
      "Epoch 428/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4479 - accuracy: 0.7830 - val_loss: 0.5399 - val_accuracy: 0.7240\n",
      "Epoch 429/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4478 - accuracy: 0.7830 - val_loss: 0.5398 - val_accuracy: 0.7240\n",
      "Epoch 430/1500\n",
      "18/18 [==============================] - 0s 987us/step - loss: 0.4477 - accuracy: 0.7830 - val_loss: 0.5398 - val_accuracy: 0.7240\n",
      "Epoch 431/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4477 - accuracy: 0.7830 - val_loss: 0.5398 - val_accuracy: 0.7240\n",
      "Epoch 432/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4476 - accuracy: 0.7830 - val_loss: 0.5397 - val_accuracy: 0.7240\n",
      "Epoch 433/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4475 - accuracy: 0.7830 - val_loss: 0.5397 - val_accuracy: 0.7240\n",
      "Epoch 434/1500\n",
      "18/18 [==============================] - 0s 972us/step - loss: 0.4475 - accuracy: 0.7830 - val_loss: 0.5397 - val_accuracy: 0.7240\n",
      "Epoch 435/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4474 - accuracy: 0.7830 - val_loss: 0.5396 - val_accuracy: 0.7240\n",
      "Epoch 436/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4474 - accuracy: 0.7830 - val_loss: 0.5396 - val_accuracy: 0.7240\n",
      "Epoch 437/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4473 - accuracy: 0.7830 - val_loss: 0.5395 - val_accuracy: 0.7240\n",
      "Epoch 438/1500\n",
      "18/18 [==============================] - 0s 999us/step - loss: 0.4473 - accuracy: 0.7847 - val_loss: 0.5395 - val_accuracy: 0.7240\n",
      "Epoch 439/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4472 - accuracy: 0.7830 - val_loss: 0.5394 - val_accuracy: 0.7240\n",
      "Epoch 440/1500\n",
      "18/18 [==============================] - 0s 999us/step - loss: 0.4472 - accuracy: 0.7847 - val_loss: 0.5394 - val_accuracy: 0.7240\n",
      "Epoch 441/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4471 - accuracy: 0.7847 - val_loss: 0.5394 - val_accuracy: 0.7240\n",
      "Epoch 442/1500\n",
      "18/18 [==============================] - 0s 995us/step - loss: 0.4471 - accuracy: 0.7830 - val_loss: 0.5393 - val_accuracy: 0.7240\n",
      "Epoch 443/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4470 - accuracy: 0.7830 - val_loss: 0.5393 - val_accuracy: 0.7240\n",
      "Epoch 444/1500\n",
      "18/18 [==============================] - 0s 967us/step - loss: 0.4470 - accuracy: 0.7830 - val_loss: 0.5392 - val_accuracy: 0.7240\n",
      "Epoch 445/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4469 - accuracy: 0.7847 - val_loss: 0.5392 - val_accuracy: 0.7240\n",
      "Epoch 446/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4469 - accuracy: 0.7847 - val_loss: 0.5391 - val_accuracy: 0.7240\n",
      "Epoch 447/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4468 - accuracy: 0.7865 - val_loss: 0.5391 - val_accuracy: 0.7240\n",
      "Epoch 448/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4468 - accuracy: 0.7865 - val_loss: 0.5390 - val_accuracy: 0.7240\n",
      "Epoch 449/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4467 - accuracy: 0.7882 - val_loss: 0.5390 - val_accuracy: 0.7240\n",
      "Epoch 450/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4467 - accuracy: 0.7865 - val_loss: 0.5389 - val_accuracy: 0.7240\n",
      "Epoch 451/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4466 - accuracy: 0.7882 - val_loss: 0.5389 - val_accuracy: 0.7240\n",
      "Epoch 452/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4466 - accuracy: 0.7865 - val_loss: 0.5389 - val_accuracy: 0.7240\n",
      "Epoch 453/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4465 - accuracy: 0.7882 - val_loss: 0.5388 - val_accuracy: 0.7240\n",
      "Epoch 454/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4465 - accuracy: 0.7865 - val_loss: 0.5388 - val_accuracy: 0.7240\n",
      "Epoch 455/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4465 - accuracy: 0.7882 - val_loss: 0.5387 - val_accuracy: 0.7240\n",
      "Epoch 456/1500\n",
      "18/18 [==============================] - 0s 983us/step - loss: 0.4464 - accuracy: 0.7882 - val_loss: 0.5387 - val_accuracy: 0.7240\n",
      "Epoch 457/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4464 - accuracy: 0.7882 - val_loss: 0.5386 - val_accuracy: 0.7240\n",
      "Epoch 458/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4463 - accuracy: 0.7882 - val_loss: 0.5386 - val_accuracy: 0.7240\n",
      "Epoch 459/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4463 - accuracy: 0.7882 - val_loss: 0.5386 - val_accuracy: 0.7240\n",
      "Epoch 460/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4462 - accuracy: 0.7882 - val_loss: 0.5385 - val_accuracy: 0.7240\n",
      "Epoch 461/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4462 - accuracy: 0.7865 - val_loss: 0.5385 - val_accuracy: 0.7240\n",
      "Epoch 462/1500\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.4461 - accuracy: 0.7899 - val_loss: 0.5384 - val_accuracy: 0.7240\n",
      "Epoch 463/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4461 - accuracy: 0.7882 - val_loss: 0.5384 - val_accuracy: 0.7240\n",
      "Epoch 464/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4461 - accuracy: 0.7899 - val_loss: 0.5383 - val_accuracy: 0.7240\n",
      "Epoch 465/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4460 - accuracy: 0.7899 - val_loss: 0.5383 - val_accuracy: 0.7240\n",
      "Epoch 466/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4460 - accuracy: 0.7917 - val_loss: 0.5382 - val_accuracy: 0.7240\n",
      "Epoch 467/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4459 - accuracy: 0.7917 - val_loss: 0.5382 - val_accuracy: 0.7240\n",
      "Epoch 468/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4459 - accuracy: 0.7917 - val_loss: 0.5381 - val_accuracy: 0.7240\n",
      "Epoch 469/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4459 - accuracy: 0.7917 - val_loss: 0.5381 - val_accuracy: 0.7240\n",
      "Epoch 470/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4458 - accuracy: 0.7899 - val_loss: 0.5381 - val_accuracy: 0.7240\n",
      "Epoch 471/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4458 - accuracy: 0.7917 - val_loss: 0.5380 - val_accuracy: 0.7240\n",
      "Epoch 472/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4457 - accuracy: 0.7899 - val_loss: 0.5380 - val_accuracy: 0.7240\n",
      "Epoch 473/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4457 - accuracy: 0.7899 - val_loss: 0.5379 - val_accuracy: 0.7240\n",
      "Epoch 474/1500\n",
      "18/18 [==============================] - 0s 979us/step - loss: 0.4456 - accuracy: 0.7899 - val_loss: 0.5379 - val_accuracy: 0.7188\n",
      "Epoch 475/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4456 - accuracy: 0.7899 - val_loss: 0.5379 - val_accuracy: 0.7188\n",
      "Epoch 476/1500\n",
      "18/18 [==============================] - 0s 979us/step - loss: 0.4455 - accuracy: 0.7899 - val_loss: 0.5378 - val_accuracy: 0.7188\n",
      "Epoch 477/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4456 - accuracy: 0.7899 - val_loss: 0.5378 - val_accuracy: 0.7188\n",
      "Epoch 478/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4455 - accuracy: 0.7899 - val_loss: 0.5377 - val_accuracy: 0.7188\n",
      "Epoch 479/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4454 - accuracy: 0.7899 - val_loss: 0.5377 - val_accuracy: 0.7188\n",
      "Epoch 480/1500\n",
      "18/18 [==============================] - 0s 974us/step - loss: 0.4454 - accuracy: 0.7899 - val_loss: 0.5377 - val_accuracy: 0.7188\n",
      "Epoch 481/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4454 - accuracy: 0.7899 - val_loss: 0.5376 - val_accuracy: 0.7188\n",
      "Epoch 482/1500\n",
      "18/18 [==============================] - 0s 971us/step - loss: 0.4453 - accuracy: 0.7899 - val_loss: 0.5376 - val_accuracy: 0.7188\n",
      "Epoch 483/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4453 - accuracy: 0.7899 - val_loss: 0.5375 - val_accuracy: 0.7188\n",
      "Epoch 484/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4453 - accuracy: 0.7899 - val_loss: 0.5375 - val_accuracy: 0.7188\n",
      "Epoch 485/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4452 - accuracy: 0.7899 - val_loss: 0.5374 - val_accuracy: 0.7188\n",
      "Epoch 486/1500\n",
      "18/18 [==============================] - 0s 984us/step - loss: 0.4452 - accuracy: 0.7899 - val_loss: 0.5374 - val_accuracy: 0.7188\n",
      "Epoch 487/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4452 - accuracy: 0.7899 - val_loss: 0.5374 - val_accuracy: 0.7188\n",
      "Epoch 488/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4451 - accuracy: 0.7899 - val_loss: 0.5373 - val_accuracy: 0.7188\n",
      "Epoch 489/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4451 - accuracy: 0.7899 - val_loss: 0.5373 - val_accuracy: 0.7188\n",
      "Epoch 490/1500\n",
      "18/18 [==============================] - 0s 996us/step - loss: 0.4451 - accuracy: 0.7899 - val_loss: 0.5372 - val_accuracy: 0.7188\n",
      "Epoch 491/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4450 - accuracy: 0.7899 - val_loss: 0.5372 - val_accuracy: 0.7188\n",
      "Epoch 492/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4450 - accuracy: 0.7899 - val_loss: 0.5372 - val_accuracy: 0.7188\n",
      "Epoch 493/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4449 - accuracy: 0.7899 - val_loss: 0.5371 - val_accuracy: 0.7188\n",
      "Epoch 494/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4449 - accuracy: 0.7899 - val_loss: 0.5371 - val_accuracy: 0.7240\n",
      "Epoch 495/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4449 - accuracy: 0.7899 - val_loss: 0.5371 - val_accuracy: 0.7240\n",
      "Epoch 496/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4449 - accuracy: 0.7899 - val_loss: 0.5370 - val_accuracy: 0.7240\n",
      "Epoch 497/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4448 - accuracy: 0.7917 - val_loss: 0.5370 - val_accuracy: 0.7240\n",
      "Epoch 498/1500\n",
      "18/18 [==============================] - 0s 988us/step - loss: 0.4448 - accuracy: 0.7917 - val_loss: 0.5369 - val_accuracy: 0.7240\n",
      "Epoch 499/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4447 - accuracy: 0.7917 - val_loss: 0.5369 - val_accuracy: 0.7240\n",
      "Epoch 500/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4447 - accuracy: 0.7917 - val_loss: 0.5369 - val_accuracy: 0.7240\n",
      "Epoch 501/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4447 - accuracy: 0.7917 - val_loss: 0.5368 - val_accuracy: 0.7240\n",
      "Epoch 502/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4447 - accuracy: 0.7917 - val_loss: 0.5368 - val_accuracy: 0.7240\n",
      "Epoch 503/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4446 - accuracy: 0.7917 - val_loss: 0.5367 - val_accuracy: 0.7240\n",
      "Epoch 504/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4446 - accuracy: 0.7917 - val_loss: 0.5367 - val_accuracy: 0.7240\n",
      "Epoch 505/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4445 - accuracy: 0.7917 - val_loss: 0.5367 - val_accuracy: 0.7240\n",
      "Epoch 506/1500\n",
      "18/18 [==============================] - 0s 999us/step - loss: 0.4445 - accuracy: 0.7917 - val_loss: 0.5366 - val_accuracy: 0.7240\n",
      "Epoch 507/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4445 - accuracy: 0.7917 - val_loss: 0.5366 - val_accuracy: 0.7240\n",
      "Epoch 508/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4444 - accuracy: 0.7917 - val_loss: 0.5365 - val_accuracy: 0.7240\n",
      "Epoch 509/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4444 - accuracy: 0.7917 - val_loss: 0.5365 - val_accuracy: 0.7292\n",
      "Epoch 510/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4444 - accuracy: 0.7917 - val_loss: 0.5365 - val_accuracy: 0.7292\n",
      "Epoch 511/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4443 - accuracy: 0.7917 - val_loss: 0.5364 - val_accuracy: 0.7292\n",
      "Epoch 512/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4443 - accuracy: 0.7917 - val_loss: 0.5364 - val_accuracy: 0.7292\n",
      "Epoch 513/1500\n",
      "18/18 [==============================] - 0s 994us/step - loss: 0.4442 - accuracy: 0.7917 - val_loss: 0.5364 - val_accuracy: 0.7292\n",
      "Epoch 514/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4442 - accuracy: 0.7917 - val_loss: 0.5363 - val_accuracy: 0.7292\n",
      "Epoch 515/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4442 - accuracy: 0.7917 - val_loss: 0.5363 - val_accuracy: 0.7292\n",
      "Epoch 516/1500\n",
      "18/18 [==============================] - 0s 970us/step - loss: 0.4442 - accuracy: 0.7917 - val_loss: 0.5363 - val_accuracy: 0.7292\n",
      "Epoch 517/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4441 - accuracy: 0.7917 - val_loss: 0.5362 - val_accuracy: 0.7292\n",
      "Epoch 518/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4441 - accuracy: 0.7917 - val_loss: 0.5362 - val_accuracy: 0.7292\n",
      "Epoch 519/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4441 - accuracy: 0.7917 - val_loss: 0.5362 - val_accuracy: 0.7292\n",
      "Epoch 520/1500\n",
      "18/18 [==============================] - 0s 967us/step - loss: 0.4441 - accuracy: 0.7917 - val_loss: 0.5361 - val_accuracy: 0.7292\n",
      "Epoch 521/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4440 - accuracy: 0.7917 - val_loss: 0.5361 - val_accuracy: 0.7292\n",
      "Epoch 522/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4440 - accuracy: 0.7917 - val_loss: 0.5361 - val_accuracy: 0.7292\n",
      "Epoch 523/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4440 - accuracy: 0.7917 - val_loss: 0.5360 - val_accuracy: 0.7292\n",
      "Epoch 524/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4439 - accuracy: 0.7917 - val_loss: 0.5360 - val_accuracy: 0.7292\n",
      "Epoch 525/1500\n",
      "18/18 [==============================] - 0s 982us/step - loss: 0.4439 - accuracy: 0.7917 - val_loss: 0.5359 - val_accuracy: 0.7292\n",
      "Epoch 526/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4439 - accuracy: 0.7917 - val_loss: 0.5359 - val_accuracy: 0.7292\n",
      "Epoch 527/1500\n",
      "18/18 [==============================] - 0s 981us/step - loss: 0.4439 - accuracy: 0.7917 - val_loss: 0.5359 - val_accuracy: 0.7292\n",
      "Epoch 528/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4438 - accuracy: 0.7917 - val_loss: 0.5358 - val_accuracy: 0.7292\n",
      "Epoch 529/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4438 - accuracy: 0.7917 - val_loss: 0.5358 - val_accuracy: 0.7292\n",
      "Epoch 530/1500\n",
      "18/18 [==============================] - 0s 994us/step - loss: 0.4437 - accuracy: 0.7917 - val_loss: 0.5358 - val_accuracy: 0.7292\n",
      "Epoch 531/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4437 - accuracy: 0.7917 - val_loss: 0.5357 - val_accuracy: 0.7292\n",
      "Epoch 532/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4437 - accuracy: 0.7917 - val_loss: 0.5357 - val_accuracy: 0.7292\n",
      "Epoch 533/1500\n",
      "18/18 [==============================] - 0s 955us/step - loss: 0.4437 - accuracy: 0.7917 - val_loss: 0.5357 - val_accuracy: 0.7344\n",
      "Epoch 534/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4436 - accuracy: 0.7917 - val_loss: 0.5356 - val_accuracy: 0.7344\n",
      "Epoch 535/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4436 - accuracy: 0.7917 - val_loss: 0.5356 - val_accuracy: 0.7344\n",
      "Epoch 536/1500\n",
      "18/18 [==============================] - 0s 975us/step - loss: 0.4436 - accuracy: 0.7917 - val_loss: 0.5356 - val_accuracy: 0.7344\n",
      "Epoch 537/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4435 - accuracy: 0.7917 - val_loss: 0.5355 - val_accuracy: 0.7344\n",
      "Epoch 538/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4435 - accuracy: 0.7917 - val_loss: 0.5355 - val_accuracy: 0.7344\n",
      "Epoch 539/1500\n",
      "18/18 [==============================] - 0s 959us/step - loss: 0.4435 - accuracy: 0.7917 - val_loss: 0.5354 - val_accuracy: 0.7344\n",
      "Epoch 540/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4435 - accuracy: 0.7934 - val_loss: 0.5354 - val_accuracy: 0.7344\n",
      "Epoch 541/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4434 - accuracy: 0.7917 - val_loss: 0.5354 - val_accuracy: 0.7344\n",
      "Epoch 542/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4434 - accuracy: 0.7917 - val_loss: 0.5353 - val_accuracy: 0.7344\n",
      "Epoch 543/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4434 - accuracy: 0.7917 - val_loss: 0.5353 - val_accuracy: 0.7344\n",
      "Epoch 544/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4433 - accuracy: 0.7917 - val_loss: 0.5353 - val_accuracy: 0.7344\n",
      "Epoch 545/1500\n",
      "18/18 [==============================] - 0s 996us/step - loss: 0.4433 - accuracy: 0.7917 - val_loss: 0.5352 - val_accuracy: 0.7344\n",
      "Epoch 546/1500\n",
      "18/18 [==============================] - 0s 992us/step - loss: 0.4433 - accuracy: 0.7917 - val_loss: 0.5352 - val_accuracy: 0.7344\n",
      "Epoch 547/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4433 - accuracy: 0.7934 - val_loss: 0.5351 - val_accuracy: 0.7344\n",
      "Epoch 548/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4432 - accuracy: 0.7917 - val_loss: 0.5351 - val_accuracy: 0.7344\n",
      "Epoch 549/1500\n",
      "18/18 [==============================] - 0s 992us/step - loss: 0.4432 - accuracy: 0.7917 - val_loss: 0.5351 - val_accuracy: 0.7344\n",
      "Epoch 550/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4432 - accuracy: 0.7917 - val_loss: 0.5350 - val_accuracy: 0.7344\n",
      "Epoch 551/1500\n",
      "18/18 [==============================] - 0s 969us/step - loss: 0.4431 - accuracy: 0.7934 - val_loss: 0.5350 - val_accuracy: 0.7344\n",
      "Epoch 552/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4431 - accuracy: 0.7934 - val_loss: 0.5350 - val_accuracy: 0.7344\n",
      "Epoch 553/1500\n",
      "18/18 [==============================] - 0s 974us/step - loss: 0.4431 - accuracy: 0.7934 - val_loss: 0.5350 - val_accuracy: 0.7344\n",
      "Epoch 554/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4430 - accuracy: 0.7934 - val_loss: 0.5349 - val_accuracy: 0.7344\n",
      "Epoch 555/1500\n",
      "18/18 [==============================] - 0s 994us/step - loss: 0.4430 - accuracy: 0.7934 - val_loss: 0.5349 - val_accuracy: 0.7344\n",
      "Epoch 556/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4430 - accuracy: 0.7934 - val_loss: 0.5349 - val_accuracy: 0.7344\n",
      "Epoch 557/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4430 - accuracy: 0.7934 - val_loss: 0.5348 - val_accuracy: 0.7344\n",
      "Epoch 558/1500\n",
      "18/18 [==============================] - 0s 969us/step - loss: 0.4429 - accuracy: 0.7934 - val_loss: 0.5348 - val_accuracy: 0.7344\n",
      "Epoch 559/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4429 - accuracy: 0.7934 - val_loss: 0.5348 - val_accuracy: 0.7292\n",
      "Epoch 560/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4429 - accuracy: 0.7934 - val_loss: 0.5347 - val_accuracy: 0.7344\n",
      "Epoch 561/1500\n",
      "18/18 [==============================] - 0s 977us/step - loss: 0.4428 - accuracy: 0.7934 - val_loss: 0.5347 - val_accuracy: 0.7344\n",
      "Epoch 562/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4428 - accuracy: 0.7934 - val_loss: 0.5347 - val_accuracy: 0.7344\n",
      "Epoch 563/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4428 - accuracy: 0.7934 - val_loss: 0.5346 - val_accuracy: 0.7344\n",
      "Epoch 564/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4428 - accuracy: 0.7934 - val_loss: 0.5346 - val_accuracy: 0.7344\n",
      "Epoch 565/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4427 - accuracy: 0.7934 - val_loss: 0.5346 - val_accuracy: 0.7344\n",
      "Epoch 566/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4427 - accuracy: 0.7951 - val_loss: 0.5345 - val_accuracy: 0.7344\n",
      "Epoch 567/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4426 - accuracy: 0.7951 - val_loss: 0.5345 - val_accuracy: 0.7344\n",
      "Epoch 568/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4426 - accuracy: 0.7951 - val_loss: 0.5345 - val_accuracy: 0.7344\n",
      "Epoch 569/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4426 - accuracy: 0.7934 - val_loss: 0.5344 - val_accuracy: 0.7344\n",
      "Epoch 570/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4425 - accuracy: 0.7951 - val_loss: 0.5344 - val_accuracy: 0.7344\n",
      "Epoch 571/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4425 - accuracy: 0.7951 - val_loss: 0.5344 - val_accuracy: 0.7344\n",
      "Epoch 572/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4425 - accuracy: 0.7934 - val_loss: 0.5343 - val_accuracy: 0.7344\n",
      "Epoch 573/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4425 - accuracy: 0.7934 - val_loss: 0.5343 - val_accuracy: 0.7344\n",
      "Epoch 574/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4425 - accuracy: 0.7951 - val_loss: 0.5343 - val_accuracy: 0.7344\n",
      "Epoch 575/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4425 - accuracy: 0.7951 - val_loss: 0.5342 - val_accuracy: 0.7344\n",
      "Epoch 576/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4424 - accuracy: 0.7951 - val_loss: 0.5342 - val_accuracy: 0.7344\n",
      "Epoch 577/1500\n",
      "18/18 [==============================] - 0s 983us/step - loss: 0.4424 - accuracy: 0.7951 - val_loss: 0.5342 - val_accuracy: 0.7344\n",
      "Epoch 578/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4424 - accuracy: 0.7951 - val_loss: 0.5341 - val_accuracy: 0.7344\n",
      "Epoch 579/1500\n",
      "18/18 [==============================] - 0s 988us/step - loss: 0.4423 - accuracy: 0.7951 - val_loss: 0.5341 - val_accuracy: 0.7344\n",
      "Epoch 580/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4423 - accuracy: 0.7951 - val_loss: 0.5341 - val_accuracy: 0.7344\n",
      "Epoch 581/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4423 - accuracy: 0.7969 - val_loss: 0.5341 - val_accuracy: 0.7344\n",
      "Epoch 582/1500\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.4422 - accuracy: 0.7969 - val_loss: 0.5340 - val_accuracy: 0.7344\n",
      "Epoch 583/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4422 - accuracy: 0.7969 - val_loss: 0.5340 - val_accuracy: 0.7344\n",
      "Epoch 584/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4422 - accuracy: 0.7969 - val_loss: 0.5340 - val_accuracy: 0.7344\n",
      "Epoch 585/1500\n",
      "18/18 [==============================] - 0s 935us/step - loss: 0.4421 - accuracy: 0.7951 - val_loss: 0.5340 - val_accuracy: 0.7344\n",
      "Epoch 586/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4421 - accuracy: 0.7951 - val_loss: 0.5339 - val_accuracy: 0.7344\n",
      "Epoch 587/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4421 - accuracy: 0.7951 - val_loss: 0.5339 - val_accuracy: 0.7344\n",
      "Epoch 588/1500\n",
      "18/18 [==============================] - 0s 1000us/step - loss: 0.4421 - accuracy: 0.7951 - val_loss: 0.5339 - val_accuracy: 0.7344\n",
      "Epoch 589/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4421 - accuracy: 0.7969 - val_loss: 0.5339 - val_accuracy: 0.7344\n",
      "Epoch 590/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4421 - accuracy: 0.7951 - val_loss: 0.5339 - val_accuracy: 0.7344\n",
      "Epoch 591/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4420 - accuracy: 0.7969 - val_loss: 0.5338 - val_accuracy: 0.7344\n",
      "Epoch 592/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4420 - accuracy: 0.7969 - val_loss: 0.5338 - val_accuracy: 0.7344\n",
      "Epoch 593/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4420 - accuracy: 0.7969 - val_loss: 0.5338 - val_accuracy: 0.7344\n",
      "Epoch 594/1500\n",
      "18/18 [==============================] - 0s 943us/step - loss: 0.4419 - accuracy: 0.7951 - val_loss: 0.5338 - val_accuracy: 0.7344\n",
      "Epoch 595/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4419 - accuracy: 0.7969 - val_loss: 0.5337 - val_accuracy: 0.7344\n",
      "Epoch 596/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4419 - accuracy: 0.7934 - val_loss: 0.5337 - val_accuracy: 0.7344\n",
      "Epoch 597/1500\n",
      "18/18 [==============================] - 0s 998us/step - loss: 0.4419 - accuracy: 0.7934 - val_loss: 0.5337 - val_accuracy: 0.7344\n",
      "Epoch 598/1500\n",
      "18/18 [==============================] - 0s 987us/step - loss: 0.4418 - accuracy: 0.7951 - val_loss: 0.5337 - val_accuracy: 0.7396\n",
      "Epoch 599/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4418 - accuracy: 0.7951 - val_loss: 0.5337 - val_accuracy: 0.7396\n",
      "Epoch 600/1500\n",
      "18/18 [==============================] - 0s 986us/step - loss: 0.4418 - accuracy: 0.7951 - val_loss: 0.5336 - val_accuracy: 0.7396\n",
      "Epoch 601/1500\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.4417 - accuracy: 0.7969 - val_loss: 0.5336 - val_accuracy: 0.7396\n",
      "Epoch 602/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4418 - accuracy: 0.7969 - val_loss: 0.5336 - val_accuracy: 0.7396\n",
      "Epoch 603/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4417 - accuracy: 0.7969 - val_loss: 0.5336 - val_accuracy: 0.7396\n",
      "Epoch 604/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4417 - accuracy: 0.7969 - val_loss: 0.5335 - val_accuracy: 0.7396\n",
      "Epoch 605/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4417 - accuracy: 0.7986 - val_loss: 0.5335 - val_accuracy: 0.7396\n",
      "Epoch 606/1500\n",
      "18/18 [==============================] - 0s 973us/step - loss: 0.4417 - accuracy: 0.7986 - val_loss: 0.5335 - val_accuracy: 0.7396\n",
      "Epoch 607/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4416 - accuracy: 0.7969 - val_loss: 0.5335 - val_accuracy: 0.7396\n",
      "Epoch 608/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4416 - accuracy: 0.7986 - val_loss: 0.5334 - val_accuracy: 0.7396\n",
      "Epoch 609/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4416 - accuracy: 0.7969 - val_loss: 0.5334 - val_accuracy: 0.7396\n",
      "Epoch 610/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4415 - accuracy: 0.7986 - val_loss: 0.5334 - val_accuracy: 0.7396\n",
      "Epoch 611/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4415 - accuracy: 0.7969 - val_loss: 0.5334 - val_accuracy: 0.7396\n",
      "Epoch 612/1500\n",
      "18/18 [==============================] - 0s 930us/step - loss: 0.4415 - accuracy: 0.7969 - val_loss: 0.5333 - val_accuracy: 0.7396\n",
      "Epoch 613/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4415 - accuracy: 0.7986 - val_loss: 0.5333 - val_accuracy: 0.7396\n",
      "Epoch 614/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4415 - accuracy: 0.7969 - val_loss: 0.5333 - val_accuracy: 0.7396\n",
      "Epoch 615/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4414 - accuracy: 0.7986 - val_loss: 0.5333 - val_accuracy: 0.7396\n",
      "Epoch 616/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4414 - accuracy: 0.7986 - val_loss: 0.5332 - val_accuracy: 0.7396\n",
      "Epoch 617/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4414 - accuracy: 0.7969 - val_loss: 0.5332 - val_accuracy: 0.7396\n",
      "Epoch 618/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4414 - accuracy: 0.7969 - val_loss: 0.5332 - val_accuracy: 0.7396\n",
      "Epoch 619/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4413 - accuracy: 0.7986 - val_loss: 0.5332 - val_accuracy: 0.7396\n",
      "Epoch 620/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4413 - accuracy: 0.7969 - val_loss: 0.5331 - val_accuracy: 0.7396\n",
      "Epoch 621/1500\n",
      "18/18 [==============================] - 0s 979us/step - loss: 0.4413 - accuracy: 0.7986 - val_loss: 0.5331 - val_accuracy: 0.7396\n",
      "Epoch 622/1500\n",
      "18/18 [==============================] - 0s 979us/step - loss: 0.4413 - accuracy: 0.7969 - val_loss: 0.5331 - val_accuracy: 0.7396\n",
      "Epoch 623/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4413 - accuracy: 0.8003 - val_loss: 0.5331 - val_accuracy: 0.7396\n",
      "Epoch 624/1500\n",
      "18/18 [==============================] - 0s 977us/step - loss: 0.4412 - accuracy: 0.7986 - val_loss: 0.5330 - val_accuracy: 0.7396\n",
      "Epoch 625/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4412 - accuracy: 0.7986 - val_loss: 0.5330 - val_accuracy: 0.7396\n",
      "Epoch 626/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4412 - accuracy: 0.7986 - val_loss: 0.5330 - val_accuracy: 0.7396\n",
      "Epoch 627/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4411 - accuracy: 0.7969 - val_loss: 0.5329 - val_accuracy: 0.7396\n",
      "Epoch 628/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4411 - accuracy: 0.7986 - val_loss: 0.5329 - val_accuracy: 0.7396\n",
      "Epoch 629/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4411 - accuracy: 0.7969 - val_loss: 0.5329 - val_accuracy: 0.7396\n",
      "Epoch 630/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4411 - accuracy: 0.7986 - val_loss: 0.5329 - val_accuracy: 0.7396\n",
      "Epoch 631/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4410 - accuracy: 0.7986 - val_loss: 0.5328 - val_accuracy: 0.7396\n",
      "Epoch 632/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4410 - accuracy: 0.7986 - val_loss: 0.5328 - val_accuracy: 0.7396\n",
      "Epoch 633/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4410 - accuracy: 0.7986 - val_loss: 0.5328 - val_accuracy: 0.7396\n",
      "Epoch 634/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4410 - accuracy: 0.7986 - val_loss: 0.5328 - val_accuracy: 0.7396\n",
      "Epoch 635/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4410 - accuracy: 0.8003 - val_loss: 0.5327 - val_accuracy: 0.7396\n",
      "Epoch 636/1500\n",
      "18/18 [==============================] - 0s 980us/step - loss: 0.4410 - accuracy: 0.7986 - val_loss: 0.5327 - val_accuracy: 0.7396\n",
      "Epoch 637/1500\n",
      "18/18 [==============================] - 0s 951us/step - loss: 0.4409 - accuracy: 0.7986 - val_loss: 0.5327 - val_accuracy: 0.7396\n",
      "Epoch 638/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4409 - accuracy: 0.8003 - val_loss: 0.5326 - val_accuracy: 0.7396\n",
      "Epoch 639/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4409 - accuracy: 0.7986 - val_loss: 0.5326 - val_accuracy: 0.7396\n",
      "Epoch 640/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4409 - accuracy: 0.7986 - val_loss: 0.5326 - val_accuracy: 0.7396\n",
      "Epoch 641/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4409 - accuracy: 0.7986 - val_loss: 0.5325 - val_accuracy: 0.7396\n",
      "Epoch 642/1500\n",
      "18/18 [==============================] - 0s 989us/step - loss: 0.4409 - accuracy: 0.7969 - val_loss: 0.5325 - val_accuracy: 0.7396\n",
      "Epoch 643/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4408 - accuracy: 0.7969 - val_loss: 0.5325 - val_accuracy: 0.7396\n",
      "Epoch 644/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4408 - accuracy: 0.7986 - val_loss: 0.5325 - val_accuracy: 0.7396\n",
      "Epoch 645/1500\n",
      "18/18 [==============================] - 0s 973us/step - loss: 0.4408 - accuracy: 0.7969 - val_loss: 0.5324 - val_accuracy: 0.7396\n",
      "Epoch 646/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4408 - accuracy: 0.7969 - val_loss: 0.5324 - val_accuracy: 0.7396\n",
      "Epoch 647/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4407 - accuracy: 0.7986 - val_loss: 0.5324 - val_accuracy: 0.7396\n",
      "Epoch 648/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4407 - accuracy: 0.8003 - val_loss: 0.5324 - val_accuracy: 0.7396\n",
      "Epoch 649/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4407 - accuracy: 0.8003 - val_loss: 0.5323 - val_accuracy: 0.7396\n",
      "Epoch 650/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4407 - accuracy: 0.7986 - val_loss: 0.5323 - val_accuracy: 0.7396\n",
      "Epoch 651/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4407 - accuracy: 0.8003 - val_loss: 0.5323 - val_accuracy: 0.7396\n",
      "Epoch 652/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4406 - accuracy: 0.8003 - val_loss: 0.5323 - val_accuracy: 0.7396\n",
      "Epoch 653/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4406 - accuracy: 0.8003 - val_loss: 0.5322 - val_accuracy: 0.7396\n",
      "Epoch 654/1500\n",
      "18/18 [==============================] - 0s 988us/step - loss: 0.4406 - accuracy: 0.8003 - val_loss: 0.5322 - val_accuracy: 0.7396\n",
      "Epoch 655/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4405 - accuracy: 0.8003 - val_loss: 0.5322 - val_accuracy: 0.7396\n",
      "Epoch 656/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4405 - accuracy: 0.8003 - val_loss: 0.5322 - val_accuracy: 0.7396\n",
      "Epoch 657/1500\n",
      "18/18 [==============================] - 0s 968us/step - loss: 0.4405 - accuracy: 0.8003 - val_loss: 0.5321 - val_accuracy: 0.7396\n",
      "Epoch 658/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4405 - accuracy: 0.7986 - val_loss: 0.5321 - val_accuracy: 0.7396\n",
      "Epoch 659/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4405 - accuracy: 0.8003 - val_loss: 0.5321 - val_accuracy: 0.7396\n",
      "Epoch 660/1500\n",
      "18/18 [==============================] - 0s 961us/step - loss: 0.4404 - accuracy: 0.8003 - val_loss: 0.5321 - val_accuracy: 0.7396\n",
      "Epoch 661/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4404 - accuracy: 0.8003 - val_loss: 0.5320 - val_accuracy: 0.7448\n",
      "Epoch 662/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4404 - accuracy: 0.8003 - val_loss: 0.5320 - val_accuracy: 0.7448\n",
      "Epoch 663/1500\n",
      "18/18 [==============================] - 0s 999us/step - loss: 0.4404 - accuracy: 0.8003 - val_loss: 0.5320 - val_accuracy: 0.7448\n",
      "Epoch 664/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4403 - accuracy: 0.8003 - val_loss: 0.5320 - val_accuracy: 0.7448\n",
      "Epoch 665/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4403 - accuracy: 0.8003 - val_loss: 0.5319 - val_accuracy: 0.7448\n",
      "Epoch 666/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4403 - accuracy: 0.7986 - val_loss: 0.5319 - val_accuracy: 0.7448\n",
      "Epoch 667/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4403 - accuracy: 0.7986 - val_loss: 0.5319 - val_accuracy: 0.7448\n",
      "Epoch 668/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4403 - accuracy: 0.7986 - val_loss: 0.5319 - val_accuracy: 0.7500\n",
      "Epoch 669/1500\n",
      "18/18 [==============================] - 0s 984us/step - loss: 0.4403 - accuracy: 0.8003 - val_loss: 0.5318 - val_accuracy: 0.7500\n",
      "Epoch 670/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4402 - accuracy: 0.7986 - val_loss: 0.5318 - val_accuracy: 0.7500\n",
      "Epoch 671/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4402 - accuracy: 0.8003 - val_loss: 0.5318 - val_accuracy: 0.7500\n",
      "Epoch 672/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4402 - accuracy: 0.7986 - val_loss: 0.5318 - val_accuracy: 0.7500\n",
      "Epoch 673/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4402 - accuracy: 0.7986 - val_loss: 0.5317 - val_accuracy: 0.7500\n",
      "Epoch 674/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4401 - accuracy: 0.7986 - val_loss: 0.5317 - val_accuracy: 0.7500\n",
      "Epoch 675/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4401 - accuracy: 0.8003 - val_loss: 0.5317 - val_accuracy: 0.7500\n",
      "Epoch 676/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4401 - accuracy: 0.7986 - val_loss: 0.5317 - val_accuracy: 0.7500\n",
      "Epoch 677/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4401 - accuracy: 0.8003 - val_loss: 0.5317 - val_accuracy: 0.7500\n",
      "Epoch 678/1500\n",
      "18/18 [==============================] - 0s 957us/step - loss: 0.4401 - accuracy: 0.7969 - val_loss: 0.5316 - val_accuracy: 0.7500\n",
      "Epoch 679/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4400 - accuracy: 0.7986 - val_loss: 0.5316 - val_accuracy: 0.7448\n",
      "Epoch 680/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4400 - accuracy: 0.8003 - val_loss: 0.5316 - val_accuracy: 0.7448\n",
      "Epoch 681/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4400 - accuracy: 0.7986 - val_loss: 0.5316 - val_accuracy: 0.7448\n",
      "Epoch 682/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4400 - accuracy: 0.8021 - val_loss: 0.5316 - val_accuracy: 0.7448\n",
      "Epoch 683/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4399 - accuracy: 0.7986 - val_loss: 0.5316 - val_accuracy: 0.7448\n",
      "Epoch 684/1500\n",
      "18/18 [==============================] - 0s 955us/step - loss: 0.4399 - accuracy: 0.7969 - val_loss: 0.5315 - val_accuracy: 0.7448\n",
      "Epoch 685/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4399 - accuracy: 0.7969 - val_loss: 0.5315 - val_accuracy: 0.7448\n",
      "Epoch 686/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4399 - accuracy: 0.7986 - val_loss: 0.5315 - val_accuracy: 0.7448\n",
      "Epoch 687/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4399 - accuracy: 0.7969 - val_loss: 0.5315 - val_accuracy: 0.7448\n",
      "Epoch 688/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4398 - accuracy: 0.7986 - val_loss: 0.5315 - val_accuracy: 0.7448\n",
      "Epoch 689/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4398 - accuracy: 0.7986 - val_loss: 0.5315 - val_accuracy: 0.7448\n",
      "Epoch 690/1500\n",
      "18/18 [==============================] - 0s 982us/step - loss: 0.4398 - accuracy: 0.8003 - val_loss: 0.5314 - val_accuracy: 0.7448\n",
      "Epoch 691/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4398 - accuracy: 0.8003 - val_loss: 0.5314 - val_accuracy: 0.7448\n",
      "Epoch 692/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4397 - accuracy: 0.8021 - val_loss: 0.5314 - val_accuracy: 0.7448\n",
      "Epoch 693/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4397 - accuracy: 0.8003 - val_loss: 0.5314 - val_accuracy: 0.7448\n",
      "Epoch 694/1500\n",
      "18/18 [==============================] - 0s 991us/step - loss: 0.4397 - accuracy: 0.8003 - val_loss: 0.5314 - val_accuracy: 0.7448\n",
      "Epoch 695/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4397 - accuracy: 0.8021 - val_loss: 0.5313 - val_accuracy: 0.7448\n",
      "Epoch 696/1500\n",
      "18/18 [==============================] - 0s 960us/step - loss: 0.4397 - accuracy: 0.8038 - val_loss: 0.5313 - val_accuracy: 0.7448\n",
      "Epoch 697/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4396 - accuracy: 0.8021 - val_loss: 0.5313 - val_accuracy: 0.7448\n",
      "Epoch 698/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4396 - accuracy: 0.8021 - val_loss: 0.5313 - val_accuracy: 0.7448\n",
      "Epoch 699/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4396 - accuracy: 0.8021 - val_loss: 0.5313 - val_accuracy: 0.7448\n",
      "Epoch 700/1500\n",
      "18/18 [==============================] - 0s 991us/step - loss: 0.4396 - accuracy: 0.8003 - val_loss: 0.5313 - val_accuracy: 0.7500\n",
      "Epoch 701/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4396 - accuracy: 0.8021 - val_loss: 0.5312 - val_accuracy: 0.7500\n",
      "Epoch 702/1500\n",
      "18/18 [==============================] - 0s 957us/step - loss: 0.4395 - accuracy: 0.8038 - val_loss: 0.5312 - val_accuracy: 0.7500\n",
      "Epoch 703/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4395 - accuracy: 0.8021 - val_loss: 0.5312 - val_accuracy: 0.7500\n",
      "Epoch 704/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4395 - accuracy: 0.8003 - val_loss: 0.5312 - val_accuracy: 0.7500\n",
      "Epoch 705/1500\n",
      "18/18 [==============================] - 0s 995us/step - loss: 0.4395 - accuracy: 0.8021 - val_loss: 0.5312 - val_accuracy: 0.7500\n",
      "Epoch 706/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4394 - accuracy: 0.8021 - val_loss: 0.5311 - val_accuracy: 0.7500\n",
      "Epoch 707/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4394 - accuracy: 0.8038 - val_loss: 0.5311 - val_accuracy: 0.7500\n",
      "Epoch 708/1500\n",
      "18/18 [==============================] - 0s 985us/step - loss: 0.4394 - accuracy: 0.8021 - val_loss: 0.5311 - val_accuracy: 0.7500\n",
      "Epoch 709/1500\n",
      "18/18 [==============================] - 0s 980us/step - loss: 0.4394 - accuracy: 0.8038 - val_loss: 0.5311 - val_accuracy: 0.7500\n",
      "Epoch 710/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4394 - accuracy: 0.8021 - val_loss: 0.5311 - val_accuracy: 0.7500\n",
      "Epoch 711/1500\n",
      "18/18 [==============================] - 0s 983us/step - loss: 0.4393 - accuracy: 0.8038 - val_loss: 0.5311 - val_accuracy: 0.7500\n",
      "Epoch 712/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4393 - accuracy: 0.8038 - val_loss: 0.5311 - val_accuracy: 0.7500\n",
      "Epoch 713/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4393 - accuracy: 0.8038 - val_loss: 0.5311 - val_accuracy: 0.7500\n",
      "Epoch 714/1500\n",
      "18/18 [==============================] - 0s 980us/step - loss: 0.4393 - accuracy: 0.8038 - val_loss: 0.5310 - val_accuracy: 0.7500\n",
      "Epoch 715/1500\n",
      "18/18 [==============================] - 0s 969us/step - loss: 0.4393 - accuracy: 0.8021 - val_loss: 0.5310 - val_accuracy: 0.7500\n",
      "Epoch 716/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4393 - accuracy: 0.8038 - val_loss: 0.5310 - val_accuracy: 0.7500\n",
      "Epoch 717/1500\n",
      "18/18 [==============================] - 0s 946us/step - loss: 0.4392 - accuracy: 0.8038 - val_loss: 0.5310 - val_accuracy: 0.7500\n",
      "Epoch 718/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4392 - accuracy: 0.8038 - val_loss: 0.5310 - val_accuracy: 0.7500\n",
      "Epoch 719/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4392 - accuracy: 0.8021 - val_loss: 0.5310 - val_accuracy: 0.7500\n",
      "Epoch 720/1500\n",
      "18/18 [==============================] - 0s 965us/step - loss: 0.4392 - accuracy: 0.8038 - val_loss: 0.5310 - val_accuracy: 0.7500\n",
      "Epoch 721/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4391 - accuracy: 0.8038 - val_loss: 0.5310 - val_accuracy: 0.7500\n",
      "Epoch 722/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4391 - accuracy: 0.8038 - val_loss: 0.5310 - val_accuracy: 0.7500\n",
      "Epoch 723/1500\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.4391 - accuracy: 0.8038 - val_loss: 0.5310 - val_accuracy: 0.7500\n",
      "Epoch 724/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4390 - accuracy: 0.8038 - val_loss: 0.5309 - val_accuracy: 0.7500\n",
      "Epoch 725/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4391 - accuracy: 0.8038 - val_loss: 0.5309 - val_accuracy: 0.7500\n",
      "Epoch 726/1500\n",
      "18/18 [==============================] - 0s 964us/step - loss: 0.4390 - accuracy: 0.8038 - val_loss: 0.5309 - val_accuracy: 0.7500\n",
      "Epoch 727/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4390 - accuracy: 0.8038 - val_loss: 0.5309 - val_accuracy: 0.7500\n",
      "Epoch 728/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4390 - accuracy: 0.8021 - val_loss: 0.5309 - val_accuracy: 0.7500\n",
      "Epoch 729/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4389 - accuracy: 0.8038 - val_loss: 0.5309 - val_accuracy: 0.7500\n",
      "Epoch 730/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4389 - accuracy: 0.8038 - val_loss: 0.5308 - val_accuracy: 0.7500\n",
      "Epoch 731/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4389 - accuracy: 0.8038 - val_loss: 0.5308 - val_accuracy: 0.7448\n",
      "Epoch 732/1500\n",
      "18/18 [==============================] - 0s 967us/step - loss: 0.4389 - accuracy: 0.8038 - val_loss: 0.5308 - val_accuracy: 0.7448\n",
      "Epoch 733/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4388 - accuracy: 0.8038 - val_loss: 0.5308 - val_accuracy: 0.7448\n",
      "Epoch 734/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4388 - accuracy: 0.8021 - val_loss: 0.5308 - val_accuracy: 0.7448\n",
      "Epoch 735/1500\n",
      "18/18 [==============================] - 0s 952us/step - loss: 0.4388 - accuracy: 0.8038 - val_loss: 0.5307 - val_accuracy: 0.7448\n",
      "Epoch 736/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4388 - accuracy: 0.8038 - val_loss: 0.5307 - val_accuracy: 0.7448\n",
      "Epoch 737/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4387 - accuracy: 0.8038 - val_loss: 0.5307 - val_accuracy: 0.7448\n",
      "Epoch 738/1500\n",
      "18/18 [==============================] - 0s 994us/step - loss: 0.4387 - accuracy: 0.8038 - val_loss: 0.5307 - val_accuracy: 0.7448\n",
      "Epoch 739/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4387 - accuracy: 0.8021 - val_loss: 0.5306 - val_accuracy: 0.7448\n",
      "Epoch 740/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4386 - accuracy: 0.8038 - val_loss: 0.5306 - val_accuracy: 0.7448\n",
      "Epoch 741/1500\n",
      "18/18 [==============================] - 0s 990us/step - loss: 0.4386 - accuracy: 0.8038 - val_loss: 0.5306 - val_accuracy: 0.7448\n",
      "Epoch 742/1500\n",
      "18/18 [==============================] - 0s 999us/step - loss: 0.4386 - accuracy: 0.8038 - val_loss: 0.5306 - val_accuracy: 0.7448\n",
      "Epoch 743/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4386 - accuracy: 0.8038 - val_loss: 0.5306 - val_accuracy: 0.7448\n",
      "Epoch 744/1500\n",
      "18/18 [==============================] - 0s 983us/step - loss: 0.4386 - accuracy: 0.8021 - val_loss: 0.5305 - val_accuracy: 0.7448\n",
      "Epoch 745/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4385 - accuracy: 0.8038 - val_loss: 0.5305 - val_accuracy: 0.7448\n",
      "Epoch 746/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4385 - accuracy: 0.8038 - val_loss: 0.5305 - val_accuracy: 0.7448\n",
      "Epoch 747/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4385 - accuracy: 0.8038 - val_loss: 0.5305 - val_accuracy: 0.7448\n",
      "Epoch 748/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4384 - accuracy: 0.8021 - val_loss: 0.5304 - val_accuracy: 0.7448\n",
      "Epoch 749/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4384 - accuracy: 0.8038 - val_loss: 0.5304 - val_accuracy: 0.7448\n",
      "Epoch 750/1500\n",
      "18/18 [==============================] - 0s 956us/step - loss: 0.4384 - accuracy: 0.8021 - val_loss: 0.5304 - val_accuracy: 0.7448\n",
      "Epoch 751/1500\n",
      "18/18 [==============================] - 0s 982us/step - loss: 0.4384 - accuracy: 0.8038 - val_loss: 0.5303 - val_accuracy: 0.7448\n",
      "Epoch 752/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4383 - accuracy: 0.8038 - val_loss: 0.5303 - val_accuracy: 0.7448\n",
      "Epoch 753/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4383 - accuracy: 0.8038 - val_loss: 0.5303 - val_accuracy: 0.7448\n",
      "Epoch 754/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4383 - accuracy: 0.8038 - val_loss: 0.5303 - val_accuracy: 0.7448\n",
      "Epoch 755/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4383 - accuracy: 0.8021 - val_loss: 0.5303 - val_accuracy: 0.7448\n",
      "Epoch 756/1500\n",
      "18/18 [==============================] - 0s 957us/step - loss: 0.4382 - accuracy: 0.8038 - val_loss: 0.5302 - val_accuracy: 0.7448\n",
      "Epoch 757/1500\n",
      "18/18 [==============================] - 0s 976us/step - loss: 0.4382 - accuracy: 0.8021 - val_loss: 0.5302 - val_accuracy: 0.7448\n",
      "Epoch 758/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4382 - accuracy: 0.8038 - val_loss: 0.5302 - val_accuracy: 0.7448\n",
      "Epoch 759/1500\n",
      "18/18 [==============================] - 0s 954us/step - loss: 0.4382 - accuracy: 0.8038 - val_loss: 0.5302 - val_accuracy: 0.7448\n",
      "Epoch 760/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4382 - accuracy: 0.8038 - val_loss: 0.5301 - val_accuracy: 0.7396\n",
      "Epoch 761/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4381 - accuracy: 0.8038 - val_loss: 0.5301 - val_accuracy: 0.7396\n",
      "Epoch 762/1500\n",
      "18/18 [==============================] - 0s 937us/step - loss: 0.4381 - accuracy: 0.8038 - val_loss: 0.5301 - val_accuracy: 0.7396\n",
      "Epoch 763/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4380 - accuracy: 0.8038 - val_loss: 0.5301 - val_accuracy: 0.7396\n",
      "Epoch 764/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4380 - accuracy: 0.8038 - val_loss: 0.5301 - val_accuracy: 0.7396\n",
      "Epoch 765/1500\n",
      "18/18 [==============================] - 0s 968us/step - loss: 0.4380 - accuracy: 0.8021 - val_loss: 0.5301 - val_accuracy: 0.7396\n",
      "Epoch 766/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4380 - accuracy: 0.8021 - val_loss: 0.5300 - val_accuracy: 0.7396\n",
      "Epoch 767/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4379 - accuracy: 0.8038 - val_loss: 0.5300 - val_accuracy: 0.7396\n",
      "Epoch 768/1500\n",
      "18/18 [==============================] - 0s 958us/step - loss: 0.4379 - accuracy: 0.8038 - val_loss: 0.5300 - val_accuracy: 0.7396\n",
      "Epoch 769/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4379 - accuracy: 0.8021 - val_loss: 0.5300 - val_accuracy: 0.7396\n",
      "Epoch 770/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4379 - accuracy: 0.8021 - val_loss: 0.5299 - val_accuracy: 0.7396\n",
      "Epoch 771/1500\n",
      "18/18 [==============================] - 0s 956us/step - loss: 0.4378 - accuracy: 0.8038 - val_loss: 0.5299 - val_accuracy: 0.7396\n",
      "Epoch 772/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4378 - accuracy: 0.8038 - val_loss: 0.5299 - val_accuracy: 0.7396\n",
      "Epoch 773/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4378 - accuracy: 0.8021 - val_loss: 0.5299 - val_accuracy: 0.7396\n",
      "Epoch 774/1500\n",
      "18/18 [==============================] - 0s 986us/step - loss: 0.4378 - accuracy: 0.8021 - val_loss: 0.5299 - val_accuracy: 0.7396\n",
      "Epoch 775/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4377 - accuracy: 0.8021 - val_loss: 0.5299 - val_accuracy: 0.7396\n",
      "Epoch 776/1500\n",
      "18/18 [==============================] - 0s 930us/step - loss: 0.4377 - accuracy: 0.8038 - val_loss: 0.5299 - val_accuracy: 0.7396\n",
      "Epoch 777/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4377 - accuracy: 0.8021 - val_loss: 0.5298 - val_accuracy: 0.7396\n",
      "Epoch 778/1500\n",
      "18/18 [==============================] - 0s 935us/step - loss: 0.4377 - accuracy: 0.8038 - val_loss: 0.5298 - val_accuracy: 0.7396\n",
      "Epoch 779/1500\n",
      "18/18 [==============================] - 0s 999us/step - loss: 0.4376 - accuracy: 0.8038 - val_loss: 0.5298 - val_accuracy: 0.7396\n",
      "Epoch 780/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4376 - accuracy: 0.8021 - val_loss: 0.5298 - val_accuracy: 0.7396\n",
      "Epoch 781/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4376 - accuracy: 0.8021 - val_loss: 0.5298 - val_accuracy: 0.7396\n",
      "Epoch 782/1500\n",
      "18/18 [==============================] - 0s 954us/step - loss: 0.4376 - accuracy: 0.8038 - val_loss: 0.5298 - val_accuracy: 0.7448\n",
      "Epoch 783/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4375 - accuracy: 0.8021 - val_loss: 0.5298 - val_accuracy: 0.7448\n",
      "Epoch 784/1500\n",
      "18/18 [==============================] - 0s 962us/step - loss: 0.4375 - accuracy: 0.8038 - val_loss: 0.5298 - val_accuracy: 0.7448\n",
      "Epoch 785/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4375 - accuracy: 0.8021 - val_loss: 0.5298 - val_accuracy: 0.7448\n",
      "Epoch 786/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4374 - accuracy: 0.8021 - val_loss: 0.5298 - val_accuracy: 0.7448\n",
      "Epoch 787/1500\n",
      "18/18 [==============================] - 0s 921us/step - loss: 0.4374 - accuracy: 0.8021 - val_loss: 0.5298 - val_accuracy: 0.7448\n",
      "Epoch 788/1500\n",
      "18/18 [==============================] - 0s 992us/step - loss: 0.4374 - accuracy: 0.8038 - val_loss: 0.5298 - val_accuracy: 0.7448\n",
      "Epoch 789/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4374 - accuracy: 0.8021 - val_loss: 0.5297 - val_accuracy: 0.7448\n",
      "Epoch 790/1500\n",
      "18/18 [==============================] - 0s 955us/step - loss: 0.4374 - accuracy: 0.8021 - val_loss: 0.5297 - val_accuracy: 0.7448\n",
      "Epoch 791/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4373 - accuracy: 0.8021 - val_loss: 0.5297 - val_accuracy: 0.7448\n",
      "Epoch 792/1500\n",
      "18/18 [==============================] - 0s 998us/step - loss: 0.4373 - accuracy: 0.8021 - val_loss: 0.5297 - val_accuracy: 0.7448\n",
      "Epoch 793/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4373 - accuracy: 0.8021 - val_loss: 0.5297 - val_accuracy: 0.7448\n",
      "Epoch 794/1500\n",
      "18/18 [==============================] - 0s 950us/step - loss: 0.4373 - accuracy: 0.8021 - val_loss: 0.5297 - val_accuracy: 0.7448\n",
      "Epoch 795/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4373 - accuracy: 0.8021 - val_loss: 0.5297 - val_accuracy: 0.7448\n",
      "Epoch 796/1500\n",
      "18/18 [==============================] - 0s 958us/step - loss: 0.4372 - accuracy: 0.8021 - val_loss: 0.5297 - val_accuracy: 0.7448\n",
      "Epoch 797/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4372 - accuracy: 0.8021 - val_loss: 0.5297 - val_accuracy: 0.7448\n",
      "Epoch 798/1500\n",
      "18/18 [==============================] - 0s 940us/step - loss: 0.4372 - accuracy: 0.8021 - val_loss: 0.5296 - val_accuracy: 0.7448\n",
      "Epoch 799/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4371 - accuracy: 0.8021 - val_loss: 0.5296 - val_accuracy: 0.7448\n",
      "Epoch 800/1500\n",
      "18/18 [==============================] - 0s 969us/step - loss: 0.4371 - accuracy: 0.8038 - val_loss: 0.5296 - val_accuracy: 0.7448\n",
      "Epoch 801/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4372 - accuracy: 0.8038 - val_loss: 0.5296 - val_accuracy: 0.7448\n",
      "Epoch 802/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4371 - accuracy: 0.8021 - val_loss: 0.5296 - val_accuracy: 0.7448\n",
      "Epoch 803/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4371 - accuracy: 0.8021 - val_loss: 0.5296 - val_accuracy: 0.7448\n",
      "Epoch 804/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4370 - accuracy: 0.8021 - val_loss: 0.5295 - val_accuracy: 0.7448\n",
      "Epoch 805/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4370 - accuracy: 0.8021 - val_loss: 0.5295 - val_accuracy: 0.7448\n",
      "Epoch 806/1500\n",
      "18/18 [==============================] - 0s 964us/step - loss: 0.4370 - accuracy: 0.8021 - val_loss: 0.5295 - val_accuracy: 0.7448\n",
      "Epoch 807/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4370 - accuracy: 0.8021 - val_loss: 0.5295 - val_accuracy: 0.7448\n",
      "Epoch 808/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4370 - accuracy: 0.8021 - val_loss: 0.5295 - val_accuracy: 0.7448\n",
      "Epoch 809/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4369 - accuracy: 0.8021 - val_loss: 0.5295 - val_accuracy: 0.7448\n",
      "Epoch 810/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4369 - accuracy: 0.8021 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 811/1500\n",
      "18/18 [==============================] - 0s 988us/step - loss: 0.4369 - accuracy: 0.8021 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 812/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4368 - accuracy: 0.8021 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 813/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4368 - accuracy: 0.8021 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 814/1500\n",
      "18/18 [==============================] - 0s 985us/step - loss: 0.4368 - accuracy: 0.8021 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 815/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4367 - accuracy: 0.8021 - val_loss: 0.5293 - val_accuracy: 0.7448\n",
      "Epoch 816/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4367 - accuracy: 0.8038 - val_loss: 0.5293 - val_accuracy: 0.7448\n",
      "Epoch 817/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4367 - accuracy: 0.8021 - val_loss: 0.5293 - val_accuracy: 0.7396\n",
      "Epoch 818/1500\n",
      "18/18 [==============================] - 0s 991us/step - loss: 0.4367 - accuracy: 0.8021 - val_loss: 0.5293 - val_accuracy: 0.7396\n",
      "Epoch 819/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4367 - accuracy: 0.8003 - val_loss: 0.5293 - val_accuracy: 0.7396\n",
      "Epoch 820/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4366 - accuracy: 0.8021 - val_loss: 0.5292 - val_accuracy: 0.7396\n",
      "Epoch 821/1500\n",
      "18/18 [==============================] - 0s 983us/step - loss: 0.4366 - accuracy: 0.8021 - val_loss: 0.5292 - val_accuracy: 0.7396\n",
      "Epoch 822/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4366 - accuracy: 0.8021 - val_loss: 0.5292 - val_accuracy: 0.7396\n",
      "Epoch 823/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4365 - accuracy: 0.8021 - val_loss: 0.5292 - val_accuracy: 0.7396\n",
      "Epoch 824/1500\n",
      "18/18 [==============================] - 0s 988us/step - loss: 0.4366 - accuracy: 0.8021 - val_loss: 0.5292 - val_accuracy: 0.7396\n",
      "Epoch 825/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4365 - accuracy: 0.8021 - val_loss: 0.5292 - val_accuracy: 0.7396\n",
      "Epoch 826/1500\n",
      "18/18 [==============================] - 0s 964us/step - loss: 0.4365 - accuracy: 0.8021 - val_loss: 0.5291 - val_accuracy: 0.7396\n",
      "Epoch 827/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4365 - accuracy: 0.8021 - val_loss: 0.5291 - val_accuracy: 0.7396\n",
      "Epoch 828/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4364 - accuracy: 0.8021 - val_loss: 0.5291 - val_accuracy: 0.7396\n",
      "Epoch 829/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4364 - accuracy: 0.8021 - val_loss: 0.5291 - val_accuracy: 0.7396\n",
      "Epoch 830/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4363 - accuracy: 0.8021 - val_loss: 0.5291 - val_accuracy: 0.7396\n",
      "Epoch 831/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4364 - accuracy: 0.8021 - val_loss: 0.5290 - val_accuracy: 0.7396\n",
      "Epoch 832/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4364 - accuracy: 0.8021 - val_loss: 0.5290 - val_accuracy: 0.7396\n",
      "Epoch 833/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4363 - accuracy: 0.8021 - val_loss: 0.5290 - val_accuracy: 0.7396\n",
      "Epoch 834/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4363 - accuracy: 0.8038 - val_loss: 0.5290 - val_accuracy: 0.7396\n",
      "Epoch 835/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4363 - accuracy: 0.8038 - val_loss: 0.5290 - val_accuracy: 0.7396\n",
      "Epoch 836/1500\n",
      "18/18 [==============================] - 0s 993us/step - loss: 0.4362 - accuracy: 0.8056 - val_loss: 0.5290 - val_accuracy: 0.7396\n",
      "Epoch 837/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4362 - accuracy: 0.8056 - val_loss: 0.5289 - val_accuracy: 0.7396\n",
      "Epoch 838/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4362 - accuracy: 0.8056 - val_loss: 0.5289 - val_accuracy: 0.7396\n",
      "Epoch 839/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4362 - accuracy: 0.8056 - val_loss: 0.5289 - val_accuracy: 0.7344\n",
      "Epoch 840/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4361 - accuracy: 0.8056 - val_loss: 0.5289 - val_accuracy: 0.7344\n",
      "Epoch 841/1500\n",
      "18/18 [==============================] - 0s 983us/step - loss: 0.4361 - accuracy: 0.8056 - val_loss: 0.5289 - val_accuracy: 0.7344\n",
      "Epoch 842/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4361 - accuracy: 0.8056 - val_loss: 0.5289 - val_accuracy: 0.7344\n",
      "Epoch 843/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4361 - accuracy: 0.8056 - val_loss: 0.5288 - val_accuracy: 0.7344\n",
      "Epoch 844/1500\n",
      "18/18 [==============================] - 0s 974us/step - loss: 0.4360 - accuracy: 0.8056 - val_loss: 0.5288 - val_accuracy: 0.7344\n",
      "Epoch 845/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4360 - accuracy: 0.8056 - val_loss: 0.5288 - val_accuracy: 0.7344\n",
      "Epoch 846/1500\n",
      "18/18 [==============================] - 0s 952us/step - loss: 0.4360 - accuracy: 0.8056 - val_loss: 0.5288 - val_accuracy: 0.7344\n",
      "Epoch 847/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4360 - accuracy: 0.8056 - val_loss: 0.5288 - val_accuracy: 0.7344\n",
      "Epoch 848/1500\n",
      "18/18 [==============================] - 0s 977us/step - loss: 0.4359 - accuracy: 0.8056 - val_loss: 0.5288 - val_accuracy: 0.7344\n",
      "Epoch 849/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4359 - accuracy: 0.8056 - val_loss: 0.5287 - val_accuracy: 0.7396\n",
      "Epoch 850/1500\n",
      "18/18 [==============================] - 0s 992us/step - loss: 0.4359 - accuracy: 0.8038 - val_loss: 0.5287 - val_accuracy: 0.7396\n",
      "Epoch 851/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4359 - accuracy: 0.8056 - val_loss: 0.5287 - val_accuracy: 0.7396\n",
      "Epoch 852/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4358 - accuracy: 0.8056 - val_loss: 0.5287 - val_accuracy: 0.7396\n",
      "Epoch 853/1500\n",
      "18/18 [==============================] - 0s 976us/step - loss: 0.4358 - accuracy: 0.8038 - val_loss: 0.5287 - val_accuracy: 0.7396\n",
      "Epoch 854/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4358 - accuracy: 0.8038 - val_loss: 0.5286 - val_accuracy: 0.7396\n",
      "Epoch 855/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4357 - accuracy: 0.8056 - val_loss: 0.5286 - val_accuracy: 0.7396\n",
      "Epoch 856/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4357 - accuracy: 0.8038 - val_loss: 0.5286 - val_accuracy: 0.7396\n",
      "Epoch 857/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4356 - accuracy: 0.8056 - val_loss: 0.5286 - val_accuracy: 0.7396\n",
      "Epoch 858/1500\n",
      "18/18 [==============================] - 0s 981us/step - loss: 0.4356 - accuracy: 0.8038 - val_loss: 0.5285 - val_accuracy: 0.7396\n",
      "Epoch 859/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4356 - accuracy: 0.8038 - val_loss: 0.5285 - val_accuracy: 0.7396\n",
      "Epoch 860/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4356 - accuracy: 0.8038 - val_loss: 0.5285 - val_accuracy: 0.7396\n",
      "Epoch 861/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4356 - accuracy: 0.8038 - val_loss: 0.5285 - val_accuracy: 0.7396\n",
      "Epoch 862/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4355 - accuracy: 0.8038 - val_loss: 0.5284 - val_accuracy: 0.7396\n",
      "Epoch 863/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4355 - accuracy: 0.8038 - val_loss: 0.5284 - val_accuracy: 0.7396\n",
      "Epoch 864/1500\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.4355 - accuracy: 0.8038 - val_loss: 0.5284 - val_accuracy: 0.7396\n",
      "Epoch 865/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4355 - accuracy: 0.8038 - val_loss: 0.5284 - val_accuracy: 0.7396\n",
      "Epoch 866/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4354 - accuracy: 0.8038 - val_loss: 0.5284 - val_accuracy: 0.7396\n",
      "Epoch 867/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4354 - accuracy: 0.8038 - val_loss: 0.5283 - val_accuracy: 0.7396\n",
      "Epoch 868/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4354 - accuracy: 0.8038 - val_loss: 0.5283 - val_accuracy: 0.7396\n",
      "Epoch 869/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4353 - accuracy: 0.8038 - val_loss: 0.5283 - val_accuracy: 0.7396\n",
      "Epoch 870/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4353 - accuracy: 0.8038 - val_loss: 0.5283 - val_accuracy: 0.7396\n",
      "Epoch 871/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4353 - accuracy: 0.8038 - val_loss: 0.5283 - val_accuracy: 0.7396\n",
      "Epoch 872/1500\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.4353 - accuracy: 0.8038 - val_loss: 0.5282 - val_accuracy: 0.7396\n",
      "Epoch 873/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4352 - accuracy: 0.8038 - val_loss: 0.5282 - val_accuracy: 0.7396\n",
      "Epoch 874/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4352 - accuracy: 0.8038 - val_loss: 0.5282 - val_accuracy: 0.7396\n",
      "Epoch 875/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4352 - accuracy: 0.8038 - val_loss: 0.5282 - val_accuracy: 0.7396\n",
      "Epoch 876/1500\n",
      "18/18 [==============================] - 0s 993us/step - loss: 0.4352 - accuracy: 0.8038 - val_loss: 0.5282 - val_accuracy: 0.7448\n",
      "Epoch 877/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4351 - accuracy: 0.8038 - val_loss: 0.5282 - val_accuracy: 0.7448\n",
      "Epoch 878/1500\n",
      "18/18 [==============================] - 0s 952us/step - loss: 0.4351 - accuracy: 0.8038 - val_loss: 0.5281 - val_accuracy: 0.7448\n",
      "Epoch 879/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4351 - accuracy: 0.8021 - val_loss: 0.5281 - val_accuracy: 0.7448\n",
      "Epoch 880/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4351 - accuracy: 0.8038 - val_loss: 0.5281 - val_accuracy: 0.7448\n",
      "Epoch 881/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4350 - accuracy: 0.8038 - val_loss: 0.5281 - val_accuracy: 0.7448\n",
      "Epoch 882/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4350 - accuracy: 0.8021 - val_loss: 0.5281 - val_accuracy: 0.7448\n",
      "Epoch 883/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4350 - accuracy: 0.8038 - val_loss: 0.5280 - val_accuracy: 0.7448\n",
      "Epoch 884/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4350 - accuracy: 0.8038 - val_loss: 0.5280 - val_accuracy: 0.7448\n",
      "Epoch 885/1500\n",
      "18/18 [==============================] - 0s 964us/step - loss: 0.4349 - accuracy: 0.8021 - val_loss: 0.5280 - val_accuracy: 0.7448\n",
      "Epoch 886/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4350 - accuracy: 0.8021 - val_loss: 0.5280 - val_accuracy: 0.7448\n",
      "Epoch 887/1500\n",
      "18/18 [==============================] - 0s 967us/step - loss: 0.4349 - accuracy: 0.8021 - val_loss: 0.5279 - val_accuracy: 0.7448\n",
      "Epoch 888/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4348 - accuracy: 0.8021 - val_loss: 0.5279 - val_accuracy: 0.7448\n",
      "Epoch 889/1500\n",
      "18/18 [==============================] - 0s 942us/step - loss: 0.4348 - accuracy: 0.8021 - val_loss: 0.5279 - val_accuracy: 0.7448\n",
      "Epoch 890/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4348 - accuracy: 0.8021 - val_loss: 0.5279 - val_accuracy: 0.7448\n",
      "Epoch 891/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4348 - accuracy: 0.8021 - val_loss: 0.5278 - val_accuracy: 0.7448\n",
      "Epoch 892/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4347 - accuracy: 0.8021 - val_loss: 0.5278 - val_accuracy: 0.7448\n",
      "Epoch 893/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4347 - accuracy: 0.8021 - val_loss: 0.5278 - val_accuracy: 0.7448\n",
      "Epoch 894/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4347 - accuracy: 0.8038 - val_loss: 0.5278 - val_accuracy: 0.7448\n",
      "Epoch 895/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4346 - accuracy: 0.8021 - val_loss: 0.5277 - val_accuracy: 0.7448\n",
      "Epoch 896/1500\n",
      "18/18 [==============================] - 0s 974us/step - loss: 0.4346 - accuracy: 0.8038 - val_loss: 0.5277 - val_accuracy: 0.7448\n",
      "Epoch 897/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4346 - accuracy: 0.8038 - val_loss: 0.5277 - val_accuracy: 0.7448\n",
      "Epoch 898/1500\n",
      "18/18 [==============================] - 0s 981us/step - loss: 0.4345 - accuracy: 0.8038 - val_loss: 0.5276 - val_accuracy: 0.7448\n",
      "Epoch 899/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4345 - accuracy: 0.8038 - val_loss: 0.5276 - val_accuracy: 0.7448\n",
      "Epoch 900/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4345 - accuracy: 0.8038 - val_loss: 0.5276 - val_accuracy: 0.7448\n",
      "Epoch 901/1500\n",
      "18/18 [==============================] - 0s 988us/step - loss: 0.4344 - accuracy: 0.8038 - val_loss: 0.5276 - val_accuracy: 0.7448\n",
      "Epoch 902/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4344 - accuracy: 0.8038 - val_loss: 0.5276 - val_accuracy: 0.7448\n",
      "Epoch 903/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4344 - accuracy: 0.8038 - val_loss: 0.5275 - val_accuracy: 0.7448\n",
      "Epoch 904/1500\n",
      "18/18 [==============================] - 0s 988us/step - loss: 0.4344 - accuracy: 0.8038 - val_loss: 0.5275 - val_accuracy: 0.7448\n",
      "Epoch 905/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4344 - accuracy: 0.8038 - val_loss: 0.5275 - val_accuracy: 0.7448\n",
      "Epoch 906/1500\n",
      "18/18 [==============================] - 0s 968us/step - loss: 0.4343 - accuracy: 0.8038 - val_loss: 0.5275 - val_accuracy: 0.7448\n",
      "Epoch 907/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4343 - accuracy: 0.8038 - val_loss: 0.5275 - val_accuracy: 0.7448\n",
      "Epoch 908/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4343 - accuracy: 0.8038 - val_loss: 0.5274 - val_accuracy: 0.7448\n",
      "Epoch 909/1500\n",
      "18/18 [==============================] - 0s 963us/step - loss: 0.4342 - accuracy: 0.8038 - val_loss: 0.5274 - val_accuracy: 0.7448\n",
      "Epoch 910/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4342 - accuracy: 0.8038 - val_loss: 0.5274 - val_accuracy: 0.7448\n",
      "Epoch 911/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4342 - accuracy: 0.8038 - val_loss: 0.5274 - val_accuracy: 0.7448\n",
      "Epoch 912/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4341 - accuracy: 0.8038 - val_loss: 0.5274 - val_accuracy: 0.7448\n",
      "Epoch 913/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4341 - accuracy: 0.8038 - val_loss: 0.5273 - val_accuracy: 0.7448\n",
      "Epoch 914/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4341 - accuracy: 0.8038 - val_loss: 0.5273 - val_accuracy: 0.7448\n",
      "Epoch 915/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4341 - accuracy: 0.8038 - val_loss: 0.5273 - val_accuracy: 0.7448\n",
      "Epoch 916/1500\n",
      "18/18 [==============================] - 0s 976us/step - loss: 0.4340 - accuracy: 0.8038 - val_loss: 0.5273 - val_accuracy: 0.7448\n",
      "Epoch 917/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4341 - accuracy: 0.8038 - val_loss: 0.5273 - val_accuracy: 0.7448\n",
      "Epoch 918/1500\n",
      "18/18 [==============================] - 0s 972us/step - loss: 0.4340 - accuracy: 0.8038 - val_loss: 0.5273 - val_accuracy: 0.7448\n",
      "Epoch 919/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4340 - accuracy: 0.8038 - val_loss: 0.5273 - val_accuracy: 0.7448\n",
      "Epoch 920/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4340 - accuracy: 0.8038 - val_loss: 0.5273 - val_accuracy: 0.7448\n",
      "Epoch 921/1500\n",
      "18/18 [==============================] - 0s 965us/step - loss: 0.4339 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 922/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4339 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 923/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4339 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 924/1500\n",
      "18/18 [==============================] - 0s 964us/step - loss: 0.4339 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 925/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4338 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 926/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4338 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 927/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4338 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 928/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4338 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 929/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4337 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 930/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4337 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 931/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4337 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 932/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4337 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 933/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4336 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 934/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4336 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 935/1500\n",
      "18/18 [==============================] - 0s 969us/step - loss: 0.4336 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 936/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4336 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 937/1500\n",
      "18/18 [==============================] - 0s 978us/step - loss: 0.4335 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 938/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4335 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 939/1500\n",
      "18/18 [==============================] - 0s 978us/step - loss: 0.4335 - accuracy: 0.8038 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 940/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4335 - accuracy: 0.8038 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 941/1500\n",
      "18/18 [==============================] - 0s 965us/step - loss: 0.4334 - accuracy: 0.8038 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 942/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4334 - accuracy: 0.8038 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 943/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4334 - accuracy: 0.8038 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 944/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4334 - accuracy: 0.8038 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 945/1500\n",
      "18/18 [==============================] - 0s 937us/step - loss: 0.4333 - accuracy: 0.8038 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 946/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4333 - accuracy: 0.8038 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 947/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4333 - accuracy: 0.8038 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 948/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4332 - accuracy: 0.8038 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 949/1500\n",
      "18/18 [==============================] - 0s 960us/step - loss: 0.4333 - accuracy: 0.8038 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 950/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4332 - accuracy: 0.8056 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 951/1500\n",
      "18/18 [==============================] - 0s 969us/step - loss: 0.4332 - accuracy: 0.8038 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 952/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4332 - accuracy: 0.8038 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 953/1500\n",
      "18/18 [==============================] - 0s 990us/step - loss: 0.4332 - accuracy: 0.8038 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 954/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4331 - accuracy: 0.8038 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 955/1500\n",
      "18/18 [==============================] - 0s 984us/step - loss: 0.4331 - accuracy: 0.8038 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 956/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4330 - accuracy: 0.8038 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 957/1500\n",
      "18/18 [==============================] - 0s 977us/step - loss: 0.4330 - accuracy: 0.8038 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 958/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4330 - accuracy: 0.8038 - val_loss: 0.5271 - val_accuracy: 0.7448\n",
      "Epoch 959/1500\n",
      "18/18 [==============================] - 0s 977us/step - loss: 0.4330 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 960/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4330 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 961/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4329 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 962/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4329 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 963/1500\n",
      "18/18 [==============================] - 0s 995us/step - loss: 0.4329 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 964/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4329 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 965/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4328 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 966/1500\n",
      "18/18 [==============================] - 0s 978us/step - loss: 0.4328 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 967/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4328 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 968/1500\n",
      "18/18 [==============================] - 0s 983us/step - loss: 0.4328 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 969/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4327 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 970/1500\n",
      "18/18 [==============================] - 0s 935us/step - loss: 0.4327 - accuracy: 0.8038 - val_loss: 0.5272 - val_accuracy: 0.7448\n",
      "Epoch 971/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4327 - accuracy: 0.8038 - val_loss: 0.5273 - val_accuracy: 0.7448\n",
      "Epoch 972/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4327 - accuracy: 0.8038 - val_loss: 0.5273 - val_accuracy: 0.7448\n",
      "Epoch 973/1500\n",
      "18/18 [==============================] - 0s 965us/step - loss: 0.4327 - accuracy: 0.8038 - val_loss: 0.5273 - val_accuracy: 0.7448\n",
      "Epoch 974/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4326 - accuracy: 0.8038 - val_loss: 0.5273 - val_accuracy: 0.7448\n",
      "Epoch 975/1500\n",
      "18/18 [==============================] - 0s 968us/step - loss: 0.4326 - accuracy: 0.8038 - val_loss: 0.5273 - val_accuracy: 0.7448\n",
      "Epoch 976/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4326 - accuracy: 0.8038 - val_loss: 0.5273 - val_accuracy: 0.7448\n",
      "Epoch 977/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4326 - accuracy: 0.8038 - val_loss: 0.5273 - val_accuracy: 0.7448\n",
      "Epoch 978/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4326 - accuracy: 0.8038 - val_loss: 0.5273 - val_accuracy: 0.7448\n",
      "Epoch 979/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4326 - accuracy: 0.8038 - val_loss: 0.5273 - val_accuracy: 0.7448\n",
      "Epoch 980/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4325 - accuracy: 0.8038 - val_loss: 0.5274 - val_accuracy: 0.7448\n",
      "Epoch 981/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4325 - accuracy: 0.8038 - val_loss: 0.5274 - val_accuracy: 0.7448\n",
      "Epoch 982/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4324 - accuracy: 0.8038 - val_loss: 0.5274 - val_accuracy: 0.7448\n",
      "Epoch 983/1500\n",
      "18/18 [==============================] - 0s 983us/step - loss: 0.4324 - accuracy: 0.8038 - val_loss: 0.5274 - val_accuracy: 0.7448\n",
      "Epoch 984/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4324 - accuracy: 0.8038 - val_loss: 0.5274 - val_accuracy: 0.7448\n",
      "Epoch 985/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4324 - accuracy: 0.8038 - val_loss: 0.5274 - val_accuracy: 0.7448\n",
      "Epoch 986/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4324 - accuracy: 0.8038 - val_loss: 0.5274 - val_accuracy: 0.7448\n",
      "Epoch 987/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4323 - accuracy: 0.8038 - val_loss: 0.5274 - val_accuracy: 0.7448\n",
      "Epoch 988/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4323 - accuracy: 0.8038 - val_loss: 0.5274 - val_accuracy: 0.7448\n",
      "Epoch 989/1500\n",
      "18/18 [==============================] - 0s 977us/step - loss: 0.4323 - accuracy: 0.8038 - val_loss: 0.5275 - val_accuracy: 0.7448\n",
      "Epoch 990/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4322 - accuracy: 0.8038 - val_loss: 0.5275 - val_accuracy: 0.7448\n",
      "Epoch 991/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4322 - accuracy: 0.8038 - val_loss: 0.5275 - val_accuracy: 0.7448\n",
      "Epoch 992/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4322 - accuracy: 0.8038 - val_loss: 0.5275 - val_accuracy: 0.7448\n",
      "Epoch 993/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4322 - accuracy: 0.8038 - val_loss: 0.5275 - val_accuracy: 0.7448\n",
      "Epoch 994/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4321 - accuracy: 0.8038 - val_loss: 0.5275 - val_accuracy: 0.7448\n",
      "Epoch 995/1500\n",
      "18/18 [==============================] - 0s 927us/step - loss: 0.4321 - accuracy: 0.8038 - val_loss: 0.5275 - val_accuracy: 0.7448\n",
      "Epoch 996/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4321 - accuracy: 0.8038 - val_loss: 0.5275 - val_accuracy: 0.7448\n",
      "Epoch 997/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4321 - accuracy: 0.8038 - val_loss: 0.5275 - val_accuracy: 0.7448\n",
      "Epoch 998/1500\n",
      "18/18 [==============================] - 0s 981us/step - loss: 0.4321 - accuracy: 0.8038 - val_loss: 0.5275 - val_accuracy: 0.7448\n",
      "Epoch 999/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4320 - accuracy: 0.8038 - val_loss: 0.5275 - val_accuracy: 0.7448\n",
      "Epoch 1000/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4320 - accuracy: 0.8038 - val_loss: 0.5275 - val_accuracy: 0.7448\n",
      "Epoch 1001/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4320 - accuracy: 0.8038 - val_loss: 0.5275 - val_accuracy: 0.7448\n",
      "Epoch 1002/1500\n",
      "18/18 [==============================] - 0s 964us/step - loss: 0.4320 - accuracy: 0.8038 - val_loss: 0.5276 - val_accuracy: 0.7448\n",
      "Epoch 1003/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4319 - accuracy: 0.8038 - val_loss: 0.5276 - val_accuracy: 0.7448\n",
      "Epoch 1004/1500\n",
      "18/18 [==============================] - 0s 951us/step - loss: 0.4319 - accuracy: 0.8056 - val_loss: 0.5276 - val_accuracy: 0.7448\n",
      "Epoch 1005/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4319 - accuracy: 0.8038 - val_loss: 0.5276 - val_accuracy: 0.7448\n",
      "Epoch 1006/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4318 - accuracy: 0.8038 - val_loss: 0.5276 - val_accuracy: 0.7448\n",
      "Epoch 1007/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4318 - accuracy: 0.8038 - val_loss: 0.5276 - val_accuracy: 0.7448\n",
      "Epoch 1008/1500\n",
      "18/18 [==============================] - 0s 970us/step - loss: 0.4318 - accuracy: 0.8038 - val_loss: 0.5276 - val_accuracy: 0.7448\n",
      "Epoch 1009/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4318 - accuracy: 0.8056 - val_loss: 0.5276 - val_accuracy: 0.7448\n",
      "Epoch 1010/1500\n",
      "18/18 [==============================] - 0s 938us/step - loss: 0.4317 - accuracy: 0.8056 - val_loss: 0.5276 - val_accuracy: 0.7448\n",
      "Epoch 1011/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4317 - accuracy: 0.8038 - val_loss: 0.5276 - val_accuracy: 0.7448\n",
      "Epoch 1012/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4317 - accuracy: 0.8056 - val_loss: 0.5277 - val_accuracy: 0.7448\n",
      "Epoch 1013/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4317 - accuracy: 0.8038 - val_loss: 0.5277 - val_accuracy: 0.7448\n",
      "Epoch 1014/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4316 - accuracy: 0.8056 - val_loss: 0.5277 - val_accuracy: 0.7448\n",
      "Epoch 1015/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4316 - accuracy: 0.8056 - val_loss: 0.5277 - val_accuracy: 0.7448\n",
      "Epoch 1016/1500\n",
      "18/18 [==============================] - 0s 937us/step - loss: 0.4316 - accuracy: 0.8056 - val_loss: 0.5277 - val_accuracy: 0.7448\n",
      "Epoch 1017/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4316 - accuracy: 0.8038 - val_loss: 0.5277 - val_accuracy: 0.7448\n",
      "Epoch 1018/1500\n",
      "18/18 [==============================] - 0s 965us/step - loss: 0.4316 - accuracy: 0.8038 - val_loss: 0.5277 - val_accuracy: 0.7448\n",
      "Epoch 1019/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4315 - accuracy: 0.8038 - val_loss: 0.5277 - val_accuracy: 0.7448\n",
      "Epoch 1020/1500\n",
      "18/18 [==============================] - 0s 981us/step - loss: 0.4315 - accuracy: 0.8038 - val_loss: 0.5277 - val_accuracy: 0.7448\n",
      "Epoch 1021/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4315 - accuracy: 0.8038 - val_loss: 0.5277 - val_accuracy: 0.7448\n",
      "Epoch 1022/1500\n",
      "18/18 [==============================] - 0s 965us/step - loss: 0.4314 - accuracy: 0.8038 - val_loss: 0.5278 - val_accuracy: 0.7448\n",
      "Epoch 1023/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4314 - accuracy: 0.8038 - val_loss: 0.5278 - val_accuracy: 0.7448\n",
      "Epoch 1024/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4314 - accuracy: 0.8038 - val_loss: 0.5278 - val_accuracy: 0.7448\n",
      "Epoch 1025/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4314 - accuracy: 0.8038 - val_loss: 0.5278 - val_accuracy: 0.7448\n",
      "Epoch 1026/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4314 - accuracy: 0.8038 - val_loss: 0.5278 - val_accuracy: 0.7448\n",
      "Epoch 1027/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4313 - accuracy: 0.8038 - val_loss: 0.5278 - val_accuracy: 0.7448\n",
      "Epoch 1028/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4313 - accuracy: 0.8038 - val_loss: 0.5278 - val_accuracy: 0.7448\n",
      "Epoch 1029/1500\n",
      "18/18 [==============================] - 0s 995us/step - loss: 0.4313 - accuracy: 0.8038 - val_loss: 0.5278 - val_accuracy: 0.7448\n",
      "Epoch 1030/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4312 - accuracy: 0.8038 - val_loss: 0.5278 - val_accuracy: 0.7448\n",
      "Epoch 1031/1500\n",
      "18/18 [==============================] - 0s 949us/step - loss: 0.4312 - accuracy: 0.8038 - val_loss: 0.5279 - val_accuracy: 0.7448\n",
      "Epoch 1032/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4312 - accuracy: 0.8038 - val_loss: 0.5279 - val_accuracy: 0.7448\n",
      "Epoch 1033/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4311 - accuracy: 0.8038 - val_loss: 0.5279 - val_accuracy: 0.7448\n",
      "Epoch 1034/1500\n",
      "18/18 [==============================] - 0s 990us/step - loss: 0.4311 - accuracy: 0.8038 - val_loss: 0.5279 - val_accuracy: 0.7448\n",
      "Epoch 1035/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4311 - accuracy: 0.8038 - val_loss: 0.5279 - val_accuracy: 0.7448\n",
      "Epoch 1036/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4311 - accuracy: 0.8038 - val_loss: 0.5279 - val_accuracy: 0.7448\n",
      "Epoch 1037/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4311 - accuracy: 0.8038 - val_loss: 0.5280 - val_accuracy: 0.7448\n",
      "Epoch 1038/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4311 - accuracy: 0.8038 - val_loss: 0.5280 - val_accuracy: 0.7448\n",
      "Epoch 1039/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4310 - accuracy: 0.8038 - val_loss: 0.5280 - val_accuracy: 0.7448\n",
      "Epoch 1040/1500\n",
      "18/18 [==============================] - 0s 965us/step - loss: 0.4310 - accuracy: 0.8038 - val_loss: 0.5280 - val_accuracy: 0.7448\n",
      "Epoch 1041/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4310 - accuracy: 0.8038 - val_loss: 0.5280 - val_accuracy: 0.7448\n",
      "Epoch 1042/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4310 - accuracy: 0.8038 - val_loss: 0.5280 - val_accuracy: 0.7448\n",
      "Epoch 1043/1500\n",
      "18/18 [==============================] - 0s 978us/step - loss: 0.4309 - accuracy: 0.8038 - val_loss: 0.5280 - val_accuracy: 0.7448\n",
      "Epoch 1044/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4309 - accuracy: 0.8038 - val_loss: 0.5280 - val_accuracy: 0.7448\n",
      "Epoch 1045/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4309 - accuracy: 0.8038 - val_loss: 0.5280 - val_accuracy: 0.7448\n",
      "Epoch 1046/1500\n",
      "18/18 [==============================] - 0s 968us/step - loss: 0.4309 - accuracy: 0.8038 - val_loss: 0.5281 - val_accuracy: 0.7448\n",
      "Epoch 1047/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4309 - accuracy: 0.8038 - val_loss: 0.5281 - val_accuracy: 0.7448\n",
      "Epoch 1048/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4309 - accuracy: 0.8038 - val_loss: 0.5281 - val_accuracy: 0.7396\n",
      "Epoch 1049/1500\n",
      "18/18 [==============================] - 0s 948us/step - loss: 0.4308 - accuracy: 0.8038 - val_loss: 0.5281 - val_accuracy: 0.7396\n",
      "Epoch 1050/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4308 - accuracy: 0.8038 - val_loss: 0.5281 - val_accuracy: 0.7396\n",
      "Epoch 1051/1500\n",
      "18/18 [==============================] - 0s 983us/step - loss: 0.4308 - accuracy: 0.8038 - val_loss: 0.5281 - val_accuracy: 0.7396\n",
      "Epoch 1052/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4307 - accuracy: 0.8038 - val_loss: 0.5281 - val_accuracy: 0.7396\n",
      "Epoch 1053/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4307 - accuracy: 0.8038 - val_loss: 0.5282 - val_accuracy: 0.7396\n",
      "Epoch 1054/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4307 - accuracy: 0.8038 - val_loss: 0.5282 - val_accuracy: 0.7396\n",
      "Epoch 1055/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4306 - accuracy: 0.8056 - val_loss: 0.5282 - val_accuracy: 0.7396\n",
      "Epoch 1056/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4307 - accuracy: 0.8056 - val_loss: 0.5282 - val_accuracy: 0.7396\n",
      "Epoch 1057/1500\n",
      "18/18 [==============================] - 0s 953us/step - loss: 0.4306 - accuracy: 0.8056 - val_loss: 0.5282 - val_accuracy: 0.7396\n",
      "Epoch 1058/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4306 - accuracy: 0.8056 - val_loss: 0.5283 - val_accuracy: 0.7396\n",
      "Epoch 1059/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4306 - accuracy: 0.8056 - val_loss: 0.5283 - val_accuracy: 0.7396\n",
      "Epoch 1060/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4306 - accuracy: 0.8056 - val_loss: 0.5283 - val_accuracy: 0.7396\n",
      "Epoch 1061/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4305 - accuracy: 0.8056 - val_loss: 0.5283 - val_accuracy: 0.7396\n",
      "Epoch 1062/1500\n",
      "18/18 [==============================] - 0s 973us/step - loss: 0.4305 - accuracy: 0.8056 - val_loss: 0.5283 - val_accuracy: 0.7396\n",
      "Epoch 1063/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4305 - accuracy: 0.8056 - val_loss: 0.5283 - val_accuracy: 0.7396\n",
      "Epoch 1064/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4305 - accuracy: 0.8056 - val_loss: 0.5284 - val_accuracy: 0.7396\n",
      "Epoch 1065/1500\n",
      "18/18 [==============================] - 0s 989us/step - loss: 0.4305 - accuracy: 0.8056 - val_loss: 0.5284 - val_accuracy: 0.7396\n",
      "Epoch 1066/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4304 - accuracy: 0.8056 - val_loss: 0.5284 - val_accuracy: 0.7396\n",
      "Epoch 1067/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4304 - accuracy: 0.8056 - val_loss: 0.5284 - val_accuracy: 0.7396\n",
      "Epoch 1068/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4304 - accuracy: 0.8056 - val_loss: 0.5284 - val_accuracy: 0.7396\n",
      "Epoch 1069/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4303 - accuracy: 0.8056 - val_loss: 0.5284 - val_accuracy: 0.7396\n",
      "Epoch 1070/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4303 - accuracy: 0.8056 - val_loss: 0.5285 - val_accuracy: 0.7396\n",
      "Epoch 1071/1500\n",
      "18/18 [==============================] - 0s 956us/step - loss: 0.4303 - accuracy: 0.8056 - val_loss: 0.5285 - val_accuracy: 0.7396\n",
      "Epoch 1072/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4303 - accuracy: 0.8056 - val_loss: 0.5285 - val_accuracy: 0.7396\n",
      "Epoch 1073/1500\n",
      "18/18 [==============================] - 0s 982us/step - loss: 0.4303 - accuracy: 0.8056 - val_loss: 0.5285 - val_accuracy: 0.7396\n",
      "Epoch 1074/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4302 - accuracy: 0.8056 - val_loss: 0.5285 - val_accuracy: 0.7396\n",
      "Epoch 1075/1500\n",
      "18/18 [==============================] - 0s 993us/step - loss: 0.4303 - accuracy: 0.8056 - val_loss: 0.5286 - val_accuracy: 0.7396\n",
      "Epoch 1076/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4302 - accuracy: 0.8056 - val_loss: 0.5286 - val_accuracy: 0.7396\n",
      "Epoch 1077/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4302 - accuracy: 0.8056 - val_loss: 0.5286 - val_accuracy: 0.7396\n",
      "Epoch 1078/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4302 - accuracy: 0.8056 - val_loss: 0.5286 - val_accuracy: 0.7396\n",
      "Epoch 1079/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4302 - accuracy: 0.8056 - val_loss: 0.5286 - val_accuracy: 0.7396\n",
      "Epoch 1080/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4301 - accuracy: 0.8056 - val_loss: 0.5287 - val_accuracy: 0.7396\n",
      "Epoch 1081/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4301 - accuracy: 0.8056 - val_loss: 0.5287 - val_accuracy: 0.7396\n",
      "Epoch 1082/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4301 - accuracy: 0.8056 - val_loss: 0.5287 - val_accuracy: 0.7396\n",
      "Epoch 1083/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4301 - accuracy: 0.8056 - val_loss: 0.5287 - val_accuracy: 0.7396\n",
      "Epoch 1084/1500\n",
      "18/18 [==============================] - 0s 958us/step - loss: 0.4300 - accuracy: 0.8056 - val_loss: 0.5287 - val_accuracy: 0.7396\n",
      "Epoch 1085/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4300 - accuracy: 0.8056 - val_loss: 0.5288 - val_accuracy: 0.7396\n",
      "Epoch 1086/1500\n",
      "18/18 [==============================] - 0s 960us/step - loss: 0.4300 - accuracy: 0.8056 - val_loss: 0.5288 - val_accuracy: 0.7396\n",
      "Epoch 1087/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4300 - accuracy: 0.8073 - val_loss: 0.5288 - val_accuracy: 0.7396\n",
      "Epoch 1088/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4300 - accuracy: 0.8056 - val_loss: 0.5288 - val_accuracy: 0.7396\n",
      "Epoch 1089/1500\n",
      "18/18 [==============================] - 0s 980us/step - loss: 0.4300 - accuracy: 0.8073 - val_loss: 0.5288 - val_accuracy: 0.7448\n",
      "Epoch 1090/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4299 - accuracy: 0.8073 - val_loss: 0.5288 - val_accuracy: 0.7448\n",
      "Epoch 1091/1500\n",
      "18/18 [==============================] - 0s 942us/step - loss: 0.4299 - accuracy: 0.8056 - val_loss: 0.5288 - val_accuracy: 0.7448\n",
      "Epoch 1092/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4299 - accuracy: 0.8056 - val_loss: 0.5288 - val_accuracy: 0.7448\n",
      "Epoch 1093/1500\n",
      "18/18 [==============================] - 0s 983us/step - loss: 0.4299 - accuracy: 0.8056 - val_loss: 0.5288 - val_accuracy: 0.7448\n",
      "Epoch 1094/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4298 - accuracy: 0.8056 - val_loss: 0.5288 - val_accuracy: 0.7448\n",
      "Epoch 1095/1500\n",
      "18/18 [==============================] - 0s 937us/step - loss: 0.4298 - accuracy: 0.8056 - val_loss: 0.5288 - val_accuracy: 0.7448\n",
      "Epoch 1096/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4298 - accuracy: 0.8056 - val_loss: 0.5288 - val_accuracy: 0.7448\n",
      "Epoch 1097/1500\n",
      "18/18 [==============================] - 0s 975us/step - loss: 0.4298 - accuracy: 0.8056 - val_loss: 0.5289 - val_accuracy: 0.7448\n",
      "Epoch 1098/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4298 - accuracy: 0.8056 - val_loss: 0.5289 - val_accuracy: 0.7448\n",
      "Epoch 1099/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4298 - accuracy: 0.8056 - val_loss: 0.5289 - val_accuracy: 0.7448\n",
      "Epoch 1100/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4297 - accuracy: 0.8056 - val_loss: 0.5289 - val_accuracy: 0.7448\n",
      "Epoch 1101/1500\n",
      "18/18 [==============================] - 0s 978us/step - loss: 0.4297 - accuracy: 0.8056 - val_loss: 0.5289 - val_accuracy: 0.7448\n",
      "Epoch 1102/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4297 - accuracy: 0.8056 - val_loss: 0.5289 - val_accuracy: 0.7448\n",
      "Epoch 1103/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4297 - accuracy: 0.8056 - val_loss: 0.5289 - val_accuracy: 0.7448\n",
      "Epoch 1104/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4296 - accuracy: 0.8056 - val_loss: 0.5289 - val_accuracy: 0.7448\n",
      "Epoch 1105/1500\n",
      "18/18 [==============================] - 0s 957us/step - loss: 0.4296 - accuracy: 0.8073 - val_loss: 0.5289 - val_accuracy: 0.7448\n",
      "Epoch 1106/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4296 - accuracy: 0.8073 - val_loss: 0.5289 - val_accuracy: 0.7448\n",
      "Epoch 1107/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4296 - accuracy: 0.8056 - val_loss: 0.5289 - val_accuracy: 0.7448\n",
      "Epoch 1108/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4295 - accuracy: 0.8056 - val_loss: 0.5290 - val_accuracy: 0.7448\n",
      "Epoch 1109/1500\n",
      "18/18 [==============================] - 0s 961us/step - loss: 0.4295 - accuracy: 0.8056 - val_loss: 0.5290 - val_accuracy: 0.7448\n",
      "Epoch 1110/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4295 - accuracy: 0.8073 - val_loss: 0.5290 - val_accuracy: 0.7448\n",
      "Epoch 1111/1500\n",
      "18/18 [==============================] - 0s 968us/step - loss: 0.4295 - accuracy: 0.8056 - val_loss: 0.5290 - val_accuracy: 0.7448\n",
      "Epoch 1112/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4295 - accuracy: 0.8056 - val_loss: 0.5290 - val_accuracy: 0.7448\n",
      "Epoch 1113/1500\n",
      "18/18 [==============================] - 0s 969us/step - loss: 0.4294 - accuracy: 0.8073 - val_loss: 0.5290 - val_accuracy: 0.7448\n",
      "Epoch 1114/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4294 - accuracy: 0.8073 - val_loss: 0.5290 - val_accuracy: 0.7448\n",
      "Epoch 1115/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4294 - accuracy: 0.8073 - val_loss: 0.5290 - val_accuracy: 0.7448\n",
      "Epoch 1116/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4294 - accuracy: 0.8073 - val_loss: 0.5290 - val_accuracy: 0.7448\n",
      "Epoch 1117/1500\n",
      "18/18 [==============================] - 0s 950us/step - loss: 0.4294 - accuracy: 0.8073 - val_loss: 0.5291 - val_accuracy: 0.7448\n",
      "Epoch 1118/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4293 - accuracy: 0.8073 - val_loss: 0.5291 - val_accuracy: 0.7448\n",
      "Epoch 1119/1500\n",
      "18/18 [==============================] - 0s 940us/step - loss: 0.4293 - accuracy: 0.8073 - val_loss: 0.5291 - val_accuracy: 0.7448\n",
      "Epoch 1120/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4293 - accuracy: 0.8073 - val_loss: 0.5291 - val_accuracy: 0.7448\n",
      "Epoch 1121/1500\n",
      "18/18 [==============================] - 0s 980us/step - loss: 0.4292 - accuracy: 0.8073 - val_loss: 0.5291 - val_accuracy: 0.7448\n",
      "Epoch 1122/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4293 - accuracy: 0.8073 - val_loss: 0.5291 - val_accuracy: 0.7448\n",
      "Epoch 1123/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4292 - accuracy: 0.8056 - val_loss: 0.5291 - val_accuracy: 0.7448\n",
      "Epoch 1124/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4292 - accuracy: 0.8056 - val_loss: 0.5291 - val_accuracy: 0.7448\n",
      "Epoch 1125/1500\n",
      "18/18 [==============================] - 0s 957us/step - loss: 0.4292 - accuracy: 0.8073 - val_loss: 0.5291 - val_accuracy: 0.7448\n",
      "Epoch 1126/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4291 - accuracy: 0.8073 - val_loss: 0.5291 - val_accuracy: 0.7448\n",
      "Epoch 1127/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4291 - accuracy: 0.8073 - val_loss: 0.5291 - val_accuracy: 0.7448\n",
      "Epoch 1128/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4291 - accuracy: 0.8073 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1129/1500\n",
      "18/18 [==============================] - 0s 942us/step - loss: 0.4291 - accuracy: 0.8073 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1130/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4290 - accuracy: 0.8090 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1131/1500\n",
      "18/18 [==============================] - 0s 961us/step - loss: 0.4290 - accuracy: 0.8073 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1132/1500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4290 - accuracy: 0.8073 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1133/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4289 - accuracy: 0.8073 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1134/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4289 - accuracy: 0.8090 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1135/1500\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.4289 - accuracy: 0.8090 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1136/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4289 - accuracy: 0.8090 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1137/1500\n",
      "18/18 [==============================] - 0s 974us/step - loss: 0.4288 - accuracy: 0.8073 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1138/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4288 - accuracy: 0.8056 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1139/1500\n",
      "18/18 [==============================] - 0s 973us/step - loss: 0.4288 - accuracy: 0.8073 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1140/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4287 - accuracy: 0.8073 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1141/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4287 - accuracy: 0.8090 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1142/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4287 - accuracy: 0.8073 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1143/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4287 - accuracy: 0.8090 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1144/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4286 - accuracy: 0.8090 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1145/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4286 - accuracy: 0.8090 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1146/1500\n",
      "18/18 [==============================] - 0s 944us/step - loss: 0.4286 - accuracy: 0.8090 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1147/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4286 - accuracy: 0.8090 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1148/1500\n",
      "18/18 [==============================] - 0s 956us/step - loss: 0.4285 - accuracy: 0.8073 - val_loss: 0.5292 - val_accuracy: 0.7448\n",
      "Epoch 1149/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4285 - accuracy: 0.8056 - val_loss: 0.5293 - val_accuracy: 0.7448\n",
      "Epoch 1150/1500\n",
      "18/18 [==============================] - 0s 953us/step - loss: 0.4285 - accuracy: 0.8090 - val_loss: 0.5293 - val_accuracy: 0.7448\n",
      "Epoch 1151/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4285 - accuracy: 0.8090 - val_loss: 0.5293 - val_accuracy: 0.7448\n",
      "Epoch 1152/1500\n",
      "18/18 [==============================] - 0s 989us/step - loss: 0.4284 - accuracy: 0.8090 - val_loss: 0.5293 - val_accuracy: 0.7448\n",
      "Epoch 1153/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4284 - accuracy: 0.8090 - val_loss: 0.5293 - val_accuracy: 0.7448\n",
      "Epoch 1154/1500\n",
      "18/18 [==============================] - 0s 962us/step - loss: 0.4284 - accuracy: 0.8090 - val_loss: 0.5293 - val_accuracy: 0.7448\n",
      "Epoch 1155/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4284 - accuracy: 0.8090 - val_loss: 0.5293 - val_accuracy: 0.7448\n",
      "Epoch 1156/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4283 - accuracy: 0.8090 - val_loss: 0.5293 - val_accuracy: 0.7448\n",
      "Epoch 1157/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4283 - accuracy: 0.8090 - val_loss: 0.5293 - val_accuracy: 0.7448\n",
      "Epoch 1158/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4283 - accuracy: 0.8090 - val_loss: 0.5293 - val_accuracy: 0.7448\n",
      "Epoch 1159/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4282 - accuracy: 0.8090 - val_loss: 0.5293 - val_accuracy: 0.7448\n",
      "Epoch 1160/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4282 - accuracy: 0.8056 - val_loss: 0.5293 - val_accuracy: 0.7448\n",
      "Epoch 1161/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4282 - accuracy: 0.8073 - val_loss: 0.5293 - val_accuracy: 0.7448\n",
      "Epoch 1162/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4282 - accuracy: 0.8073 - val_loss: 0.5293 - val_accuracy: 0.7448\n",
      "Epoch 1163/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4282 - accuracy: 0.8090 - val_loss: 0.5293 - val_accuracy: 0.7448\n",
      "Epoch 1164/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4281 - accuracy: 0.8090 - val_loss: 0.5293 - val_accuracy: 0.7448\n",
      "Epoch 1165/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4281 - accuracy: 0.8090 - val_loss: 0.5293 - val_accuracy: 0.7448\n",
      "Epoch 1166/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4281 - accuracy: 0.8090 - val_loss: 0.5293 - val_accuracy: 0.7448\n",
      "Epoch 1167/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4280 - accuracy: 0.8090 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 1168/1500\n",
      "18/18 [==============================] - 0s 986us/step - loss: 0.4280 - accuracy: 0.8073 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 1169/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4280 - accuracy: 0.8090 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 1170/1500\n",
      "18/18 [==============================] - 0s 987us/step - loss: 0.4280 - accuracy: 0.8090 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 1171/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4279 - accuracy: 0.8073 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 1172/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4279 - accuracy: 0.8090 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 1173/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4278 - accuracy: 0.8090 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 1174/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4278 - accuracy: 0.8073 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 1175/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4278 - accuracy: 0.8056 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 1176/1500\n",
      "18/18 [==============================] - 0s 998us/step - loss: 0.4278 - accuracy: 0.8090 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 1177/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4277 - accuracy: 0.8073 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 1178/1500\n",
      "18/18 [==============================] - 0s 974us/step - loss: 0.4277 - accuracy: 0.8056 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 1179/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4277 - accuracy: 0.8073 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 1180/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4277 - accuracy: 0.8056 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 1181/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4276 - accuracy: 0.8073 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 1182/1500\n",
      "18/18 [==============================] - 0s 988us/step - loss: 0.4276 - accuracy: 0.8056 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 1183/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4275 - accuracy: 0.8090 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 1184/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4276 - accuracy: 0.8056 - val_loss: 0.5294 - val_accuracy: 0.7448\n",
      "Epoch 1185/1500\n",
      "18/18 [==============================] - 0s 980us/step - loss: 0.4275 - accuracy: 0.8056 - val_loss: 0.5294 - val_accuracy: 0.7500\n",
      "Epoch 1186/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4275 - accuracy: 0.8090 - val_loss: 0.5294 - val_accuracy: 0.7500\n",
      "Epoch 1187/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4274 - accuracy: 0.8056 - val_loss: 0.5294 - val_accuracy: 0.7500\n",
      "Epoch 1188/1500\n",
      "18/18 [==============================] - 0s 951us/step - loss: 0.4274 - accuracy: 0.8090 - val_loss: 0.5294 - val_accuracy: 0.7500\n",
      "Epoch 1189/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4274 - accuracy: 0.8073 - val_loss: 0.5294 - val_accuracy: 0.7500\n",
      "Epoch 1190/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4273 - accuracy: 0.8073 - val_loss: 0.5294 - val_accuracy: 0.7500\n",
      "Epoch 1191/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4273 - accuracy: 0.8090 - val_loss: 0.5294 - val_accuracy: 0.7500\n",
      "Epoch 1192/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4273 - accuracy: 0.8073 - val_loss: 0.5294 - val_accuracy: 0.7500\n",
      "Epoch 1193/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4272 - accuracy: 0.8073 - val_loss: 0.5294 - val_accuracy: 0.7500\n",
      "Epoch 1194/1500\n",
      "18/18 [==============================] - 0s 979us/step - loss: 0.4272 - accuracy: 0.8073 - val_loss: 0.5294 - val_accuracy: 0.7500\n",
      "Epoch 1195/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4272 - accuracy: 0.8073 - val_loss: 0.5294 - val_accuracy: 0.7500\n",
      "Epoch 1196/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4272 - accuracy: 0.8073 - val_loss: 0.5294 - val_accuracy: 0.7500\n",
      "Epoch 1197/1500\n",
      "18/18 [==============================] - 0s 937us/step - loss: 0.4271 - accuracy: 0.8073 - val_loss: 0.5294 - val_accuracy: 0.7500\n",
      "Epoch 1198/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4271 - accuracy: 0.8073 - val_loss: 0.5294 - val_accuracy: 0.7500\n",
      "Epoch 1199/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4271 - accuracy: 0.8073 - val_loss: 0.5294 - val_accuracy: 0.7500\n",
      "Epoch 1200/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4271 - accuracy: 0.8073 - val_loss: 0.5294 - val_accuracy: 0.7500\n",
      "Epoch 1201/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4270 - accuracy: 0.8073 - val_loss: 0.5294 - val_accuracy: 0.7500\n",
      "Epoch 1202/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4270 - accuracy: 0.8073 - val_loss: 0.5294 - val_accuracy: 0.7500\n",
      "Epoch 1203/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4270 - accuracy: 0.8073 - val_loss: 0.5294 - val_accuracy: 0.7500\n",
      "Epoch 1204/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4269 - accuracy: 0.8073 - val_loss: 0.5295 - val_accuracy: 0.7500\n",
      "Epoch 1205/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4269 - accuracy: 0.8073 - val_loss: 0.5295 - val_accuracy: 0.7500\n",
      "Epoch 1206/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4269 - accuracy: 0.8073 - val_loss: 0.5295 - val_accuracy: 0.7500\n",
      "Epoch 1207/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4268 - accuracy: 0.8073 - val_loss: 0.5295 - val_accuracy: 0.7500\n",
      "Epoch 1208/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4268 - accuracy: 0.8073 - val_loss: 0.5295 - val_accuracy: 0.7500\n",
      "Epoch 1209/1500\n",
      "18/18 [==============================] - 0s 918us/step - loss: 0.4268 - accuracy: 0.8073 - val_loss: 0.5295 - val_accuracy: 0.7500\n",
      "Epoch 1210/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4267 - accuracy: 0.8073 - val_loss: 0.5295 - val_accuracy: 0.7500\n",
      "Epoch 1211/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4267 - accuracy: 0.8073 - val_loss: 0.5295 - val_accuracy: 0.7500\n",
      "Epoch 1212/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4267 - accuracy: 0.8090 - val_loss: 0.5295 - val_accuracy: 0.7500\n",
      "Epoch 1213/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4266 - accuracy: 0.8090 - val_loss: 0.5295 - val_accuracy: 0.7500\n",
      "Epoch 1214/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4266 - accuracy: 0.8090 - val_loss: 0.5295 - val_accuracy: 0.7500\n",
      "Epoch 1215/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4266 - accuracy: 0.8090 - val_loss: 0.5295 - val_accuracy: 0.7500\n",
      "Epoch 1216/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4266 - accuracy: 0.8090 - val_loss: 0.5295 - val_accuracy: 0.7500\n",
      "Epoch 1217/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4265 - accuracy: 0.8090 - val_loss: 0.5296 - val_accuracy: 0.7500\n",
      "Epoch 1218/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4265 - accuracy: 0.8108 - val_loss: 0.5296 - val_accuracy: 0.7500\n",
      "Epoch 1219/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4265 - accuracy: 0.8090 - val_loss: 0.5296 - val_accuracy: 0.7500\n",
      "Epoch 1220/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4264 - accuracy: 0.8090 - val_loss: 0.5296 - val_accuracy: 0.7500\n",
      "Epoch 1221/1500\n",
      "18/18 [==============================] - 0s 984us/step - loss: 0.4264 - accuracy: 0.8090 - val_loss: 0.5296 - val_accuracy: 0.7500\n",
      "Epoch 1222/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4264 - accuracy: 0.8090 - val_loss: 0.5296 - val_accuracy: 0.7500\n",
      "Epoch 1223/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4264 - accuracy: 0.8090 - val_loss: 0.5296 - val_accuracy: 0.7500\n",
      "Epoch 1224/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4264 - accuracy: 0.8090 - val_loss: 0.5296 - val_accuracy: 0.7500\n",
      "Epoch 1225/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4263 - accuracy: 0.8090 - val_loss: 0.5296 - val_accuracy: 0.7500\n",
      "Epoch 1226/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4263 - accuracy: 0.8090 - val_loss: 0.5296 - val_accuracy: 0.7500\n",
      "Epoch 1227/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4263 - accuracy: 0.8090 - val_loss: 0.5297 - val_accuracy: 0.7500\n",
      "Epoch 1228/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4262 - accuracy: 0.8090 - val_loss: 0.5297 - val_accuracy: 0.7500\n",
      "Epoch 1229/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4262 - accuracy: 0.8090 - val_loss: 0.5297 - val_accuracy: 0.7500\n",
      "Epoch 1230/1500\n",
      "18/18 [==============================] - 0s 984us/step - loss: 0.4262 - accuracy: 0.8090 - val_loss: 0.5297 - val_accuracy: 0.7500\n",
      "Epoch 1231/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4262 - accuracy: 0.8108 - val_loss: 0.5297 - val_accuracy: 0.7500\n",
      "Epoch 1232/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4261 - accuracy: 0.8090 - val_loss: 0.5297 - val_accuracy: 0.7500\n",
      "Epoch 1233/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4261 - accuracy: 0.8090 - val_loss: 0.5297 - val_accuracy: 0.7500\n",
      "Epoch 1234/1500\n",
      "18/18 [==============================] - 0s 952us/step - loss: 0.4261 - accuracy: 0.8108 - val_loss: 0.5297 - val_accuracy: 0.7500\n",
      "Epoch 1235/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4260 - accuracy: 0.8125 - val_loss: 0.5297 - val_accuracy: 0.7500\n",
      "Epoch 1236/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4260 - accuracy: 0.8090 - val_loss: 0.5298 - val_accuracy: 0.7500\n",
      "Epoch 1237/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4260 - accuracy: 0.8108 - val_loss: 0.5298 - val_accuracy: 0.7500\n",
      "Epoch 1238/1500\n",
      "18/18 [==============================] - 0s 980us/step - loss: 0.4260 - accuracy: 0.8090 - val_loss: 0.5298 - val_accuracy: 0.7500\n",
      "Epoch 1239/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4259 - accuracy: 0.8090 - val_loss: 0.5298 - val_accuracy: 0.7500\n",
      "Epoch 1240/1500\n",
      "18/18 [==============================] - 0s 989us/step - loss: 0.4259 - accuracy: 0.8108 - val_loss: 0.5298 - val_accuracy: 0.7500\n",
      "Epoch 1241/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.8090 - val_loss: 0.5298 - val_accuracy: 0.7500\n",
      "Epoch 1242/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4259 - accuracy: 0.8090 - val_loss: 0.5298 - val_accuracy: 0.7500\n",
      "Epoch 1243/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4258 - accuracy: 0.8108 - val_loss: 0.5298 - val_accuracy: 0.7500\n",
      "Epoch 1244/1500\n",
      "18/18 [==============================] - 0s 983us/step - loss: 0.4258 - accuracy: 0.8108 - val_loss: 0.5298 - val_accuracy: 0.7500\n",
      "Epoch 1245/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4258 - accuracy: 0.8090 - val_loss: 0.5298 - val_accuracy: 0.7552\n",
      "Epoch 1246/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4258 - accuracy: 0.8090 - val_loss: 0.5298 - val_accuracy: 0.7552\n",
      "Epoch 1247/1500\n",
      "18/18 [==============================] - 0s 988us/step - loss: 0.4257 - accuracy: 0.8108 - val_loss: 0.5298 - val_accuracy: 0.7552\n",
      "Epoch 1248/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4257 - accuracy: 0.8108 - val_loss: 0.5299 - val_accuracy: 0.7552\n",
      "Epoch 1249/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4257 - accuracy: 0.8090 - val_loss: 0.5299 - val_accuracy: 0.7552\n",
      "Epoch 1250/1500\n",
      "18/18 [==============================] - 0s 998us/step - loss: 0.4256 - accuracy: 0.8108 - val_loss: 0.5299 - val_accuracy: 0.7552\n",
      "Epoch 1251/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4256 - accuracy: 0.8108 - val_loss: 0.5299 - val_accuracy: 0.7552\n",
      "Epoch 1252/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4256 - accuracy: 0.8108 - val_loss: 0.5299 - val_accuracy: 0.7552\n",
      "Epoch 1253/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4256 - accuracy: 0.8108 - val_loss: 0.5299 - val_accuracy: 0.7552\n",
      "Epoch 1254/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4255 - accuracy: 0.8108 - val_loss: 0.5299 - val_accuracy: 0.7552\n",
      "Epoch 1255/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4255 - accuracy: 0.8108 - val_loss: 0.5299 - val_accuracy: 0.7552\n",
      "Epoch 1256/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4255 - accuracy: 0.8108 - val_loss: 0.5299 - val_accuracy: 0.7552\n",
      "Epoch 1257/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4255 - accuracy: 0.8108 - val_loss: 0.5300 - val_accuracy: 0.7552\n",
      "Epoch 1258/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4254 - accuracy: 0.8108 - val_loss: 0.5300 - val_accuracy: 0.7552\n",
      "Epoch 1259/1500\n",
      "18/18 [==============================] - 0s 960us/step - loss: 0.4254 - accuracy: 0.8108 - val_loss: 0.5300 - val_accuracy: 0.7552\n",
      "Epoch 1260/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4254 - accuracy: 0.8108 - val_loss: 0.5300 - val_accuracy: 0.7552\n",
      "Epoch 1261/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4253 - accuracy: 0.8108 - val_loss: 0.5300 - val_accuracy: 0.7552\n",
      "Epoch 1262/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4253 - accuracy: 0.8108 - val_loss: 0.5300 - val_accuracy: 0.7552\n",
      "Epoch 1263/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4253 - accuracy: 0.8108 - val_loss: 0.5300 - val_accuracy: 0.7552\n",
      "Epoch 1264/1500\n",
      "18/18 [==============================] - 0s 957us/step - loss: 0.4253 - accuracy: 0.8108 - val_loss: 0.5300 - val_accuracy: 0.7552\n",
      "Epoch 1265/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4253 - accuracy: 0.8108 - val_loss: 0.5300 - val_accuracy: 0.7552\n",
      "Epoch 1266/1500\n",
      "18/18 [==============================] - 0s 976us/step - loss: 0.4252 - accuracy: 0.8108 - val_loss: 0.5301 - val_accuracy: 0.7552\n",
      "Epoch 1267/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4252 - accuracy: 0.8108 - val_loss: 0.5301 - val_accuracy: 0.7552\n",
      "Epoch 1268/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4251 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7552\n",
      "Epoch 1269/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4251 - accuracy: 0.8108 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1270/1500\n",
      "18/18 [==============================] - 0s 961us/step - loss: 0.4251 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1271/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4251 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1272/1500\n",
      "18/18 [==============================] - 0s 912us/step - loss: 0.4251 - accuracy: 0.8108 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1273/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4250 - accuracy: 0.8108 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1274/1500\n",
      "18/18 [==============================] - 0s 970us/step - loss: 0.4250 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1275/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4250 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1276/1500\n",
      "18/18 [==============================] - 0s 958us/step - loss: 0.4249 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1277/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4249 - accuracy: 0.8108 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1278/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4248 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1279/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4248 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1280/1500\n",
      "18/18 [==============================] - 0s 952us/step - loss: 0.4248 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1281/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4248 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1282/1500\n",
      "18/18 [==============================] - 0s 963us/step - loss: 0.4247 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1283/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4247 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1284/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4247 - accuracy: 0.8108 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1285/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4246 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1286/1500\n",
      "18/18 [==============================] - 0s 926us/step - loss: 0.4246 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1287/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4246 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1288/1500\n",
      "18/18 [==============================] - 0s 970us/step - loss: 0.4246 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1289/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4245 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1290/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4245 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1291/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4244 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1292/1500\n",
      "18/18 [==============================] - 0s 915us/step - loss: 0.4245 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1293/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4244 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7604\n",
      "Epoch 1294/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4243 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7656\n",
      "Epoch 1295/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4243 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7656\n",
      "Epoch 1296/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4243 - accuracy: 0.8125 - val_loss: 0.5301 - val_accuracy: 0.7656\n",
      "Epoch 1297/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4242 - accuracy: 0.8125 - val_loss: 0.5302 - val_accuracy: 0.7656\n",
      "Epoch 1298/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4242 - accuracy: 0.8125 - val_loss: 0.5302 - val_accuracy: 0.7656\n",
      "Epoch 1299/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4242 - accuracy: 0.8125 - val_loss: 0.5302 - val_accuracy: 0.7656\n",
      "Epoch 1300/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4242 - accuracy: 0.8125 - val_loss: 0.5302 - val_accuracy: 0.7656\n",
      "Epoch 1301/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4241 - accuracy: 0.8125 - val_loss: 0.5302 - val_accuracy: 0.7656\n",
      "Epoch 1302/1500\n",
      "18/18 [==============================] - 0s 957us/step - loss: 0.4241 - accuracy: 0.8125 - val_loss: 0.5302 - val_accuracy: 0.7656\n",
      "Epoch 1303/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4241 - accuracy: 0.8125 - val_loss: 0.5302 - val_accuracy: 0.7656\n",
      "Epoch 1304/1500\n",
      "18/18 [==============================] - 0s 999us/step - loss: 0.4241 - accuracy: 0.8125 - val_loss: 0.5302 - val_accuracy: 0.7656\n",
      "Epoch 1305/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4240 - accuracy: 0.8125 - val_loss: 0.5303 - val_accuracy: 0.7656\n",
      "Epoch 1306/1500\n",
      "18/18 [==============================] - 0s 928us/step - loss: 0.4240 - accuracy: 0.8125 - val_loss: 0.5303 - val_accuracy: 0.7656\n",
      "Epoch 1307/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4240 - accuracy: 0.8125 - val_loss: 0.5303 - val_accuracy: 0.7656\n",
      "Epoch 1308/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4239 - accuracy: 0.8125 - val_loss: 0.5303 - val_accuracy: 0.7656\n",
      "Epoch 1309/1500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4240 - accuracy: 0.8125 - val_loss: 0.5303 - val_accuracy: 0.7656\n",
      "Epoch 1310/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4239 - accuracy: 0.8125 - val_loss: 0.5303 - val_accuracy: 0.7656\n",
      "Epoch 1311/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4238 - accuracy: 0.8125 - val_loss: 0.5303 - val_accuracy: 0.7656\n",
      "Epoch 1312/1500\n",
      "18/18 [==============================] - 0s 960us/step - loss: 0.4239 - accuracy: 0.8125 - val_loss: 0.5303 - val_accuracy: 0.7656\n",
      "Epoch 1313/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4238 - accuracy: 0.8125 - val_loss: 0.5303 - val_accuracy: 0.7656\n",
      "Epoch 1314/1500\n",
      "18/18 [==============================] - 0s 938us/step - loss: 0.4238 - accuracy: 0.8125 - val_loss: 0.5303 - val_accuracy: 0.7656\n",
      "Epoch 1315/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4237 - accuracy: 0.8125 - val_loss: 0.5303 - val_accuracy: 0.7656\n",
      "Epoch 1316/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4237 - accuracy: 0.8125 - val_loss: 0.5303 - val_accuracy: 0.7656\n",
      "Epoch 1317/1500\n",
      "18/18 [==============================] - 0s 985us/step - loss: 0.4237 - accuracy: 0.8125 - val_loss: 0.5304 - val_accuracy: 0.7656\n",
      "Epoch 1318/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4237 - accuracy: 0.8125 - val_loss: 0.5304 - val_accuracy: 0.7656\n",
      "Epoch 1319/1500\n",
      "18/18 [==============================] - 0s 945us/step - loss: 0.4236 - accuracy: 0.8125 - val_loss: 0.5304 - val_accuracy: 0.7656\n",
      "Epoch 1320/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4236 - accuracy: 0.8125 - val_loss: 0.5304 - val_accuracy: 0.7656\n",
      "Epoch 1321/1500\n",
      "18/18 [==============================] - 0s 922us/step - loss: 0.4236 - accuracy: 0.8125 - val_loss: 0.5304 - val_accuracy: 0.7656\n",
      "Epoch 1322/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4236 - accuracy: 0.8125 - val_loss: 0.5304 - val_accuracy: 0.7656\n",
      "Epoch 1323/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4235 - accuracy: 0.8125 - val_loss: 0.5304 - val_accuracy: 0.7656\n",
      "Epoch 1324/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4235 - accuracy: 0.8125 - val_loss: 0.5304 - val_accuracy: 0.7656\n",
      "Epoch 1325/1500\n",
      "18/18 [==============================] - 0s 942us/step - loss: 0.4235 - accuracy: 0.8125 - val_loss: 0.5304 - val_accuracy: 0.7656\n",
      "Epoch 1326/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4234 - accuracy: 0.8125 - val_loss: 0.5304 - val_accuracy: 0.7656\n",
      "Epoch 1327/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4234 - accuracy: 0.8125 - val_loss: 0.5304 - val_accuracy: 0.7656\n",
      "Epoch 1328/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4234 - accuracy: 0.8125 - val_loss: 0.5305 - val_accuracy: 0.7656\n",
      "Epoch 1329/1500\n",
      "18/18 [==============================] - 0s 923us/step - loss: 0.4234 - accuracy: 0.8125 - val_loss: 0.5305 - val_accuracy: 0.7656\n",
      "Epoch 1330/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4234 - accuracy: 0.8125 - val_loss: 0.5305 - val_accuracy: 0.7656\n",
      "Epoch 1331/1500\n",
      "18/18 [==============================] - 0s 951us/step - loss: 0.4234 - accuracy: 0.8125 - val_loss: 0.5305 - val_accuracy: 0.7656\n",
      "Epoch 1332/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4233 - accuracy: 0.8125 - val_loss: 0.5305 - val_accuracy: 0.7656\n",
      "Epoch 1333/1500\n",
      "18/18 [==============================] - 0s 923us/step - loss: 0.4233 - accuracy: 0.8125 - val_loss: 0.5305 - val_accuracy: 0.7656\n",
      "Epoch 1334/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4232 - accuracy: 0.8125 - val_loss: 0.5305 - val_accuracy: 0.7656\n",
      "Epoch 1335/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4232 - accuracy: 0.8125 - val_loss: 0.5305 - val_accuracy: 0.7656\n",
      "Epoch 1336/1500\n",
      "18/18 [==============================] - 0s 992us/step - loss: 0.4232 - accuracy: 0.8125 - val_loss: 0.5305 - val_accuracy: 0.7656\n",
      "Epoch 1337/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4232 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1338/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4231 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1339/1500\n",
      "18/18 [==============================] - 0s 935us/step - loss: 0.4231 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1340/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4231 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1341/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4231 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1342/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4231 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1343/1500\n",
      "18/18 [==============================] - 0s 908us/step - loss: 0.4230 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1344/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4230 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1345/1500\n",
      "18/18 [==============================] - 0s 951us/step - loss: 0.4229 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1346/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4229 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1347/1500\n",
      "18/18 [==============================] - 0s 956us/step - loss: 0.4229 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1348/1500\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4229 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1349/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4228 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1350/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4228 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1351/1500\n",
      "18/18 [==============================] - 0s 905us/step - loss: 0.4228 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1352/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4228 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1353/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4228 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1354/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4227 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1355/1500\n",
      "18/18 [==============================] - 0s 923us/step - loss: 0.4227 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1356/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4227 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1357/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4226 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1358/1500\n",
      "18/18 [==============================] - 0s 989us/step - loss: 0.4227 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1359/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4226 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1360/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4226 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1361/1500\n",
      "18/18 [==============================] - 0s 928us/step - loss: 0.4226 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1362/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4225 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1363/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4225 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1364/1500\n",
      "18/18 [==============================] - 0s 950us/step - loss: 0.4225 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1365/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4225 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1366/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4224 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1367/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4224 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1368/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4224 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1369/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4224 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1370/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4223 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1371/1500\n",
      "18/18 [==============================] - 0s 959us/step - loss: 0.4223 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1372/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4223 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1373/1500\n",
      "18/18 [==============================] - 0s 945us/step - loss: 0.4223 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1374/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4222 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1375/1500\n",
      "18/18 [==============================] - 0s 970us/step - loss: 0.4222 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1376/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4222 - accuracy: 0.8125 - val_loss: 0.5307 - val_accuracy: 0.7656\n",
      "Epoch 1377/1500\n",
      "18/18 [==============================] - 0s 901us/step - loss: 0.4222 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1378/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4221 - accuracy: 0.8125 - val_loss: 0.5307 - val_accuracy: 0.7656\n",
      "Epoch 1379/1500\n",
      "18/18 [==============================] - 0s 987us/step - loss: 0.4221 - accuracy: 0.8125 - val_loss: 0.5306 - val_accuracy: 0.7656\n",
      "Epoch 1380/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4221 - accuracy: 0.8142 - val_loss: 0.5307 - val_accuracy: 0.7656\n",
      "Epoch 1381/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4220 - accuracy: 0.8142 - val_loss: 0.5307 - val_accuracy: 0.7656\n",
      "Epoch 1382/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4220 - accuracy: 0.8142 - val_loss: 0.5307 - val_accuracy: 0.7656\n",
      "Epoch 1383/1500\n",
      "18/18 [==============================] - 0s 924us/step - loss: 0.4220 - accuracy: 0.8125 - val_loss: 0.5307 - val_accuracy: 0.7656\n",
      "Epoch 1384/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4220 - accuracy: 0.8125 - val_loss: 0.5307 - val_accuracy: 0.7656\n",
      "Epoch 1385/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4220 - accuracy: 0.8142 - val_loss: 0.5307 - val_accuracy: 0.7656\n",
      "Epoch 1386/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4219 - accuracy: 0.8125 - val_loss: 0.5307 - val_accuracy: 0.7656\n",
      "Epoch 1387/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4219 - accuracy: 0.8125 - val_loss: 0.5307 - val_accuracy: 0.7656\n",
      "Epoch 1388/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4219 - accuracy: 0.8142 - val_loss: 0.5308 - val_accuracy: 0.7656\n",
      "Epoch 1389/1500\n",
      "18/18 [==============================] - 0s 931us/step - loss: 0.4219 - accuracy: 0.8142 - val_loss: 0.5307 - val_accuracy: 0.7656\n",
      "Epoch 1390/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4219 - accuracy: 0.8142 - val_loss: 0.5308 - val_accuracy: 0.7656\n",
      "Epoch 1391/1500\n",
      "18/18 [==============================] - 0s 993us/step - loss: 0.4218 - accuracy: 0.8142 - val_loss: 0.5308 - val_accuracy: 0.7656\n",
      "Epoch 1392/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4218 - accuracy: 0.8142 - val_loss: 0.5308 - val_accuracy: 0.7656\n",
      "Epoch 1393/1500\n",
      "18/18 [==============================] - 0s 953us/step - loss: 0.4218 - accuracy: 0.8142 - val_loss: 0.5308 - val_accuracy: 0.7656\n",
      "Epoch 1394/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4218 - accuracy: 0.8142 - val_loss: 0.5308 - val_accuracy: 0.7656\n",
      "Epoch 1395/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4217 - accuracy: 0.8142 - val_loss: 0.5308 - val_accuracy: 0.7656\n",
      "Epoch 1396/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4217 - accuracy: 0.8142 - val_loss: 0.5308 - val_accuracy: 0.7656\n",
      "Epoch 1397/1500\n",
      "18/18 [==============================] - 0s 985us/step - loss: 0.4217 - accuracy: 0.8142 - val_loss: 0.5308 - val_accuracy: 0.7656\n",
      "Epoch 1398/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4217 - accuracy: 0.8142 - val_loss: 0.5309 - val_accuracy: 0.7656\n",
      "Epoch 1399/1500\n",
      "18/18 [==============================] - 0s 967us/step - loss: 0.4216 - accuracy: 0.8142 - val_loss: 0.5309 - val_accuracy: 0.7656\n",
      "Epoch 1400/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4216 - accuracy: 0.8142 - val_loss: 0.5309 - val_accuracy: 0.7656\n",
      "Epoch 1401/1500\n",
      "18/18 [==============================] - 0s 940us/step - loss: 0.4216 - accuracy: 0.8142 - val_loss: 0.5309 - val_accuracy: 0.7656\n",
      "Epoch 1402/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4216 - accuracy: 0.8142 - val_loss: 0.5309 - val_accuracy: 0.7656\n",
      "Epoch 1403/1500\n",
      "18/18 [==============================] - 0s 934us/step - loss: 0.4215 - accuracy: 0.8142 - val_loss: 0.5309 - val_accuracy: 0.7656\n",
      "Epoch 1404/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4215 - accuracy: 0.8142 - val_loss: 0.5309 - val_accuracy: 0.7656\n",
      "Epoch 1405/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4215 - accuracy: 0.8160 - val_loss: 0.5310 - val_accuracy: 0.7656\n",
      "Epoch 1406/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4215 - accuracy: 0.8160 - val_loss: 0.5310 - val_accuracy: 0.7656\n",
      "Epoch 1407/1500\n",
      "18/18 [==============================] - 0s 909us/step - loss: 0.4214 - accuracy: 0.8160 - val_loss: 0.5310 - val_accuracy: 0.7656\n",
      "Epoch 1408/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4214 - accuracy: 0.8160 - val_loss: 0.5310 - val_accuracy: 0.7656\n",
      "Epoch 1409/1500\n",
      "18/18 [==============================] - 0s 996us/step - loss: 0.4214 - accuracy: 0.8160 - val_loss: 0.5310 - val_accuracy: 0.7656\n",
      "Epoch 1410/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4213 - accuracy: 0.8160 - val_loss: 0.5311 - val_accuracy: 0.7656\n",
      "Epoch 1411/1500\n",
      "18/18 [==============================] - 0s 954us/step - loss: 0.4214 - accuracy: 0.8160 - val_loss: 0.5311 - val_accuracy: 0.7656\n",
      "Epoch 1412/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4213 - accuracy: 0.8160 - val_loss: 0.5311 - val_accuracy: 0.7656\n",
      "Epoch 1413/1500\n",
      "18/18 [==============================] - 0s 970us/step - loss: 0.4213 - accuracy: 0.8160 - val_loss: 0.5311 - val_accuracy: 0.7656\n",
      "Epoch 1414/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4213 - accuracy: 0.8160 - val_loss: 0.5311 - val_accuracy: 0.7656\n",
      "Epoch 1415/1500\n",
      "18/18 [==============================] - 0s 937us/step - loss: 0.4212 - accuracy: 0.8160 - val_loss: 0.5311 - val_accuracy: 0.7656\n",
      "Epoch 1416/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4212 - accuracy: 0.8160 - val_loss: 0.5311 - val_accuracy: 0.7656\n",
      "Epoch 1417/1500\n",
      "18/18 [==============================] - 0s 934us/step - loss: 0.4212 - accuracy: 0.8160 - val_loss: 0.5311 - val_accuracy: 0.7656\n",
      "Epoch 1418/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4212 - accuracy: 0.8160 - val_loss: 0.5312 - val_accuracy: 0.7656\n",
      "Epoch 1419/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4211 - accuracy: 0.8160 - val_loss: 0.5312 - val_accuracy: 0.7656\n",
      "Epoch 1420/1500\n",
      "18/18 [==============================] - 0s 911us/step - loss: 0.4211 - accuracy: 0.8160 - val_loss: 0.5312 - val_accuracy: 0.7656\n",
      "Epoch 1421/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4211 - accuracy: 0.8160 - val_loss: 0.5312 - val_accuracy: 0.7656\n",
      "Epoch 1422/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4211 - accuracy: 0.8160 - val_loss: 0.5313 - val_accuracy: 0.7656\n",
      "Epoch 1423/1500\n",
      "18/18 [==============================] - 0s 989us/step - loss: 0.4210 - accuracy: 0.8160 - val_loss: 0.5313 - val_accuracy: 0.7656\n",
      "Epoch 1424/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4210 - accuracy: 0.8160 - val_loss: 0.5313 - val_accuracy: 0.7656\n",
      "Epoch 1425/1500\n",
      "18/18 [==============================] - 0s 949us/step - loss: 0.4210 - accuracy: 0.8160 - val_loss: 0.5313 - val_accuracy: 0.7656\n",
      "Epoch 1426/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4210 - accuracy: 0.8160 - val_loss: 0.5314 - val_accuracy: 0.7656\n",
      "Epoch 1427/1500\n",
      "18/18 [==============================] - 0s 948us/step - loss: 0.4210 - accuracy: 0.8160 - val_loss: 0.5314 - val_accuracy: 0.7656\n",
      "Epoch 1428/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4209 - accuracy: 0.8160 - val_loss: 0.5314 - val_accuracy: 0.7656\n",
      "Epoch 1429/1500\n",
      "18/18 [==============================] - 0s 946us/step - loss: 0.4209 - accuracy: 0.8160 - val_loss: 0.5314 - val_accuracy: 0.7656\n",
      "Epoch 1430/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4209 - accuracy: 0.8160 - val_loss: 0.5315 - val_accuracy: 0.7656\n",
      "Epoch 1431/1500\n",
      "18/18 [==============================] - 0s 978us/step - loss: 0.4209 - accuracy: 0.8160 - val_loss: 0.5315 - val_accuracy: 0.7656\n",
      "Epoch 1432/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4208 - accuracy: 0.8160 - val_loss: 0.5315 - val_accuracy: 0.7656\n",
      "Epoch 1433/1500\n",
      "18/18 [==============================] - 0s 946us/step - loss: 0.4208 - accuracy: 0.8160 - val_loss: 0.5315 - val_accuracy: 0.7656\n",
      "Epoch 1434/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4208 - accuracy: 0.8160 - val_loss: 0.5316 - val_accuracy: 0.7656\n",
      "Epoch 1435/1500\n",
      "18/18 [==============================] - 0s 984us/step - loss: 0.4207 - accuracy: 0.8160 - val_loss: 0.5316 - val_accuracy: 0.7656\n",
      "Epoch 1436/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4207 - accuracy: 0.8160 - val_loss: 0.5316 - val_accuracy: 0.7656\n",
      "Epoch 1437/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4208 - accuracy: 0.8160 - val_loss: 0.5317 - val_accuracy: 0.7656\n",
      "Epoch 1438/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4207 - accuracy: 0.8160 - val_loss: 0.5317 - val_accuracy: 0.7656\n",
      "Epoch 1439/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4207 - accuracy: 0.8160 - val_loss: 0.5317 - val_accuracy: 0.7656\n",
      "Epoch 1440/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4206 - accuracy: 0.8160 - val_loss: 0.5317 - val_accuracy: 0.7656\n",
      "Epoch 1441/1500\n",
      "18/18 [==============================] - 0s 984us/step - loss: 0.4206 - accuracy: 0.8160 - val_loss: 0.5318 - val_accuracy: 0.7656\n",
      "Epoch 1442/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4206 - accuracy: 0.8160 - val_loss: 0.5318 - val_accuracy: 0.7656\n",
      "Epoch 1443/1500\n",
      "18/18 [==============================] - 0s 960us/step - loss: 0.4206 - accuracy: 0.8160 - val_loss: 0.5318 - val_accuracy: 0.7656\n",
      "Epoch 1444/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4206 - accuracy: 0.8160 - val_loss: 0.5318 - val_accuracy: 0.7656\n",
      "Epoch 1445/1500\n",
      "18/18 [==============================] - 0s 967us/step - loss: 0.4206 - accuracy: 0.8160 - val_loss: 0.5319 - val_accuracy: 0.7656\n",
      "Epoch 1446/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4205 - accuracy: 0.8160 - val_loss: 0.5319 - val_accuracy: 0.7656\n",
      "Epoch 1447/1500\n",
      "18/18 [==============================] - 0s 974us/step - loss: 0.4205 - accuracy: 0.8160 - val_loss: 0.5319 - val_accuracy: 0.7656\n",
      "Epoch 1448/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4205 - accuracy: 0.8160 - val_loss: 0.5320 - val_accuracy: 0.7656\n",
      "Epoch 1449/1500\n",
      "18/18 [==============================] - 0s 981us/step - loss: 0.4205 - accuracy: 0.8160 - val_loss: 0.5320 - val_accuracy: 0.7656\n",
      "Epoch 1450/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4205 - accuracy: 0.8160 - val_loss: 0.5320 - val_accuracy: 0.7656\n",
      "Epoch 1451/1500\n",
      "18/18 [==============================] - 0s 972us/step - loss: 0.4205 - accuracy: 0.8160 - val_loss: 0.5320 - val_accuracy: 0.7656\n",
      "Epoch 1452/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4204 - accuracy: 0.8160 - val_loss: 0.5321 - val_accuracy: 0.7656\n",
      "Epoch 1453/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4204 - accuracy: 0.8160 - val_loss: 0.5321 - val_accuracy: 0.7656\n",
      "Epoch 1454/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4204 - accuracy: 0.8160 - val_loss: 0.5321 - val_accuracy: 0.7656\n",
      "Epoch 1455/1500\n",
      "18/18 [==============================] - 0s 917us/step - loss: 0.4203 - accuracy: 0.8160 - val_loss: 0.5321 - val_accuracy: 0.7656\n",
      "Epoch 1456/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4203 - accuracy: 0.8160 - val_loss: 0.5322 - val_accuracy: 0.7656\n",
      "Epoch 1457/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4203 - accuracy: 0.8160 - val_loss: 0.5322 - val_accuracy: 0.7656\n",
      "Epoch 1458/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4203 - accuracy: 0.8160 - val_loss: 0.5322 - val_accuracy: 0.7656\n",
      "Epoch 1459/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4203 - accuracy: 0.8160 - val_loss: 0.5322 - val_accuracy: 0.7656\n",
      "Epoch 1460/1500\n",
      "18/18 [==============================] - 0s 931us/step - loss: 0.4203 - accuracy: 0.8160 - val_loss: 0.5323 - val_accuracy: 0.7656\n",
      "Epoch 1461/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4203 - accuracy: 0.8160 - val_loss: 0.5323 - val_accuracy: 0.7656\n",
      "Epoch 1462/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4202 - accuracy: 0.8160 - val_loss: 0.5323 - val_accuracy: 0.7656\n",
      "Epoch 1463/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4202 - accuracy: 0.8160 - val_loss: 0.5323 - val_accuracy: 0.7656\n",
      "Epoch 1464/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4202 - accuracy: 0.8160 - val_loss: 0.5323 - val_accuracy: 0.7656\n",
      "Epoch 1465/1500\n",
      "18/18 [==============================] - 0s 946us/step - loss: 0.4202 - accuracy: 0.8160 - val_loss: 0.5323 - val_accuracy: 0.7708\n",
      "Epoch 1466/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4201 - accuracy: 0.8160 - val_loss: 0.5324 - val_accuracy: 0.7708\n",
      "Epoch 1467/1500\n",
      "18/18 [==============================] - 0s 962us/step - loss: 0.4201 - accuracy: 0.8160 - val_loss: 0.5324 - val_accuracy: 0.7708\n",
      "Epoch 1468/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4201 - accuracy: 0.8160 - val_loss: 0.5324 - val_accuracy: 0.7708\n",
      "Epoch 1469/1500\n",
      "18/18 [==============================] - 0s 951us/step - loss: 0.4200 - accuracy: 0.8160 - val_loss: 0.5324 - val_accuracy: 0.7708\n",
      "Epoch 1470/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4200 - accuracy: 0.8160 - val_loss: 0.5325 - val_accuracy: 0.7708\n",
      "Epoch 1471/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4200 - accuracy: 0.8160 - val_loss: 0.5325 - val_accuracy: 0.7708\n",
      "Epoch 1472/1500\n",
      "18/18 [==============================] - 0s 986us/step - loss: 0.4200 - accuracy: 0.8160 - val_loss: 0.5325 - val_accuracy: 0.7708\n",
      "Epoch 1473/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4200 - accuracy: 0.8160 - val_loss: 0.5325 - val_accuracy: 0.7708\n",
      "Epoch 1474/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4199 - accuracy: 0.8160 - val_loss: 0.5326 - val_accuracy: 0.7708\n",
      "Epoch 1475/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4199 - accuracy: 0.8177 - val_loss: 0.5326 - val_accuracy: 0.7708\n",
      "Epoch 1476/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4199 - accuracy: 0.8160 - val_loss: 0.5326 - val_accuracy: 0.7708\n",
      "Epoch 1477/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4199 - accuracy: 0.8177 - val_loss: 0.5326 - val_accuracy: 0.7708\n",
      "Epoch 1478/1500\n",
      "18/18 [==============================] - 0s 916us/step - loss: 0.4198 - accuracy: 0.8177 - val_loss: 0.5326 - val_accuracy: 0.7708\n",
      "Epoch 1479/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4198 - accuracy: 0.8177 - val_loss: 0.5326 - val_accuracy: 0.7708\n",
      "Epoch 1480/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4198 - accuracy: 0.8160 - val_loss: 0.5327 - val_accuracy: 0.7708\n",
      "Epoch 1481/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4198 - accuracy: 0.8177 - val_loss: 0.5327 - val_accuracy: 0.7708\n",
      "Epoch 1482/1500\n",
      "18/18 [==============================] - 0s 956us/step - loss: 0.4198 - accuracy: 0.8177 - val_loss: 0.5327 - val_accuracy: 0.7708\n",
      "Epoch 1483/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4198 - accuracy: 0.8177 - val_loss: 0.5327 - val_accuracy: 0.7708\n",
      "Epoch 1484/1500\n",
      "18/18 [==============================] - 0s 974us/step - loss: 0.4197 - accuracy: 0.8177 - val_loss: 0.5328 - val_accuracy: 0.7708\n",
      "Epoch 1485/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4197 - accuracy: 0.8177 - val_loss: 0.5328 - val_accuracy: 0.7708\n",
      "Epoch 1486/1500\n",
      "18/18 [==============================] - 0s 960us/step - loss: 0.4197 - accuracy: 0.8160 - val_loss: 0.5328 - val_accuracy: 0.7708\n",
      "Epoch 1487/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4197 - accuracy: 0.8177 - val_loss: 0.5328 - val_accuracy: 0.7708\n",
      "Epoch 1488/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4196 - accuracy: 0.8160 - val_loss: 0.5329 - val_accuracy: 0.7708\n",
      "Epoch 1489/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4196 - accuracy: 0.8177 - val_loss: 0.5329 - val_accuracy: 0.7708\n",
      "Epoch 1490/1500\n",
      "18/18 [==============================] - 0s 920us/step - loss: 0.4196 - accuracy: 0.8177 - val_loss: 0.5329 - val_accuracy: 0.7708\n",
      "Epoch 1491/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4196 - accuracy: 0.8177 - val_loss: 0.5329 - val_accuracy: 0.7708\n",
      "Epoch 1492/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4195 - accuracy: 0.8177 - val_loss: 0.5329 - val_accuracy: 0.7708\n",
      "Epoch 1493/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4195 - accuracy: 0.8177 - val_loss: 0.5330 - val_accuracy: 0.7708\n",
      "Epoch 1494/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4195 - accuracy: 0.8177 - val_loss: 0.5330 - val_accuracy: 0.7708\n",
      "Epoch 1495/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4194 - accuracy: 0.8177 - val_loss: 0.5330 - val_accuracy: 0.7708\n",
      "Epoch 1496/1500\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4194 - accuracy: 0.8160 - val_loss: 0.5331 - val_accuracy: 0.7708\n",
      "Epoch 1497/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4194 - accuracy: 0.8177 - val_loss: 0.5331 - val_accuracy: 0.7708\n",
      "Epoch 1498/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4194 - accuracy: 0.8160 - val_loss: 0.5331 - val_accuracy: 0.7708\n",
      "Epoch 1499/1500\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4194 - accuracy: 0.8160 - val_loss: 0.5332 - val_accuracy: 0.7708\n",
      "Epoch 1500/1500\n",
      "18/18 [==============================] - 0s 953us/step - loss: 0.4194 - accuracy: 0.8160 - val_loss: 0.5332 - val_accuracy: 0.7708\n"
     ]
    }
   ],
   "source": [
    "### BEGIN SOLUTION\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(6, input_shape=(8,), activation=\"relu\"))\n",
    "model_2.add(Dense(6,  activation=\"relu\"))\n",
    "model_2.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_2.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_2 = model_2.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_hist_2.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy over iterations')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(run_hist_2.history[\"loss\"])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(range(n), (run_hist_2.history[\"loss\"]),'r.', label=\"Train Loss\")\n",
    "ax.plot(range(n), (run_hist_2.history[\"val_loss\"]),'b.', label=\"Validation Loss\")\n",
    "ax.legend()\n",
    "ax.set_title('Loss over iterations')\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(range(n), (run_hist_2.history[\"accuracy\"]),'r.', label=\"Train Acc\")\n",
    "ax.plot(range(n), (run_hist_2.history[\"val_accuracy\"]),'b.', label=\"Validation Acc\")\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_title('Accuracy over iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 438us/step\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/carlospalominovidal/Library/CloudStorage/OneDrive-UniversidadPeruanadeCiencias/ESTUDIOS/Machine learning - big data/Machine learning ibm especialización/Codigo/05_Deep_Learning_and_Reinforcement_Learning/05d_LAB_Keras_Intro.ipynb Celda 39\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/carlospalominovidal/Library/CloudStorage/OneDrive-UniversidadPeruanadeCiencias/ESTUDIOS/Machine%20learning%20-%20big%20data/Machine%20learning%20ibm%20especializaci%C3%B3n/Codigo/05_Deep_Learning_and_Reinforcement_Learning/05d_LAB_Keras_Intro.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m y_pred_prob_nn_2\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39margmax(y_pred_prob_nn_1,axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/carlospalominovidal/Library/CloudStorage/OneDrive-UniversidadPeruanadeCiencias/ESTUDIOS/Machine%20learning%20-%20big%20data/Machine%20learning%20ibm%20especializaci%C3%B3n/Codigo/05_Deep_Learning_and_Reinforcement_Learning/05d_LAB_Keras_Intro.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/carlospalominovidal/Library/CloudStorage/OneDrive-UniversidadPeruanadeCiencias/ESTUDIOS/Machine%20learning%20-%20big%20data/Machine%20learning%20ibm%20especializaci%C3%B3n/Codigo/05_Deep_Learning_and_Reinforcement_Learning/05d_LAB_Keras_Intro.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39maccuracy is \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(accuracy_score(y_test,y_pred_class_nn_2)))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/carlospalominovidal/Library/CloudStorage/OneDrive-UniversidadPeruanadeCiencias/ESTUDIOS/Machine%20learning%20-%20big%20data/Machine%20learning%20ibm%20especializaci%C3%B3n/Codigo/05_Deep_Learning_and_Reinforcement_Learning/05d_LAB_Keras_Intro.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mroc-auc is \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(roc_auc_score(y_test,y_pred_prob_nn_2)))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/carlospalominovidal/Library/CloudStorage/OneDrive-UniversidadPeruanadeCiencias/ESTUDIOS/Machine%20learning%20-%20big%20data/Machine%20learning%20ibm%20especializaci%C3%B3n/Codigo/05_Deep_Learning_and_Reinforcement_Learning/05d_LAB_Keras_Intro.ipynb#X53sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m plot_roc(y_test, y_pred_prob_nn_2, \u001b[39m'\u001b[39m\u001b[39mNN-2\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:211\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \u001b[39mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39m0.5\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    212\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    213\u001b[0m \u001b[39mif\u001b[39;00m y_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     90\u001b[0m     y_type \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_type) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     94\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mClassification metrics can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt handle a mix of \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m targets\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     95\u001b[0m             type_true, type_pred\n\u001b[1;32m     96\u001b[0m         )\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     99\u001b[0m \u001b[39m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[1;32m    100\u001b[0m y_type \u001b[39m=\u001b[39m y_type\u001b[39m.\u001b[39mpop()\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "y_pred_class_nn_2 = model_1.predict(X_test_norm)\n",
    "y_pred_prob_nn_2=np.argmax(y_pred_prob_nn_1,axis=1)\n",
    "print('')\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_2)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_2)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_2, 'NN-2')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
